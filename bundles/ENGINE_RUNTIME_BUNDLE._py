# ======================================================================
# LLM INSTRUCTIONS
# ======================================================================
# This file is an auto-generated, complete source code bundle of the Intergrax framework.
#
# Bundle scope:
#   - folder=intergrax/runtime/drop_in_knowledge_mode/
#   - Project root: D:\Projekty\intergrax
#
# IMPORTANT RULES FOR THE MODEL:
# 1) Treat THIS file as the single source of truth for the included scope.
# 2) Do NOT assume any missing code exists elsewhere.
# 3) When proposing changes, always reference the exact FILE and MODULE headers below.
# 4) Prefer edits that preserve existing architecture, naming, and conventions.
#
# How to navigate this bundle:
# - Use the MODULE MAP and INDEX to find modules.
# - Each original file is included below with a header:
#     FILE: <relative path>
#     MODULE: <python import path>
#     MODULE_GROUP: <first folder under intergrax/>
#     SYMBOLS: <top-level classes/functions>
#
# Included module groups (dynamic):
# - runtime/
#
# Files included: 55
# Total lines: 8407
# ======================================================================

# INTERGRAX MODULE BUNDLE: ENGINE_RUNTIME_BUNDLE (auto-generated)
# ROOT: D:\Projekty\intergrax
# SCOPE: folder=intergrax/runtime/drop_in_knowledge_mode/
# FILES: 55
#
# MODULE MAP (dynamic):
# - runtime/ (55 files)
#
# INDEX (path | module | module_group | lines | sha256[0:12]):
# - intergrax/runtime/drop_in_knowledge_mode/__init__.py | intergrax.runtime.drop_in_knowledge_mode | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/config.py | intergrax.runtime.drop_in_knowledge_mode.config | runtime | 203 | caf6c2dfe986
# - intergrax/runtime/drop_in_knowledge_mode/context/__init__.py | intergrax.runtime.drop_in_knowledge_mode.context | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/context/context_builder.py | intergrax.runtime.drop_in_knowledge_mode.context.context_builder | runtime | 485 | 2cca85dc5dcb
# - intergrax/runtime/drop_in_knowledge_mode/context/engine_history_layer.py | intergrax.runtime.drop_in_knowledge_mode.context.engine_history_layer | runtime | 565 | d4c1a0d530cd
# - intergrax/runtime/drop_in_knowledge_mode/engine/__init__.py | intergrax.runtime.drop_in_knowledge_mode.engine | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/__init__.py | intergrax.runtime.drop_in_knowledge_mode.engine.pipelines | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/contract.py | intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.contract | runtime | 90 | ecdf2240c085
# - intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/no_planner_pipeline.py | intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.no_planner_pipeline | runtime | 55 | 55af7c238de2
# - intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/pipeline_factory.py | intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.pipeline_factory | runtime | 60 | c64139f8be94
# - intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/planner_dynamic_pipeline.py | intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.planner_dynamic_pipeline | runtime | 18 | 7f1bc8b3779d
# - intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/planner_static_pipeline.py | intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.planner_static_pipeline | runtime | 82 | 2bf2a082521a
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime | runtime | 145 | 66c268d64f6f
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_context.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_context | runtime | 230 | 8c778f777561
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_state.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state | runtime | 171 | 97c512f6593d
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/__init__.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/build_base_history_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.build_base_history_step | runtime | 18 | 2be250632ec9
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/contract.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract | runtime | 11 | 98bef6402285
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/core_llm_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.core_llm_step | runtime | 95 | 473febe5e9b6
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/ensure_current_user_message_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.ensure_current_user_message_step | runtime | 40 | 6871578778c5
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/history_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.history_step | runtime | 62 | 3614c40a16b9
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/instructions_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.instructions_step | runtime | 84 | 3de4e2340ab6
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/persist_and_build_answer_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.persist_and_build_answer_step | runtime | 134 | 9511313df01b
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/profile_based_memory_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.profile_based_memory_step | runtime | 83 | efc7a68b87a6
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/rag_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.rag_step | runtime | 93 | c079859fbe82
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/retrieve_attachments_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.retrieve_attachments_step | runtime | 104 | e58c576f2ca5
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/session_and_ingest_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.session_and_ingest_step | runtime | 117 | 3781275bf2fb
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/tools.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools | runtime | 96 | f6d2ff1e397b
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/tools_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools_step | runtime | 157 | f15f1281f0e3
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/user_longterm_memory_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.user_longterm_memory_step | runtime | 123 | 98b9a1ee076a
# - intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/websearch_step.py | intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.websearch_step | runtime | 117 | fbc460761d88
# - intergrax/runtime/drop_in_knowledge_mode/ingestion/__init__.py | intergrax.runtime.drop_in_knowledge_mode.ingestion | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/ingestion/attachments.py | intergrax.runtime.drop_in_knowledge_mode.ingestion.attachments | runtime | 124 | f1f76309b50f
# - intergrax/runtime/drop_in_knowledge_mode/ingestion/ingestion_service.py | intergrax.runtime.drop_in_knowledge_mode.ingestion.ingestion_service | runtime | 470 | f85b4e1154a7
# - intergrax/runtime/drop_in_knowledge_mode/planning/__init__.py | intergrax.runtime.drop_in_knowledge_mode.planning | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/planning/engine_plan_models.py | intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models | runtime | 323 | 17f0b8c31ab4
# - intergrax/runtime/drop_in_knowledge_mode/planning/engine_planner.py | intergrax.runtime.drop_in_knowledge_mode.planning.engine_planner | runtime | 385 | b52a6588b3ad
# - intergrax/runtime/drop_in_knowledge_mode/planning/plan_builder_helper.py | intergrax.runtime.drop_in_knowledge_mode.planning.plan_builder_helper | runtime | 77 | e386380d18e4
# - intergrax/runtime/drop_in_knowledge_mode/planning/runtime_step_handlers.py | intergrax.runtime.drop_in_knowledge_mode.planning.runtime_step_handlers | runtime | 136 | 3fc4960f4dc0
# - intergrax/runtime/drop_in_knowledge_mode/planning/step_executor.py | intergrax.runtime.drop_in_knowledge_mode.planning.step_executor | runtime | 352 | b07fcb4b4aaf
# - intergrax/runtime/drop_in_knowledge_mode/planning/step_executor_models.py | intergrax.runtime.drop_in_knowledge_mode.planning.step_executor_models | runtime | 160 | c06fb5917829
# - intergrax/runtime/drop_in_knowledge_mode/planning/step_planner.py | intergrax.runtime.drop_in_knowledge_mode.planning.step_planner | runtime | 820 | 920e42eea374
# - intergrax/runtime/drop_in_knowledge_mode/planning/stepplan_models.py | intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models | runtime | 411 | ab165f6c4c46
# - intergrax/runtime/drop_in_knowledge_mode/prompts/__init__.py | intergrax.runtime.drop_in_knowledge_mode.prompts | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/prompts/history_prompt_builder.py | intergrax.runtime.drop_in_knowledge_mode.prompts.history_prompt_builder | runtime | 89 | 7a99ef869b92
# - intergrax/runtime/drop_in_knowledge_mode/prompts/rag_prompt_builder.py | intergrax.runtime.drop_in_knowledge_mode.prompts.rag_prompt_builder | runtime | 105 | 487e17f834e1
# - intergrax/runtime/drop_in_knowledge_mode/prompts/user_longterm_memory_prompt_builder.py | intergrax.runtime.drop_in_knowledge_mode.prompts.user_longterm_memory_prompt_builder | runtime | 117 | 8608ee04efaf
# - intergrax/runtime/drop_in_knowledge_mode/prompts/websearch_prompt_builder.py | intergrax.runtime.drop_in_knowledge_mode.prompts.websearch_prompt_builder | runtime | 112 | b51b50c3d26a
# - intergrax/runtime/drop_in_knowledge_mode/responses/__init__.py | intergrax.runtime.drop_in_knowledge_mode.responses | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/responses/response_schema.py | intergrax.runtime.drop_in_knowledge_mode.responses.response_schema | runtime | 182 | 0edf2ebbb1b9
# - intergrax/runtime/drop_in_knowledge_mode/session/__init__.py | intergrax.runtime.drop_in_knowledge_mode.session | runtime | 0 | e3b0c44298fc
# - intergrax/runtime/drop_in_knowledge_mode/session/chat_session.py | intergrax.runtime.drop_in_knowledge_mode.session.chat_session | runtime | 145 | 3b849ae2b512
# - intergrax/runtime/drop_in_knowledge_mode/session/in_memory_session_storage.py | intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage | runtime | 181 | 5d9d175f4412
# - intergrax/runtime/drop_in_knowledge_mode/session/session_manager.py | intergrax.runtime.drop_in_knowledge_mode.session.session_manager | runtime | 651 | 1f580d02a008
# - intergrax/runtime/drop_in_knowledge_mode/session/session_storage.py | intergrax.runtime.drop_in_knowledge_mode.session.session_storage | runtime | 129 | 135a750dd885
#
# TOTAL LINES: 8407
# ======================================================================

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/config.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.config
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=config.py
# LINES: 203
# SHA256: caf6c2dfe98650bbaf97efc1e88bfea325d41a052dec26a15dcbe124bbc162ed
# SYMBOLS:
#   - class ToolsContextScope
#   - class StepPlanningStrategy
#   - class RuntimeConfig
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, Optional, Literal

from intergrax.llm_adapters.llm_adapter import LLMAdapter
from intergrax.rag.embedding_manager import EmbeddingManager
from intergrax.rag.vectorstore_manager import VectorstoreManager
from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import PlannerPromptConfig
from intergrax.runtime.drop_in_knowledge_mode.planning.step_executor_models import StepExecutorConfig
from intergrax.runtime.drop_in_knowledge_mode.planning.step_planner import StepPlannerConfig
from intergrax.tools.tools_agent import ToolsAgent
from intergrax.websearch.service.websearch_config import WebSearchConfig
from intergrax.websearch.service.websearch_executor import WebSearchExecutor


# Defines how the runtime should interact with tools.
# - "off": tools are never used, even if a tools_agent is provided.
# - "auto": runtime may decide to call tools when appropriate.
# - "required": runtime must use tools to answer the request.
ToolChoiceMode = Literal["off", "auto", "required"]


class ToolsContextScope(str, Enum):
    CURRENT_MESSAGE_ONLY = "current_message_only"
    
    CONVERSATION = "conversation"
    
    FULL = "full"


class StepPlanningStrategy(str, Enum):
    OFF = "off"
    STATIC_PLAN = "static_plan"
    DYNAMIC_LOOP = "dynamic_loop"


@dataclass
class RuntimeConfig:
    """
    Global configuration object for the Drop-In Knowledge Runtime.

    This configuration defines:
      - Which LLM is used for generation.
      - How RAG (vectorstore-based retrieval) is applied.
      - Whether web search is available as an additional context source.
      - Whether a tools agent (for function/tool calling) can be used.

    The runtime is backend-agnostic and only depends on the abstract
    interfaces defined in the Intergrax framework.
    """

    # ------------------------------------------------------------------
    # CORE MODEL & RAG BACKENDS
    # ------------------------------------------------------------------

    # Primary LLM adapter used for chat-style generation.
    llm_adapter: LLMAdapter

    # Embedding manager used for RAG/document indexing and retrieval.
    embedding_manager: Optional[EmbeddingManager] = None

    # Vectorstore manager providing semantic search over stored chunks.
    vectorstore_manager: Optional[VectorstoreManager] = None

    # ------------------------------------------------------------------
    # FEATURE FLAGS
    # ------------------------------------------------------------------

    # Enables Retrieval-Augmented Generation based on stored documents.
    enable_rag: bool = True

    # Enables real-time web search as an additional context layer.
    enable_websearch: bool = True
    

    # ------------------------------------------------------------------
    # MULTI-TENANCY
    # ------------------------------------------------------------------

    tenant_id: Optional[str] = None
    workspace_id: Optional[str] = None

    # ------------------------------------------------------------------
    # RAG CONFIGURATION
    # ------------------------------------------------------------------

    # Maximum number of retrieved chunks per query.
    max_docs_per_query: int = 8

    # Maximum token budget reserved for RAG content.
    max_rag_tokens: int = 4096

    # Optional semantic score threshold for filtering low-quality hits.
    rag_score_threshold: Optional[float] = None


    # ------------------------------------------------------------------
    # LONG-TERM MEMORY (USER) RETRIEVAL CONFIGURATION
    # ------------------------------------------------------------------

    # Maximum number of long-term memory entries retrieved per query.
    max_longterm_entries_per_query: int = 8

    # Maximum token budget reserved for long-term memory context.
    max_longterm_tokens: int = 4096

    # Optional semantic score threshold for filtering low-quality long-term hits.
    longterm_score_threshold: Optional[float] = None


    # ------------------------------------------------------------------
    # WEB SEARCH CONFIGURATION
    # ------------------------------------------------------------------

    # Pre-configured executor capable of performing web search queries.
    # If None, web search is effectively unavailable.
    websearch_executor: Optional[WebSearchExecutor] = None

    websearch_config: Optional[WebSearchConfig] = None

    # ------------------------------------------------------------------
    # TOOLS / AGENT EXECUTION
    # ------------------------------------------------------------------

    # Optional tools agent responsible for:
    #   - planning tool calls,
    #   - invoking tools,
    #   - merging tool results into the final answer.
    #
    # If None, tools cannot be used regardless of tools_mode.
    tools_agent: Optional[ToolsAgent] = None

    # High-level policy defining whether tools may or must be used:
    #   - "off": do not use tools at all.
    #   - "auto": runtime may call tools if useful.
    #   - "required": runtime must use at least one tool.
    tools_mode: ToolChoiceMode = "auto"

    # Determines how much contextual information the tools agent receives:
    #
    #   - "current_message_only":
    #       ToolsAgent sees only the newest user query.
    #       Useful for strict function-calling, cost optimization
    #       and predictable single-turn behavior.
    #
    #   - "conversation":
    #       ToolsAgent sees full conversation history up to this point.
    #
    #   - "full":
    #       ToolsAgent receives the same context as the LLM:
    #       system → profile → history → RAG → websearch.
    #
    tools_context_scope: ToolsContextScope = ToolsContextScope.CURRENT_MESSAGE_ONLY


    # Memory toggles
    enable_user_profile_memory: bool = True
    enable_org_profile_memory: bool = True
    enable_user_longterm_memory: bool = True

    # ------------------------------------------------------------------
    # MISC METADATA
    # ------------------------------------------------------------------

    # Arbitrary metadata for app-specific instrumentation or tags.
    metadata: Dict[str, Any] = field(default_factory=dict)



    # ------------------------------------------------------------------
    # DIAGNOSTICS
    # ------------------------------------------------------------------
    enable_llm_usage_collection: bool = True


    # ------------------------------------------------------------------
    # PLANNING
    # ------------------------------------------------------------------
    step_planning_strategy: StepPlanningStrategy = StepPlanningStrategy.OFF

    step_planner_cfg: Optional[StepPlannerConfig]

    step_executor_cfg: Optional[StepExecutorConfig]
    
    planner_prompt_config: Optional[PlannerPromptConfig]


    # ------------------------------------------------------------------
    # VALIDATION
    # ------------------------------------------------------------------
    def validate(self) -> None:
        """
        Validates config consistency. Keeps the runtime fail-fast and predictable.
        """
        if self.enable_rag:
            if self.embedding_manager is None or self.vectorstore_manager is None:
                raise ValueError(
                    "enable_rag=True requires embedding_manager and vectorstore_manager."
                )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/context/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.context
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/context/context_builder.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.context.context_builder
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=context_builder.py
# LINES: 485
# SHA256: 2cca85dc5dcbde6059a625ff220ac733db5b694ea433527fe9f57be923ab01e6
# SYMBOLS:
#   - class RetrievedChunk
#   - class BuiltContext
#   - class ContextBuilder
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

"""
Context builder for Drop-In Knowledge Mode.

This module is responsible for:
- Deciding whether RAG should be used for a given request.
- Retrieving relevant document chunks from the vector store for the current session.
- Providing:
    * a RAG-specific system prompt,
    * a list of retrieved chunks,
    * debug metadata for observability.

Design principles:
- ContextBuilder does NOT own or build conversation history.
  Conversation history is managed by SessionStore and composed by the runtime engine.
- ContextBuilder is ignorant of:
    * LLM adapter details,
    * how messages are serialized for OpenAI/Gemini/Claude,
    * how RouteInfo is built.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from intergrax.rag.vectorstore_manager import VectorstoreManager
from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest
from intergrax.runtime.drop_in_knowledge_mode.session.chat_session import ChatSession


@dataclass
class RetrievedChunk:
    """
    Lightweight representation of a single retrieved document chunk.

    This is an internal structure used by Drop-In Knowledge Mode.
    It wraps whatever the underlying vector store returns into a
    stable shape that can be:
    - injected into prompts,
    - exposed in debug traces,
    - later used for citations.
    """

    id: str
    text: str
    metadata: Dict[str, Any]
    score: float


@dataclass
class BuiltContext:
    """
    Result of ContextBuilder.build_context(...).

    This object is consumed by the runtime engine and prompt builders:
    - system_prompt: RAG-related system message for the LLM.
    - history_messages: conversation history built by the runtime / SessionStore.
      ContextBuilder does not build or trim history; it only passes through
      the list it receives from the engine.
    - retrieved_chunks: RAG context (can be serialized into prompt).
    - rag_debug_info: structured debug trace to be surfaced in
      RuntimeAnswer.debug_trace["rag"].
    """
    history_messages: List[ChatMessage]
    retrieved_chunks: List[RetrievedChunk]
    rag_debug_info: Dict[str, Any]


class ContextBuilder:
    """
    Build RAG-related context for Drop-In Knowledge Mode.

    Responsibilities:
    - Decide whether to use RAG for a given (session, request).
    - Retrieve relevant document chunks from the vector store using
      session/user/tenant/workspace metadata.

    This class does NOT:
    - build or trim conversation history,
    - know anything about tools,
    - know anything about user/organization profiles.
    """

    def __init__(
        self,
        config: RuntimeConfig,
        vectorstore_manager: VectorstoreManager,
        *,
        collection_name: Optional[str] = None,
    ) -> None:
        """
        Args:
            config: Drop-In Knowledge Mode runtime configuration.
            vectorstore_manager: Shared vector store manager instance.
            collection_name: Optional explicit collection/index name.
                If None, the manager's default collection should be used.
        """
        self._config = config
        self._vectorstore = vectorstore_manager
        self._collection_name = collection_name

    
    async def build_context(
        self,
        session: ChatSession,
        request: RuntimeRequest,
        base_history: List[ChatMessage]
    ) -> BuiltContext:
        """
        High-level orchestration method.

        Steps:
        1. Receive base conversation history (already built/reduced by the runtime).
        2. Decide whether RAG should be used for this request.
        3. If yes, retrieve document chunks from the vector store.
        4. Compose a RAG-specific system prompt (for now: DEFAULT_SYSTEM_PROMPT).
        5. Return BuiltContext with:
            - system_prompt,
            - reduced history_messages,
            - retrieved_chunks,
            - structured RAG debug info.

        Important:
        - Conversation history comes from the ChatSession, which is populated
          by SessionStore. ContextBuilder does NOT own any persistence layer.
        """

        # 1. Decide whether we should use RAG for this request
        use_rag, rag_reason = self._should_use_rag(session, request)

        if use_rag:
            retrieved_chunks, rag_debug_info = self._retrieve_for_session(session, request)
        else:
            # No RAG for this request – keep debug info explicit so it is easy
            # to see in RuntimeAnswer.debug_trace why RAG was skipped.
            retrieved_chunks = []
            rag_debug_info = {
                "enabled": bool(self._config.enable_rag),
                "used": False,
                "reason": rag_reason,
                "hits_count": 0,
                "where_filter": {},
                "top_k": int(self._config.max_docs_per_query),
                "score_threshold": self._config.rag_score_threshold,
                "hits": [],
            }

        return BuiltContext(
            history_messages=list(base_history or []),
            retrieved_chunks=retrieved_chunks,
            rag_debug_info=rag_debug_info,
        )


    # -------------------------------------------------------------------------
    # Internal helpers
    # -------------------------------------------------------------------------

    def _should_use_rag(
        self,
        session: ChatSession,
        request: RuntimeRequest,
    ) -> Tuple[bool, str]:
        """
        Decide whether to use RAG for this request.

        Current policy (intentionally simple and predictable):

        - If RAG is disabled in the runtime config -> do not use it.
        - If RAG is enabled -> always query the vector store.

        Whether any chunks are actually retrieved depends on vector store
        contents and metadata filters.

        More sophisticated heuristics (e.g. based on attachments or message type)
        can be added later without changing the engine API.
        """
        if not self._config.enable_rag:
            return False, "rag_disabled_in_config"

        return True, "rag_enabled_in_config"

    
    def _retrieve_for_session(
        self,
        session: ChatSession,
        request: RuntimeRequest,
    ) -> Tuple[List[RetrievedChunk], Dict[str, Any]]:
        """
        Perform a vector store query for this session.

        Strategy:
        - Build a logical `where` filter using:
            * session_id
            * user_id
            * tenant_id
            * workspace_id
          (optionally you can extend this in the future with additional
           filters derived from request.metadata or attachments).
        - Use `request.message` as the query text.
        - Compute query embeddings via the configured embedding manager.
        - Call `IntergraxVectorstoreManager.query(...)` with `query_embeddings`.
        """
        # 1) Build the logical `where` based on session and request metadata        
        query_text = request.message
        query_text = str(query_text or "")

        # Base metadata filters – this keeps all chunks that belong to this
        # logical conversation scope (session/user/tenant/workspace).
        where: Dict[str, Any] = {}
        for attr in ("id", "user_id", "tenant_id", "workspace_id"):            
            value = getattr(session, attr) if hasattr(session, attr) else None
            if value is not None:
                # We normalize "id" to "session_id" for clarity in the metadata.
                if attr == "id":
                    where["session_id"] = value
                else:
                    where[attr] = value

        # NOTE:
        # We intentionally do NOT filter by a single attachment_id here, because
        # RuntimeRequest currently exposes attachments as a list[AttachmentRef],
        # not as a single "attachment_id". At this stage we want to retrieve
        # all chunks for the given session/user/tenant/workspace.
        #
        # In the future, when the attachment model is fully stabilized, you can
        # extend this method to support additional scoping such as:
        # - "only chunks for the last uploaded attachment",
        # - "only chunks for a specific AttachmentRef.id",
        # based on request.attachments or request.metadata.

        max_docs: int = int(self._config.max_docs_per_query)
        score_threshold: Optional[float] = self._config.rag_score_threshold

        # Translate logical `where` into backend-specific filter
        backend_where = self._build_backend_where(where)

        # 2) Get embedding manager from runtime config
        embedding_manager = self._config.embedding_manager
        if embedding_manager is None:
            # Without an embedding manager we cannot perform semantic search.
            # We return an empty result with a clear diagnostic reason.
            return [], {
                "enabled": self._config.enable_rag,
                "used": False,
                "reason": "no_embedding_manager_in_config",
                "where_filter": where,
                "top_k": max_docs,
                "score_threshold": score_threshold,
                "hits": [],
            }

        # 3) Compute query embeddings using IntergraxEmbeddingManager API
        # Preferred path: single-text embedding
        try:
            query_embeddings = embedding_manager.embed_one(query_text)
        except Exception:
            # Fallback: some providers might only support batch embedding
            query_embeddings = embedding_manager.embed_texts([query_text])

        # Normalize embeddings shape for vector store:
        # - numpy array: ensure 2D
        # - plain list: wrap 1D into batch-of-1
        if hasattr(query_embeddings, "ndim"):
            try:
                if query_embeddings.ndim == 1:
                    query_embeddings = query_embeddings.reshape(1, -1)
            except Exception:
                pass
        else:
            if isinstance(query_embeddings, (list, tuple)) and query_embeddings:
                first = query_embeddings[0]
                if isinstance(first, (float, int)):
                    # 1D -> wrap into batch
                    query_embeddings = [query_embeddings]

        # 4) Call vector store with embeddings + backend_where
        hits_dict = self._vectorstore.query(
            query_embeddings=query_embeddings,
            top_k=max_docs,
            where=backend_where,
            include_embeddings=False,
        )

        # 5) Normalize hits into RetrievedChunk objects
        retrieved_chunks = self._map_hits_to_chunks(hits_dict)

        # Apply score_threshold as an extra safety net
        if score_threshold is not None:
            filtered_chunks: List[RetrievedChunk] = []
            for ch in retrieved_chunks:
                if ch.score >= score_threshold:
                    filtered_chunks.append(ch)
            retrieved_chunks = filtered_chunks

        # 6) Build RAG debug info (backend-agnostic view)
        rag_used = bool(retrieved_chunks)

        rag_debug_info: Dict[str, Any] = {
            "enabled": bool(self._config.enable_rag),
            "used": rag_used,
            "hits_count": len(retrieved_chunks or []),
            "where_filter": where,
            "top_k": max_docs,
            "score_threshold": score_threshold,
        }

        # Only store full hit metadata if something was actually retrieved
        if rag_used:
            rag_debug_info["hits"] = [
                {
                    "id": ch.id,
                    "score": round(ch.score, 4),
                    "metadata": ch.metadata,
                    "preview": ch.text[:200],
                }
                for ch in retrieved_chunks
            ]
        else:
            rag_debug_info["hits"] = []

        return retrieved_chunks, rag_debug_info


    def _build_backend_where(self, where: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Translate a simple metadata dict into a backend-compatible `where` filter.

        For a Chroma-like backend we produce:

            {
                "$and": [
                    {"session_id": {"$eq": "..."}},
                    {"user_id": {"$eq": "..."}},
                    ...
                ]
            }

        If the input dict is empty, returns None (no filter).
        """
        if not where:
            return None

        conditions: List[Dict[str, Any]] = []
        for key, value in where.items():
            if value is None:
                continue
            conditions.append({key: {"$eq": value}})

        if not conditions:
            return None

        return {"$and": conditions}

    def _map_hits_to_chunks(self, hits: Any) -> List[RetrievedChunk]:
        """
        Normalize raw hits from the vector store into RetrievedChunk objects.

        Supported patterns:
        - Dict with parallel lists (Chroma-style).
        - Dict with a `matches` list.
        - Flat list or list-of-lists of dict-like objects.
        """
        if not hits:
            return []

        if isinstance(hits, dict):
            if "matches" in hits and isinstance(hits["matches"], list):
                flat_hits = hits["matches"]
            else:
                docs = (
                    hits.get("documents")
                    or hits.get("texts")
                    or hits.get("contents")
                )
                metas = hits.get("metadatas") or hits.get("metadata")
                scores = hits.get("scores") or hits.get("distances")
                ids = hits.get("ids") or hits.get("id")

                if docs is None:
                    return []

                if isinstance(docs, list) and docs and isinstance(docs[0], list):
                    docs_list = docs[0]
                    metas_list = metas[0] if isinstance(metas, list) and metas else metas
                    scores_list = scores[0] if isinstance(scores, list) and scores else scores
                    ids_list = ids[0] if isinstance(ids, list) and ids else ids
                else:
                    docs_list = docs
                    metas_list = metas
                    scores_list = scores
                    ids_list = ids

                flat_hits = []
                n = len(docs_list)

                for i in range(n):
                    doc_text = docs_list[i]

                    if isinstance(metas_list, (list, tuple)) and i < len(metas_list):
                        meta_i = metas_list[i]
                    else:
                        meta_i = metas_list or {}

                    if isinstance(scores_list, (list, tuple)) and i < len(scores_list):
                        score_i = scores_list[i]
                    else:
                        score_i = scores_list

                    if isinstance(ids_list, (list, tuple)) and i < len(ids_list):
                        id_i = ids_list[i]
                    else:
                        id_i = ids_list

                    flat_hits.append(
                        {
                            "id": id_i,
                            "text": doc_text,
                            "metadata": meta_i,
                            "distance": score_i,
                        }
                    )
        else:
            if isinstance(hits, list) and hits and isinstance(hits[0], list):
                flat_hits = hits[0]
            else:
                flat_hits = hits

        chunks: List[RetrievedChunk] = []

        for raw in flat_hits:
            if not isinstance(raw, dict):
                raw_dict = raw.__dict__ if hasattr(raw, "__dict__") else {}
            else:
                raw_dict = raw

            metadata = raw_dict.get("metadata") or raw_dict.get("meta") or {}
            if not isinstance(metadata, dict):
                metadata = {"_raw_metadata": metadata}

            raw_id = (
                raw_dict.get("id")
                or raw_dict.get("doc_id")
                or metadata.get("id")
                or metadata.get("doc_id")
                or "unknown"
            )

            raw_text = (
                raw_dict.get("text")
                or raw_dict.get("page_content")
                or raw_dict.get("content")
                or ""
            )

            raw_score = raw_dict.get("score")
            if raw_score is None:
                raw_score = raw_dict.get("distance")
                if raw_score is not None:
                    try:
                        raw_score = 1.0 / (1.0 + float(raw_score))
                    except Exception:
                        raw_score = 0.0

            try:
                score = float(raw_score) if raw_score is not None else 0.0
            except Exception:
                score = 0.0

            chunks.append(
                RetrievedChunk(
                    id=str(raw_id),
                    text=str(raw_text),
                    metadata=metadata,
                    score=score,
                )
            )

        return chunks

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/context/engine_history_layer.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.context.engine_history_layer
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=engine_history_layer.py
# LINES: 565
# SHA256: d4c1a0d530cde40a72a9acc62f8f12ea70908b12faafc9a286c359369669a274
# SYMBOLS:
#   - class HistoryCompressionResult
#   - class HistoryLayer
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.prompts.history_prompt_builder import HistorySummaryPromptBuilder
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import HistoryCompressionStrategy, RuntimeRequest
from intergrax.runtime.drop_in_knowledge_mode.session.chat_session import ChatSession
from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager


@dataclass
class HistoryCompressionResult:
    """
    Result of applying a history compression strategy.

    This object groups both the compressed messages (the actual base history
    to be sent to the LLM) and all diagnostic / bookkeeping information
    that is useful for debugging and telemetry.
    """

    # Final history that should be used as the base conversation context.
    history: List[ChatMessage]

    # Whether any truncation or summarization was applied.
    truncated: bool

    # Strategy that was actually used. This may differ from the requested
    # strategy in case of fallbacks (e.g. summarization failing and falling
    # back to pure truncation).
    effective_strategy: HistoryCompressionStrategy

    # Whether a summarization step was successfully used.
    summary_used: bool

    # Token budgets used during compression. These are best-effort diagnostic
    # values and may be zero if the strategy did not rely on them.
    summary_tokens_budget: int
    tail_tokens_budget: int

    # Raw metrics for the original history.
    raw_history_messages: int
    raw_history_tokens: Optional[int]

    # Budget that was passed into the compressor for the history.
    history_budget_tokens: Optional[int]

class HistoryLayer:
    
    def __init__(
        self,
        config: RuntimeConfig,
        session_manager: SessionManager,
        history_prompt_builder: HistorySummaryPromptBuilder,
    ) -> None:
        """
        HistoryLayer encapsulates all logic related to:

          - loading raw conversation history,
          - counting tokens,
          - applying history compression strategies,
          - updating RuntimeState with base_history and debug info.
        """
        self._config = config
        self._session_manager = session_manager
        self._history_prompt_builder = history_prompt_builder

    
    async def build_base_history(self, state: RuntimeState) -> None:
        """
        Load and preprocess the conversation history for the current session.

        This step is the single place where we:
          - fetch the full session history from SessionStore,
          - compute token usage (if the adapter supports it),
          - apply token-based truncation according to the per-request
            history compression strategy.

        The resulting `state.base_history` is treated as the canonical,
        preprocessed conversation history for all subsequent steps.
        """
        session = state.session
        assert session is not None, "Session must be set before building history."

        # 1. Load raw history from SessionStore.
        raw_history: List[ChatMessage] = await self._build_chat_history(session)

        # 2. Compute token usage for the raw history, if possible.
        raw_token_count = self._count_tokens_for_messages(raw_history)
        state.history_token_count = raw_token_count

        # 3. Resolve per-request settings.
        request = state.request
        strategy = request.history_compression_strategy
        adapter = self._config.llm_adapter

        # If we cannot count tokens at all, we cannot apply token-based
        # trimming. In that case we simply keep the full history and log
        # what we know in a unified way.
        if raw_token_count is None:
            compression_result = HistoryCompressionResult(
                history=raw_history,
                truncated=False,
                effective_strategy=strategy,
                summary_used=False,
                summary_tokens_budget=0,
                tail_tokens_budget=0,
                raw_history_messages=len(raw_history),
                raw_history_tokens=None,
                history_budget_tokens=None,
            )

            state.base_history = compression_result.history
            state.debug_trace["base_history_length"] = len(compression_result.history)
            state.debug_trace["history_tokens"] = self._build_history_debug_trace(
                requested_strategy=strategy,
                compression_result=compression_result,
            )
            return

        # 4. Compute a token budget for history based on:
        #    - the model context window,
        #    - the requested max_output_tokens (if any).
        #
        # We use a simple, conservative heuristic:
        #   - reserve a portion of the context window for the model output,
        #   - reserve a portion of the remaining input for system instructions,
        #     memory, RAG, websearch, tools, etc.
        #   - whatever remains is the history budget.
        context_window = adapter.context_window_tokens

        # Determine how many tokens we should reserve for the output.
        # If the user does not specify max_output_tokens, we assume
        # roughly 1/4 of the context window is available for the output.
        if request.max_output_tokens is not None:
            reserved_for_output = request.max_output_tokens
            # Never reserve more than half of the context window for output.
            if reserved_for_output > context_window // 2:
                reserved_for_output = context_window // 2
        else:
            reserved_for_output = context_window // 4

        if reserved_for_output < 0:
            reserved_for_output = 0
        if reserved_for_output >= context_window:
            # Degenerate case – leave at least some room for input.
            reserved_for_output = context_window // 2

        # Budget for the entire input (system + history + RAG + tools...).
        input_budget = context_window - reserved_for_output

        if input_budget <= 0:
            # Extremely small or misconfigured budget; in this case we keep
            # the history as-is and log the situation in a unified way.
            compression_result = HistoryCompressionResult(
                history=raw_history,
                truncated=False,
                effective_strategy=strategy,
                summary_used=False,
                summary_tokens_budget=0,
                tail_tokens_budget=0,
                raw_history_messages=len(raw_history),
                raw_history_tokens=raw_token_count,
                history_budget_tokens=0,
            )

            state.base_history = compression_result.history
            state.debug_trace["base_history_length"] = len(compression_result.history)
            state.debug_trace["history_tokens"] = self._build_history_debug_trace(
                requested_strategy=strategy,
                compression_result=compression_result,
            )
            return

        # Reserve a portion of the input budget for non-history input
        # (system instructions, memory, RAG/websearch/tools context).
        # The remaining portion becomes the token budget for history.
        reserved_for_meta = input_budget // 3  # ~1/3 for meta context
        if reserved_for_meta < 0:
            reserved_for_meta = 0
        if reserved_for_meta >= input_budget:
            reserved_for_meta = input_budget // 2

        history_budget_tokens = input_budget - reserved_for_meta

        # 5. Apply history compression strategy.
        compression_result = self._compress_history(
            request=request,
            raw_history=raw_history,
            raw_token_count=raw_token_count,
            strategy=strategy,
            history_budget_tokens=history_budget_tokens,
            run_id=state.run_id,
        )

        state.base_history = compression_result.history

        # 6. Update debug trace with history-related info and token stats.
        state.debug_trace["base_history_length"] = len(compression_result.history)
        state.debug_trace["history_tokens"] = self._build_history_debug_trace(
            requested_strategy=strategy,
            compression_result=compression_result,
        )


    async def _build_chat_history(self, session: ChatSession) -> List[ChatMessage]:
        """
        Load raw conversation history for the given session.

        This method is responsible only for fetching history from SessionStore.
        Any model-specific preprocessing (truncation, summarization, token
        accounting) should happen in `_step_build_base_history`, not here.
        """
        return await self._session_manager.get_history(session_id=session.id)


    def _build_history_debug_trace(
            self,
            *,
            requested_strategy: HistoryCompressionStrategy,
            compression_result: HistoryCompressionResult,
        ) -> dict:
            """
            Build a unified debug trace dictionary for history compression.
            Ensures that all call paths (OFF, no-token, no-budget, full compression)
            produce the exact same set of keys.
            """
            return {
                "raw_history_messages": compression_result.raw_history_messages,
                "raw_history_tokens": compression_result.raw_history_tokens,
                "history_budget_tokens": compression_result.history_budget_tokens,
                "strategy_requested": requested_strategy.value,
                "strategy_effective": compression_result.effective_strategy.value,
                "truncated": compression_result.truncated,
                "summary_used": compression_result.summary_used,
                "summary_tokens_budget": compression_result.summary_tokens_budget,
                "tail_tokens_budget": compression_result.tail_tokens_budget,
            }
    

    def _count_tokens_for_messages(self, messages: List[ChatMessage]) -> Optional[int]:
        """
        Best-effort token counting for a list of ChatMessage objects.

        Design:
          - Delegates to the underlying LLM adapter if it exposes a
            `count_messages_tokens` method.
          - Returns None if no token counter is available or an error occurs.

        Note:
          - We deliberately avoid any dynamic attribute lookup (no getattr),
            to keep the integration surface with the adapter explicit and
            stable.
        """
        adapter = self._config.llm_adapter
        if adapter is None:
            return None

        try:
            # The adapter is expected to implement this method.
            return int(adapter.count_messages_tokens(messages))
        except AttributeError:
            # Adapter does not implement token counting – leave it as None.
            return None
        except Exception:
            # Any other error should not break the runtime; we just skip
            # token accounting in this case.
            return None


    def _truncate_history_by_tokens(
        self,
        messages: List[ChatMessage],
        max_tokens: int,
    ) -> List[ChatMessage]:
        """
        Truncate conversation history to fit within a token budget.

        Strategy:
        - Keep the most recent messages (suffix of the history).
        - Walk the history from the end backwards and accumulate messages
            until the token budget is exhausted.
        - If token counting is not available, this method returns the
            input list unchanged.

        Important:
        - This helper is intentionally conservative; it does NOT attempt to
            summarize older messages, it only drops them.
        - Summarization-based compression is implemented on top of this
            function in `_compress_history`.
        """
        # Degenerate cases – no budget or empty history.
        if max_tokens is None or max_tokens <= 0:
            return []

        if not messages:
            return []

        # If we cannot count tokens at all, we cannot safely truncate.
        # In that case we keep the history as-is and let the caller decide
        # how to handle potential context overflow.
        if self._count_tokens_for_messages(messages) is None:
            return messages

        truncated: List[ChatMessage] = []

        # Walk from the end (most recent) to the beginning.
        # We always keep a suffix of the conversation in chronological order.
        for msg in reversed(messages):
            # Candidate history if we include this message at the front
            # of the already-kept suffix.
            candidate = [msg] + truncated
            candidate_tokens = self._count_tokens_for_messages(candidate)
            if candidate_tokens is None:
                # If counting suddenly fails, bail out and keep what we have.
                break

            if candidate_tokens > max_tokens:
                # Adding this message would exceed the budget; stop here.
                break

            # Safe to include – prepend to keep chronological order.
            truncated.insert(0, msg)

        # If we ended up with an empty truncated list (e.g. a single message
        # already exceeds the budget), we at least keep the last message.
        if not truncated and messages:
            truncated = [messages[-1]]

        return truncated
    
    
    def _compress_history(
        self,
        *,
        request: RuntimeRequest,
        raw_history: List[ChatMessage],
        raw_token_count: Optional[int],
        strategy: HistoryCompressionStrategy,
        history_budget_tokens: int,
        run_id: Optional[str]=None,
    ) -> HistoryCompressionResult:
        """
        Apply the configured history compression strategy to the raw history
        and return a structured result object with both the final history
        and diagnostic metadata.
        """
        # Defaults for the result – will be updated below.
        effective_strategy = strategy
        truncated = False
        summary_used = False
        summary_tokens_budget = 0
        tail_tokens_budget = 0

        raw_len = len(raw_history)

        # Helper to build the result object in one place.
        def _build_result(history: List[ChatMessage]) -> HistoryCompressionResult:
            return HistoryCompressionResult(
                history=history,
                truncated=truncated,
                effective_strategy=effective_strategy,
                summary_used=summary_used,
                summary_tokens_budget=summary_tokens_budget,
                tail_tokens_budget=tail_tokens_budget,
                raw_history_messages=raw_len,
                raw_history_tokens=raw_token_count,
                history_budget_tokens=history_budget_tokens,
            )

        # 0) OFF → do not touch the history at all.
        if strategy == HistoryCompressionStrategy.OFF:
            effective_strategy = HistoryCompressionStrategy.OFF
            return _build_result(raw_history)

        # 1) If we have no token info or a non-positive budget, we cannot
        # meaningfully compress the history. Keep it as-is.
        if raw_token_count is None or history_budget_tokens <= 0:
            # We keep the requested strategy in effective_strategy for
            # diagnostic purposes, but we do not modify the history.
            return _build_result(raw_history)

        # 2) If history already fits into the budget -> nothing to do.
        if raw_token_count <= history_budget_tokens:
            return _build_result(raw_history)

        # 3) Pure truncation strategy.
        if strategy == HistoryCompressionStrategy.TRUNCATE_OLDEST:
            compressed = self._truncate_history_by_tokens(
                messages=raw_history,
                max_tokens=history_budget_tokens,
            )
            truncated = True
            effective_strategy = HistoryCompressionStrategy.TRUNCATE_OLDEST
            tail_tokens_budget = history_budget_tokens
            return _build_result(compressed)

        # 4) Summarization-based strategies.
        if strategy in (
            HistoryCompressionStrategy.SUMMARIZE_OLDEST,
            HistoryCompressionStrategy.HYBRID,
        ):
            # If the budget is extremely small, summarization will not be
            # very helpful. Fall back to pure truncation.
            if history_budget_tokens <= 64:
                compressed = self._truncate_history_by_tokens(
                    messages=raw_history,
                    max_tokens=history_budget_tokens,
                )
                truncated = True
                effective_strategy = HistoryCompressionStrategy.TRUNCATE_OLDEST
                tail_tokens_budget = history_budget_tokens
                return _build_result(compressed)

            # Basic split of the budget between summary and tail.
            summary_max_tokens = max(
                32,
                min(history_budget_tokens // 4, 256),
            )
            tail_budget = history_budget_tokens - summary_max_tokens

            if tail_budget <= 32:
                tail_budget = max(32, history_budget_tokens // 2)
                summary_max_tokens = history_budget_tokens - tail_budget

            summary_tokens_budget = summary_max_tokens
            tail_tokens_budget = tail_budget

            # Build the most recent tail first.
            tail_messages = self._truncate_history_by_tokens(
                messages=raw_history,
                max_tokens=tail_budget,
            )
            if not tail_messages:
                compressed = self._truncate_history_by_tokens(
                    messages=raw_history,
                    max_tokens=history_budget_tokens,
                )
                truncated = True
                effective_strategy = HistoryCompressionStrategy.TRUNCATE_OLDEST
                tail_tokens_budget = history_budget_tokens
                return _build_result(compressed)

            tail_len = len(tail_messages)
            prefix_len = max(0, len(raw_history) - tail_len)
            older_messages = raw_history[:prefix_len]

            if not older_messages:
                # Nothing older to summarize; we effectively behave like pure
                # truncation here, but we still mark the requested strategy.
                truncated = True
                effective_strategy = strategy
                return _build_result(tail_messages)

            prompt_bundle = self._history_prompt_builder.build_history_summary_prompt(
                request=request,
                strategy=strategy,
                older_messages=older_messages,
                tail_messages=tail_messages,
            )

            summary_msg = self._summarize_history_chunk(
                messages=older_messages,
                max_summary_tokens=summary_max_tokens,
                system_prompt=prompt_bundle.system_prompt,
                run_id=run_id,
            )

            if summary_msg is None:
                # Summarization failed; fall back to truncation.
                compressed = self._truncate_history_by_tokens(
                    messages=raw_history,
                    max_tokens=history_budget_tokens,
                )
                truncated = True
                effective_strategy = HistoryCompressionStrategy.TRUNCATE_OLDEST
                tail_tokens_budget = history_budget_tokens
                summary_tokens_budget = 0
                return _build_result(compressed)

            compressed_history: List[ChatMessage] = [summary_msg]
            compressed_history.extend(tail_messages)

            truncated = True
            summary_used = True
            effective_strategy = strategy

            return _build_result(compressed_history)

        # 5) Unknown strategy -> keep as-is.
        return _build_result(raw_history)



    
    def _summarize_history_chunk(
        self,
        messages: List[ChatMessage],
        max_summary_tokens: int,
        system_prompt: str,
        run_id: Optional[str] = None,
    ) -> Optional[ChatMessage]:
        """
        Summarize a block of older conversation history into a single
        compact system-level message.

        This helper uses the core LLM adapter synchronously. The summary
        message is NOT persisted in the SessionStore; it is meant to be
        injected into the prompt as a synthetic meta-history.
        """
        if not messages:
            return None

        if max_summary_tokens <= 0:
            return None

        adapter = self._config.llm_adapter
        if adapter is None:
            return None

        # Build a simple, robust summarization prompt.
        summary_prompt: List[ChatMessage] = [
            ChatMessage(
                role="system",
                content=system_prompt,
            )
        ]
        summary_prompt.extend(messages)

        generate_kwargs: Dict[str, Any] = {}
        # We keep the summary small and controlled by a separate token budget.
        if max_summary_tokens > 0:
            generate_kwargs["max_tokens"] = max_summary_tokens

        try:
            raw = adapter.generate_messages(
                summary_prompt, 
                run_id=run_id,
                **generate_kwargs)
        except Exception:
            # If summarization fails for any reason, we simply return None
            # and let the caller fall back to truncation.
            return None
        
        if not isinstance(raw, str):
            return None

        if not raw:
            return None
        
        text = raw.strip()

        # We wrap the summary in a system message so that it is clearly
        # separated from user/assistant turns.
        return ChatMessage(
            role="system",
            content=f"Conversation summary (earlier turns):\n{text}",
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.pipelines
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/contract.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.contract
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=contract.py
# LINES: 90
# SHA256: ecdf2240c085fc5cb484d885dafe150155ef4e9fc69b5b53dfb160221833b470
# SYMBOLS:
#   - class RuntimePipeline
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations
from abc import ABC, abstractmethod
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeAnswer

class RuntimePipeline(ABC):
    """
    Lightweight base class for pipeline runners.
    Shared implementation: execute_pipeline.
    """
    
    async def run(self, state: RuntimeState) -> RuntimeAnswer:
        """
        Public entrypoint. Provides shared validation and invariants.
        Subclasses must implement _inner_run().
        """
        self._validate_state(state)

        runtime_answer = await self._inner_run(state)

        if runtime_answer is None:
            raise RuntimeError("Pipeline returned None RuntimeAnswer.")

        # Hard invariant: Persist step (or equivalent) must set state.runtime_answer
        if state.runtime_answer is None:
            raise RuntimeError("Persist step did not set state.runtime_answer.")

        # Prefer the actual object returned by inner_run, but enforce consistency with state
        # (Optional) If you want to hard-enforce object identity:
        # if runtime_answer is not state.runtime_answer: ...
        self._assert_valid_answer(runtime_answer)

        return runtime_answer

    
    @abstractmethod
    async def _inner_run(self, state: RuntimeState) -> RuntimeAnswer:
        """
        Pipeline-specific execution. Must produce and return RuntimeAnswer.
        """
        raise NotImplementedError


    def _validate_state(self, state: RuntimeState) -> None:
        if state is None:
            raise ValueError("state is None.")
        if state.context is None:
            raise ValueError("state.context is None.")
        if state.request is None:
            raise ValueError("state.request is None.")
        if not state.run_id:
            raise ValueError("state.run_id is missing.")
        

    async def _execute_pipeline(self, steps: list[RuntimeStep], state: RuntimeState) -> None:
        for step in steps:
            step_name = step.__class__.__name__
            state.trace_event(
                component="runtime",
                step=step_name,
                message="Step started",
                data={}
            )

            try:
                await step.run(state)
            except Exception as e:
                state.trace_event(
                    component="pipeline",
                    step=step_name,
                    message="Step failed",
                    data={"error": repr(e)},
                )
                raise

            state.trace_event(
                component="runtime",
                step=step_name,
                message="Step finished",
                data={}
            )

    def _assert_valid_answer(self, answer: RuntimeAnswer) -> None:
        assert answer is not None, "Pipeline returned None answer"
        assert answer.route is not None, "RuntimeAnswer has no route"

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/no_planner_pipeline.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.no_planner_pipeline
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=no_planner_pipeline.py
# LINES: 55
# SHA256: 55af7c238de2034de583a55cd6b60e396d1380eb9a5a3b2999a24f1550c031e2
# SYMBOLS:
#   - class NoPlannerPipeline
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.contract import RuntimePipeline
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.build_base_history_step import BuildBaseHistoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.core_llm_step import CoreLLMStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.ensure_current_user_message_step import EnsureCurrentUserMessageStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.history_step import HistoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.retrieve_attachments_step import RetrieveAttachmentsStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.instructions_step import InstructionsStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.persist_and_build_answer_step import PersistAndBuildAnswerStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.profile_based_memory_step import ProfileBasedMemoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.rag_step import RagStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.session_and_ingest_step import SessionAndIngestStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools_step import ToolsStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.user_longterm_memory_step import UserLongtermMemoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.websearch_step import WebsearchStep
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeAnswer


class NoPlannerPipeline(RuntimePipeline):

    async def _inner_run(self, state: RuntimeState) -> RuntimeAnswer:
        steps = [
            SessionAndIngestStep(),
            ProfileBasedMemoryStep(),
            BuildBaseHistoryStep(),
            HistoryStep(),
            InstructionsStep(),

            # Must exist before any step that injects context "before last user".
            EnsureCurrentUserMessageStep(),

            RagStep(),
            UserLongtermMemoryStep(),
            RetrieveAttachmentsStep(),
            WebsearchStep(),

            ToolsStep(),
            CoreLLMStep(),
            PersistAndBuildAnswerStep(),
        ]

        await self._execute_pipeline(steps, state)

        runtime_answer = state.runtime_answer
        if runtime_answer is None:
            raise RuntimeError("Persist step did not set state.runtime_answer.")
        
        return runtime_answer
    

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/pipeline_factory.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.pipeline_factory
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=pipeline_factory.py
# LINES: 60
# SHA256: c64139f8be94be39cb1f659433708ac271e339864536414c2598274cdbf56252
# SYMBOLS:
#   - class PipelineFactory
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations
from typing import Callable, Dict

from intergrax.runtime.drop_in_knowledge_mode.config import StepPlanningStrategy
from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.contract import RuntimePipeline
from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.no_planner_pipeline import NoPlannerPipeline
from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.planner_dynamic_pipeline import PlannerDynamicPipeline
from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.planner_static_pipeline import PlannerStaticPipeline
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.core_llm_step import CoreLLMStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.retrieve_attachments_step import RetrieveAttachmentsStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.persist_and_build_answer_step import PersistAndBuildAnswerStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.rag_step import RagStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools_step import ToolsStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.user_longterm_memory_step import UserLongtermMemoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.websearch_step import WebsearchStep
from intergrax.runtime.drop_in_knowledge_mode.planning.runtime_step_handlers import RuntimeStep, build_runtime_step_registry
from intergrax.runtime.drop_in_knowledge_mode.planning.step_executor_models import StepHandlerRegistry
from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import StepAction


class PipelineFactory:

    @classmethod
    def build_pipeline(cls, state: RuntimeState) -> RuntimePipeline:

        if state.context.config.step_planning_strategy == StepPlanningStrategy.OFF:
            return NoPlannerPipeline()
        
        if state.context.config.step_planning_strategy == StepPlanningStrategy.STATIC_PLAN:
            return PlannerStaticPipeline()
        
        if state.context.config.step_planning_strategy == StepPlanningStrategy.DYNAMIC_LOOP:
            return PlannerDynamicPipeline()
        
        raise ValueError(f"Unknown step_planning_strategy: {state.context.config.step_planning_strategy}")
    

    @classmethod
    def build_default_planning_step_registry(cls) -> StepHandlerRegistry:
        """
        Default registry for StepExecutor planning actions (STATIC/DYNAMIC).
        This registry is explicit and production-safe (no reflection).
        """
        bindings: Dict[StepAction, Callable[[], RuntimeStep]] = {
            StepAction.USE_WEBSEARCH: lambda: WebsearchStep(),
            StepAction.USE_TOOLS: lambda: ToolsStep(),
            StepAction.USE_RAG_RETRIEVAL: lambda: RagStep(),
            StepAction.USE_ATTACHMENTS_RETRIEVAL: lambda: RetrieveAttachmentsStep(),
            StepAction.USE_USER_LONGTERM_MEMORY_SEARCH: lambda: UserLongtermMemoryStep(),
            StepAction.SYNTHESIZE_DRAFT: lambda: CoreLLMStep(),
            StepAction.VERIFY_ANSWER: lambda: CoreLLMStep(),
            StepAction.FINALIZE_ANSWER: lambda: PersistAndBuildAnswerStep(),            
        }

        return build_runtime_step_registry(bindings=bindings)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/planner_dynamic_pipeline.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.planner_dynamic_pipeline
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=planner_dynamic_pipeline.py
# LINES: 18
# SHA256: 7f1bc8b3779d12038916765c45489fe199e9b48f1bd847edebec57efd8feaa05
# SYMBOLS:
#   - class PlannerDynamicPipeline
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.contract import RuntimePipeline
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeAnswer


class PlannerDynamicPipeline(RuntimePipeline):

    async def _inner_run(self, state: RuntimeState) -> RuntimeAnswer:
        raise NotImplementedError(
            "StepPlanningStrategy.DYNAMIC_LOOP is configured, but step planner is not implemented."
        ) 
    

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/pipelines/planner_static_pipeline.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.planner_static_pipeline
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=planner_static_pipeline.py
# LINES: 82
# SHA256: 2bf2a082521ac6311a0264c42d87042634e797c08b6f57ef2e4b43f5231fda10
# SYMBOLS:
#   - class PlannerStaticPipeline
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.contract import RuntimePipeline
from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.pipeline_factory import PipelineFactory
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.planning.engine_planner import EnginePlanner
from intergrax.runtime.drop_in_knowledge_mode.planning.step_executor_models import PlanExecutionReport
from intergrax.runtime.drop_in_knowledge_mode.planning.step_planner import StepPlanner
from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import PlanBuildMode
from intergrax.runtime.drop_in_knowledge_mode.planning.step_executor import StepExecutor
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeAnswer

# Setup steps (outside planner)
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.session_and_ingest_step import SessionAndIngestStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.profile_based_memory_step import ProfileBasedMemoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.build_base_history_step import BuildBaseHistoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.history_step import HistoryStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.instructions_step import InstructionsStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.ensure_current_user_message_step import EnsureCurrentUserMessageStep


class PlannerStaticPipeline(RuntimePipeline):
    """
    STATIC plan pipeline:
      - deterministic setup outside planner (no LLM planning)
      - EnginePlanner -> StepPlanner(STATIC) -> StepExecutor(registry)
      - runtime_answer MUST be produced by FINALIZE_ANSWER handler (PersistAndBuildAnswerStep)
    """

    async def _inner_run(self, state: RuntimeState) -> RuntimeAnswer:
        # 1) Deterministic setup outside planner
        setup_steps = [
            SessionAndIngestStep(),
            ProfileBasedMemoryStep(),
            BuildBaseHistoryStep(),
            HistoryStep(),
            InstructionsStep(),
            EnsureCurrentUserMessageStep(),
        ]
        await self._execute_pipeline(setup_steps, state)

        # 2) Build components
        ctx = state.context
        cfg = ctx.config
        req = state.request

        if cfg.llm_adapter is None:
            raise RuntimeError("PlannerStaticPipeline: config.llm_adapter is required for EnginePlanner.")

        engine_planner = EnginePlanner(llm_adapter=cfg.llm_adapter)
        step_planner = StepPlanner(cfg.step_planner_cfg)

        registry = PipelineFactory.build_default_planning_step_registry()
        step_executor = StepExecutor(registry=registry, cfg=cfg.step_executor_cfg)

        # 3) Engine plan (LLM)
        engine_plan = await engine_planner.plan(
            req=req,
            state=state,
            config=cfg,
            prompt_config=cfg.planner_prompt_config,
            run_id=state.run_id,
        )

        # 4) Step plan (deterministic) + execute
        exec_plan = step_planner.build_from_engine_plan(
            user_message=req.message or "",
            engine_plan=engine_plan,
            plan_id=f"static-{state.run_id}",
            build_mode=PlanBuildMode.STATIC,
        )

        executed_plan: PlanExecutionReport = await step_executor.execute(plan=exec_plan, state=state)

        # 5) Enforce invariant: FINALIZE step must set runtime_answer
        if state.runtime_answer is None:
            raise RuntimeError("PlannerStaticPipeline: runtime_answer is not set. FINALIZE_ANSWER step failed or missing.")

        return state.runtime_answer

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=runtime.py
# LINES: 145
# SHA256: 66c268d64f6f4b0fe24027c9e84a3bf7b3730583845e47325984ca5aac498fd1
# SYMBOLS:
#   - class RuntimeEngine
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

"""
Core runtime engine for Drop-In Knowledge Mode.

This module defines the `DropInKnowledgeRuntime` class, which:
  - loads or creates chat sessions,
  - appends user messages,
  - builds a conversation history for the LLM,
  - augments context with RAG, web search and tools,
  - produces a `RuntimeAnswer` object as a high-level response.

The goal is to provide a single, simple entrypoint that can be used from
FastAPI, Streamlit, MCP-like environments, CLI tools, etc.

Refactored as a stateful pipeline:

  - RuntimeState holds all intermediate data (session, history, flags, debug).
  - Each step mutates the state and can be inspected in isolation.
  - ask() just wires the steps together in a readable order.
"""

from __future__ import annotations

import asyncio
import uuid

from intergrax.llm_adapters.llm_usage_track import LLMUsageTracker
from intergrax.runtime.drop_in_knowledge_mode.engine.pipelines.pipeline_factory import PipelineFactory
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_context import RuntimeContext
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import (
    RuntimeRequest,
    RuntimeAnswer,
)
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState


# ----------------------------------------------------------------------
# RuntimeEngine
# ----------------------------------------------------------------------


class RuntimeEngine:
    """
    High-level conversational runtime for the Intergrax framework.

    This class is designed to behave like a ChatGPT/Claude-style engine,
    but fully powered by Intergrax components (LLM adapters, RAG, web search,
    tools, memory, etc.).

    Responsibilities (current stage):
      - Accept a RuntimeRequest.
      - Load or create a ChatSession via SessionManager.
      - Append the user message to the session.
      - Build an LLM-ready context:
          * system prompt(s),
          * chat history from SessionManager,
          * optional retrieved chunks from documents (RAG),
          * optional web search context (if enabled),
          * optional tools results.
      - Call the main LLM adapter once with the fully enriched context
        to produce the final answer.
      - Append the assistant message to the session.
      - Return a RuntimeAnswer with the final answer text and metadata.
    """

    def __init__(
        self,
        context: RuntimeContext
    ) -> None:
        self.context = context


    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def run(self, request: RuntimeRequest) -> RuntimeAnswer:
        """
        Main async entrypoint for the runtime.
        """

        run_id = f"run_{uuid.uuid4().hex}"

        state = RuntimeState(
            context=self.context,
            request=request,
            run_id=run_id,
            llm_usage_tracker=LLMUsageTracker(run_id=run_id)
        )        
        
        state.configure_llm_tracker()

        # Initial trace entry for this request.
        state.trace_event(
            component="engine",
            step="run_start",
            message="RuntimeEngine.run() called.",
            data={
                "session_id": request.session_id,
                "user_id": request.user_id,
                "tenant_id": request.tenant_id or self.context.config.tenant_id,
                "run_id": state.run_id,
                "step_planning_strategy": str(self.context.config.step_planning_strategy),
            },
        )
        
        try:
            pipeline = PipelineFactory.build_pipeline(state=state)
            runtime_answer = await pipeline.run(state=state)

            # Final trace entry for this request.
            state.trace_event(
                component="engine",
                step="run_end",
                message="RuntimeEngine.run() finished.",
                data={
                    "strategy": runtime_answer.route.strategy,
                    "used_rag": runtime_answer.route.used_rag,
                    "used_websearch": runtime_answer.route.used_websearch,
                    "used_tools": runtime_answer.route.used_tools,
                    "used_user_longterm_memory": runtime_answer.route.used_user_longterm_memory,
                    "run_id":state.run_id
                },
            )
            
            await state.finalize_llm_tracker(request, runtime_answer)

            return runtime_answer
        
        finally:
            # finalize even on exceptions; only if we have an answer
            if runtime_answer is not None:
                await state.finalize_llm_tracker(request, runtime_answer)
            else:
                # Optional: trace that run aborted before producing answer
                state.trace_event(
                    component="engine",
                    step="run_abort",
                    message="RuntimeEngine.run() aborted before RuntimeAnswer was produced.",
                    data={"run_id": state.run_id},
                )
    

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_context.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_context
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=runtime_context.py
# LINES: 230
# SHA256: 8c778f777561e959185612ed3b52136df96ae936dba1b251a646546ba645bab8
# SYMBOLS:
#   - class LLMUsageRunRecord
#   - class RuntimeContext
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

import asyncio
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional
from intergrax.llm_adapters.llm_usage_track import LLMUsageReport
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.runtime.drop_in_knowledge_mode.context.context_builder import ContextBuilder
from intergrax.runtime.drop_in_knowledge_mode.context.engine_history_layer import HistoryLayer
from intergrax.runtime.drop_in_knowledge_mode.ingestion.ingestion_service import AttachmentIngestionService
from intergrax.runtime.drop_in_knowledge_mode.prompts.history_prompt_builder import DefaultHistorySummaryPromptBuilder, HistorySummaryPromptBuilder
from intergrax.runtime.drop_in_knowledge_mode.prompts.rag_prompt_builder import DefaultRagPromptBuilder, RagPromptBuilder
from intergrax.runtime.drop_in_knowledge_mode.prompts.user_longterm_memory_prompt_builder import DefaultUserLongTermMemoryPromptBuilder, UserLongTermMemoryPromptBuilder
from intergrax.runtime.drop_in_knowledge_mode.prompts.websearch_prompt_builder import DefaultWebSearchPromptBuilder, WebSearchPromptBuilder
from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager
from intergrax.websearch.service.websearch_executor import WebSearchExecutor


@dataclass(frozen=True)
class LLMUsageRunRecord:
    seq: int
    ts_utc: datetime
    run_id: str
    session_id: str
    user_id: str
    report: LLMUsageReport

    def pretty(self) -> str:
        lines: List[str] = []
        lines.append(f"Run #{self.seq}")
        lines.append(f"  ts_utc     : {self.ts_utc.isoformat()}")
        lines.append(f"  run_id     : {self.run_id}")
        lines.append(f"  session_id : {self.session_id}")
        lines.append(f"  user_id    : {self.user_id}")
        lines.append(self.report.pretty())        

        return "\n".join(lines)

@dataclass(frozen=False)
class RuntimeContext:
    """
    Per-runtime context: resolved dependencies + configuration.

    This object is intended to be:
    - configuration & dependencies are stable; diagnostics mutate
    - reusable in tests (build() can create the same defaults as Runtime.__init__)
    - passed to steps: step.run(state, ctx)

    IMPORTANT:
    - per-request flags/results belong to RuntimeState, not here.
    """

    config: RuntimeConfig
    session_manager: SessionManager

    ingestion_service: Optional[AttachmentIngestionService]
    context_builder: Optional[ContextBuilder]

    rag_prompt_builder: RagPromptBuilder
    user_longterm_memory_prompt_builder: UserLongTermMemoryPromptBuilder

    websearch_executor: Optional[WebSearchExecutor]
    websearch_prompt_builder: Optional[WebSearchPromptBuilder]

    history_prompt_builder: HistorySummaryPromptBuilder
    history_layer: HistoryLayer

    llm_usage_run_seq: int = 0
    llm_usage_runs: List[LLMUsageRunRecord] = field(default_factory=list)
    llm_usage_lock: asyncio.Lock = field(default_factory=asyncio.Lock)


    async def get_llm_usage_runs(self) -> list[LLMUsageRunRecord]:
        async with self.llm_usage_lock:
            return list(self.llm_usage_runs)
            

    async def clear_llm_usage_runs(self) -> None:
        async with self.llm_usage_lock:
            self.llm_usage_runs.clear()
            self.llm_usage_run_seq = 0 


    async def print_usage_runs(self):
        runs = await self.get_llm_usage_runs()
        print("runs:", len(runs))

        # Aggregate totals across all runs
        total_calls = 0
        total_in = 0
        total_out = 0
        total_tokens = 0
        total_ms = 0
        total_errors = 0

        # Aggregate by provider/model string key
        by_key = {}  # key -> dict(calls,in,out,total,ms,err)

        for r in runs:
            # r.total is expected to have these fields (as shown in pretty())
            t = r.report.total
            total_calls += int(t.calls or 0)
            total_in += int(t.input_tokens or 0)
            total_out += int(t.output_tokens or 0)
            total_tokens += int(t.total_tokens or 0)
            total_ms += int(t.duration_ms or 0)
            total_errors += int(t.errors or 0)

            # r.by_provider_model is expected to be iterable of items with key + stats
            # (use exactly what your report object exposes; below assumes dict-like)
            bpm = r.report.by_provider_model

            for k, st in bpm.items():
                agg = by_key.get(k)
                if agg is None:
                    agg = {"calls": 0, "in": 0, "out": 0, "total": 0, "ms": 0, "err": 0}
                    by_key[k] = agg
                agg["calls"] += int(st.calls or 0)
                agg["in"] += int(st.input_tokens or 0)
                agg["out"] += int(st.output_tokens or 0)
                agg["total"] += int(st.total_tokens or 0)
                agg["ms"] += int(st.duration_ms or 0)
                agg["err"] += int(st.errors or 0)

        if runs:
            print("=" * 100)
            print("ALL RUNS (aggregated)")
            print(f"  calls        : {total_calls}")
            print(f"  input_tokens : {total_in}")
            print(f"  output_tokens: {total_out}")
            print(f"  total_tokens : {total_tokens}")
            print(f"  duration_ms  : {total_ms}")
            print(f"  errors       : {total_errors}")

            if by_key:
                print("By provider/model (aggregated):")
                for k, st in by_key.items():
                    print(
                        f"  - {k}: calls={st['calls']} in={st['in']} out={st['out']} "
                        f"total={st['total']} ms={st['ms']} err={st['err']}"
                    )

        for r in runs:
            print("=" * 100)
            print(r.pretty())


    @classmethod
    def build(
        cls,
        *,
        config: RuntimeConfig,
        session_manager: SessionManager,
        ingestion_service: Optional[AttachmentIngestionService] = None,
        context_builder: Optional[ContextBuilder] = None,
        rag_prompt_builder: Optional[RagPromptBuilder] = None,
        user_longterm_memory_prompt_builder: Optional[UserLongTermMemoryPromptBuilder] = None,
        websearch_prompt_builder: Optional[WebSearchPromptBuilder] = None,
        history_prompt_builder: Optional[HistorySummaryPromptBuilder] = None,
    ) -> "RuntimeContext":
        """
        Build a fully-resolved RuntimeContext using the same resolution rules as Runtime.__init__:

        - config.validate()
        - context_builder defaults to ContextBuilder(...) when enable_rag and not provided
        - prompt builders default to their Default* implementations
        - websearch_executor resolved from config if enabled and provided
        - history_layer constructed using resolved history_prompt_builder
        """
        config.validate()

        # Resolve ContextBuilder (RAG)
        resolved_context_builder = context_builder
        if resolved_context_builder is None and config.enable_rag:
            resolved_context_builder = ContextBuilder(
                config=config,
                vectorstore_manager=config.vectorstore_manager,
            )

        # Resolve RAG prompt builder
        resolved_rag_prompt_builder: RagPromptBuilder = (
            rag_prompt_builder or DefaultRagPromptBuilder(config)
        )

        # Resolve user long-term memory prompt builder
        resolved_user_ltm_prompt_builder: UserLongTermMemoryPromptBuilder = (
            user_longterm_memory_prompt_builder
            or DefaultUserLongTermMemoryPromptBuilder(
                max_entries=config.max_longterm_entries_per_query,
                max_chars=int(config.max_longterm_tokens * 4),
            )
        )

        # Resolve websearch executor (from config)
        resolved_websearch_executor: Optional[WebSearchExecutor] = None
        if config.enable_websearch and config.websearch_executor:
            resolved_websearch_executor = config.websearch_executor

        # Resolve websearch prompt builder
        resolved_websearch_prompt_builder: Optional[WebSearchPromptBuilder] = (
            websearch_prompt_builder or DefaultWebSearchPromptBuilder(config)
        )

        # Resolve history prompt builder
        resolved_history_prompt_builder: HistorySummaryPromptBuilder = (
            history_prompt_builder or DefaultHistorySummaryPromptBuilder(config)
        )

        # Build HistoryLayer using resolved builder
        resolved_history_layer = HistoryLayer(
            config=config,
            session_manager=session_manager,
            history_prompt_builder=resolved_history_prompt_builder,
        )

        return cls(
            config=config,
            session_manager=session_manager,
            ingestion_service=ingestion_service,
            context_builder=resolved_context_builder,
            rag_prompt_builder=resolved_rag_prompt_builder,
            user_longterm_memory_prompt_builder=resolved_user_ltm_prompt_builder,
            websearch_executor=resolved_websearch_executor,
            websearch_prompt_builder=resolved_websearch_prompt_builder,
            history_prompt_builder=resolved_history_prompt_builder,
            history_layer=resolved_history_layer,            
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_state.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=runtime_state.py
# LINES: 171
# SHA256: 97c512f6593d49737fa782cad13a54c17161f9f9f2cc3188f802d556b1613f59
# SYMBOLS:
#   - class RuntimeState
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from intergrax.llm.messages import ChatMessage
from intergrax.llm_adapters.llm_usage_track import LLMUsageTracker
from intergrax.runtime.drop_in_knowledge_mode.context.context_builder import BuiltContext
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_context import LLMUsageRunRecord, RuntimeContext
from intergrax.runtime.drop_in_knowledge_mode.ingestion.ingestion_service import IngestionResult
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeAnswer, RuntimeRequest
from intergrax.runtime.drop_in_knowledge_mode.session.chat_session import ChatSession


@dataclass
class RuntimeState:
    """
    Mutable state object passed through the runtime pipeline.

    It aggregates:
      - request and session metadata,
      - ingestion results,
      - conversation history and model-ready messages,
      - flags indicating which subsystems were used (RAG, websearch, tools, memory),
      - tools traces and agent answer,
      - full debug_trace for observability & diagnostics.
    """

    # Context
    context: RuntimeContext

    # Input
    request: RuntimeRequest

    run_id: str

    llm_usage_tracker: LLMUsageTracker

    # Session and ingestion
    session: Optional[ChatSession] = None
    ingestion_results: List[IngestionResult] = field(default_factory=list)
    used_attachments_context: bool = False

    # Conversation / context
    base_history: List[ChatMessage] = field(default_factory=list)
    messages_for_llm: List[ChatMessage] = field(default_factory=list)
    tools_context_parts: List[str] = field(default_factory=list)
    built_history_messages: List[ChatMessage] = field(default_factory=list)
    history_includes_current_user: bool = False

    # ContextBuilder intermediate result (history + retrieved chunks)
    context_builder_result: Optional[BuiltContext] = None

    # Long-term memory retrieval intermediate result (retrieved entries + context messages)
    user_longterm_memory_result: Optional[Any] = None

    # Profile-based instruction fragments prepared by the memory layer
    profile_user_instructions: Optional[str] = None
    profile_org_instructions: Optional[str] = None

    # Usage flags
    used_rag: bool = False
    used_websearch: bool = False
    used_tools: bool = False
    used_user_profile: bool = False
    used_user_longterm_memory: bool = False

    # Tools
    tools_agent_answer: Optional[str] = None
    tool_traces: List[Dict[str, Any]] = field(default_factory=list)

    # Debug / diagnostics
    debug_trace: Dict[str, Any] = field(default_factory=dict)
    websearch_debug: Dict[str, Any] = field(default_factory=dict)

    # Token accounting (filled in _step_build_base_history)
    history_token_count: Optional[int] = None

    # Reasoning flags
    cap_rag_available: bool = False
    cap_user_ltm_available: bool = False
    cap_attachments_available: bool = False
    cap_websearch_available: bool = False
    cap_tools_available: bool = False


    # --- Core output (pipeline contract) ---
    # Filled by CoreLLM step
    raw_answer: Optional[str] = None

    # Filled by Persist step (final runtime output)
    runtime_answer: Optional[RuntimeAnswer] = None


    def trace_event(
        self,
        *,
        component: str,
        step: str,
        message: str,
        data: Optional[Dict[str, Any]] = None,
        level: str = "info",
    ) -> None:
        """
        Append-only event log, stored under debug_trace["events"].
        """
        evt = {
            "ts_utc": datetime.now(timezone.utc).isoformat(),
            "level": level,
            "component": component,
            "step": step,
            "message": message,
            "data": data or {},
        }
        self.debug_trace.setdefault("events", []).append(evt)


    def set_debug_section(self, key: str, value: Dict[str, Any]) -> None:
        """
        Structured debug snapshot (non-event) for a given step.
        """
        self.debug_trace[key] = value


    def set_debug_value(self, key: str, value: Any) -> None:
        self.debug_trace[key] = value


    def configure_llm_tracker(self) -> None:     

        if self.llm_usage_tracker is None:
            return
           
        self.llm_usage_tracker.register_adapter(self.context.config.llm_adapter, label="core_adapter")

        ta = self.context.config.tools_agent
        if ta is not None and ta.llm is not None:
            self.llm_usage_tracker.register_adapter(ta.llm, label="tools_agent")

        ws = self.context.config.websearch_config
        if ws is not None and ws.llm is not None:
            if ws.llm.map_adapter is not None:
                self.llm_usage_tracker.register_adapter(ws.llm.map_adapter, label="web_map_adapter")
            if ws.llm.reduce_adapter is not None:
                self.llm_usage_tracker.register_adapter(ws.llm.reduce_adapter, label="web_reduce_adapter")
            if ws.llm.rerank_adapter is not None:
                self.llm_usage_tracker.register_adapter(ws.llm.rerank_adapter, label="web_rerank_adapter")

    
    async def finalize_llm_tracker(self, request: RuntimeRequest, runtime_answer: RuntimeAnswer) -> None:
        if self.llm_usage_tracker is not None:
            runtime_answer.llm_usage_report = self.llm_usage_tracker.build_report()
            llm_usage_snapshot = runtime_answer.llm_usage_report.to_dict()
            self.set_debug_value("llm_usage", llm_usage_snapshot)

            if self.context.config.enable_llm_usage_collection and runtime_answer.llm_usage_report is not None:
                async with self.context.llm_usage_lock:
                    self.context.llm_usage_run_seq += 1
                    rec = LLMUsageRunRecord(
                        seq=self.context.llm_usage_run_seq,
                        ts_utc=datetime.now(timezone.utc),
                        run_id=self.run_id,
                        session_id=request.session_id,
                        user_id=request.user_id,
                        report=runtime_answer.llm_usage_report,
                    )
                    self.context.llm_usage_runs.append(rec)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/build_base_history_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.build_base_history_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=build_base_history_step.py
# LINES: 18
# SHA256: 2be250632ec9f7725e354938c5793d75c686c39d24c7850a4791c40138903fa7
# SYMBOLS:
#   - class BuildBaseHistoryStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep


class BuildBaseHistoryStep(RuntimeStep):
    """
    Build base history for the session (project/user/system seed history),
    using HistoryLayer.
    """

    async def run(self, state: RuntimeState) -> None:
        await state.context.history_layer.build_base_history(state)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/contract.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=contract.py
# LINES: 11
# SHA256: 98bef6402285705d65d61ac03cf47265e424beec53c8c50af373806a6bb6414f
# SYMBOLS:
#   - class RuntimeStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations
from typing import Protocol
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState

class RuntimeStep(Protocol):
    async def run(self, state: RuntimeState) -> None:        
        ...

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/core_llm_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.core_llm_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=core_llm_step.py
# LINES: 95
# SHA256: 473febe5e9b6f200d34f66417fe52eb4951c2ba96cb911957f22ed0cb1ab9d6b
# SYMBOLS:
#   - class CoreLLMStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from typing import Any, Dict

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep


class CoreLLMStep(RuntimeStep):
    """
    Call the core LLM adapter and decide on the final answer text,
    possibly falling back to tools_agent_answer when needed.
    """

    async def run(self, state: RuntimeState) -> None:
        # If tools were used and we have an explicit agent answer, prefer it.
        if state.used_tools and state.tools_agent_answer:
            # Trace the fact that we are reusing the tools agent answer
            # instead of calling the core LLM adapter.
            state.trace_event(
                component="engine",
                step="core_llm",
                message="Using tools_agent_answer as the final answer.",
                data={
                    "used_tools_answer": True,
                    "has_tools_agent_answer": True,
                },
            )
            state.raw_answer =  str(state.tools_agent_answer)
            return

        try:
            # Determine the per-request max output tokens, if any.
            max_output_tokens = state.request.max_output_tokens

            generate_kwargs: Dict[str, Any] = {}
            if max_output_tokens is not None:
                # Pass a max_tokens hint to the adapter. If the adapter ignores
                # it or uses a different keyword, that should be handled inside
                # the adapter implementation.
                generate_kwargs["max_tokens"] = max_output_tokens

            msgs = state.messages_for_llm
            if not msgs or msgs[-1].role != "user":
                raise Exception(
                    f"Last message must be 'user' (got: {msgs[-1].role if msgs else 'None'})."
                )

            raw_answer = state.context.config.llm_adapter.generate_messages(
                state.messages_for_llm,
                run_id=state.run_id,
                **generate_kwargs,
            )

            state.trace_event(
                component="engine",
                step="core_llm",
                message="Core LLM adapter returned answer.",
                data={
                    "used_tools_answer": False,
                    "adapter_return_type": "str",
                    "answer_len": len(raw_answer),
                    "answer_is_empty": not bool(raw_answer),
                },
            )
            
            state.raw_answer = raw_answer

        except Exception as e:
            state.set_debug_value("llm_error", str(e))

            # Trace the error and whether a tools_agent_answer fallback is available.
            state.trace_event(
                component="engine",
                step="core_llm_error",
                message="Core LLM adapter failed; falling back if possible.",
                data={
                    "error": str(e),
                    "has_tools_agent_answer": bool(state.tools_agent_answer),
                },
            )

            if state.tools_agent_answer:
                state.raw_answer = (
                    "[ERROR] LLM adapter failed, falling back to tools agent answer.\n"
                    f"Details: {e}\n\n"
                    f"{state.tools_agent_answer}"
                )
                return

            state.raw_answer = f"[ERROR] LLM adapter failed: {e}"

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/ensure_current_user_message_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.ensure_current_user_message_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=ensure_current_user_message_step.py
# LINES: 40
# SHA256: 6871578778c5900334d83aaba825be0b1092e82391a4c980b4386258c47b3cae
# SYMBOLS:
#   - class EnsureCurrentUserMessageStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep


class EnsureCurrentUserMessageStep(RuntimeStep):
    """
    Ensure the current user message is present as the last prompt message.

    Rules:
      - If request.message is empty -> no-op.
      - If messages_for_llm is empty -> add user message.
      - If last message is the same user message -> no-op.
      - Otherwise append user message to enforce user-last semantics.
    """

    async def run(self, state: RuntimeState) -> None:
        msg = (state.request.message or "").strip()
        if not msg:
            return

        if not state.messages_for_llm:
            state.messages_for_llm.append(ChatMessage(role="user", content=msg))
            return

        last = state.messages_for_llm[-1]
        last_content = (last.content or "").strip()

        # If the last message already equals the current user prompt, do nothing.
        if last.role == "user" and last_content == msg:
            return

        # Otherwise append current user prompt to enforce user-last semantics.
        state.messages_for_llm.append(ChatMessage(role="user", content=msg))

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/history_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.history_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=history_step.py
# LINES: 62
# SHA256: 3614c40a16b9c2ba46f66bcac63ff345c7819e4ddd67a713784ae043cd46e5e0
# SYMBOLS:
#   - class HistoryStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep


class HistoryStep(RuntimeStep):
    """
    Build conversation history for the LLM.

    This step is responsible only for selecting and shaping the
    conversational context (previous user/assistant turns).

    Retrieval (RAG) is handled separately in `RagStep`.
    """

    async def run(self, state: RuntimeState) -> None:
        session = state.session
        assert session is not None, "Session must be set before history step."
        req = state.request
        base_history = state.base_history

        if state.context.context_builder is not None:
            # Delegate history shaping (truncation, system message stitching, etc.)
            # to ContextBuilder, but do NOT inject RAG here.
            built = await state.context.context_builder.build_context(
                session=session,
                request=req,
                base_history=base_history,
            )

            # Keep the full result for the RAG step.
            state.context_builder_result = built

            history_messages = built.history_messages or []
            state.messages_for_llm.extend(history_messages)
            state.built_history_messages = history_messages
            state.history_includes_current_user = True
            state.set_debug_value("history_length", len(history_messages))
        else:
            # Fall back to using the base_history as-is (no additional
            # history layer beyond what ContextBuilder already produced).
            state.messages_for_llm.extend(base_history)
            state.built_history_messages = base_history
            state.history_includes_current_user = True
            state.set_debug_value("history_length", len(base_history))

        # Trace history building step.
        state.trace_event(
            component="engine",
            step="history",
            message="Conversation history built for LLM.",
            data={
                "history_length": len(state.built_history_messages),
                "base_history_length": len(state.base_history),
                "history_includes_current_user": state.history_includes_current_user,
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/instructions_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.instructions_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=instructions_step.py
# LINES: 84
# SHA256: 3de4e2340ab607471234082f22f85fb03397cfd840655013d7acc264ec3ce2a0
# SYMBOLS:
#   - class InstructionsStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from typing import Dict, List, Optional

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep


class InstructionsStep(RuntimeStep):
    """
    Inject the final instructions as the first `system` message in the LLM prompt,
    if any instructions exist.

    Combines:
      1) per-request instructions (RuntimeRequest.instructions),
      2) user profile instructions (state.profile_user_instructions),
      3) organization profile instructions (state.profile_org_instructions).

    Must be called AFTER history step to ensure:
      - instructions are always the first system message,
      - instructions are never persisted in SessionStore,
      - history can be trimmed/summarized freely before injection.
    """

    async def run(self, state: RuntimeState) -> None:
        instructions_text = self._build_final_instructions(state)
        if not instructions_text:
            return

        system_message = ChatMessage(role="system", content=instructions_text)

        # `messages_for_llm` at this point should contain only history
        # (built by HistoryStep). We now prepend the system message.
        state.messages_for_llm = [system_message] + state.messages_for_llm

    def _build_final_instructions(self, state: RuntimeState) -> Optional[str]:
        parts: List[str] = []
        sources: Dict[str, bool] = {
            "request": False,
            "user_profile": False,
            "organization_profile": False,
        }

        # 1) User-provided instructions (per-request, ChatGPT/Gemini-style)
        if isinstance(state.request.instructions, str):
            user_instr = state.request.instructions.strip()
            if user_instr:
                parts.append(user_instr)
                sources["request"] = True

        # 2) User profile instructions prepared by the memory layer
        if isinstance(state.profile_user_instructions, str):
            profile_user = state.profile_user_instructions.strip()
            if profile_user:
                parts.append(profile_user)
                sources["user_profile"] = True

        # 3) Organization profile instructions prepared by the memory layer
        if isinstance(state.profile_org_instructions, str):
            profile_org = state.profile_org_instructions.strip()
            if profile_org:
                parts.append(profile_org)
                sources["organization_profile"] = True

        if not parts:
            state.set_debug_section("instructions", {
                "has_instructions": False,
                "sources": sources,
            })
            return None

        final_text = "\n\n".join(parts)

        state.set_debug_section("instructions", {
            "has_instructions": True,
            "sources": sources,
        })

        return final_text

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/persist_and_build_answer_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.persist_and_build_answer_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=persist_and_build_answer_step.py
# LINES: 134
# SHA256: 9511313df01b7be5f8ee646cc50c7fc0f76271a83024a20ddb19064774b4adf0
# SYMBOLS:
#   - class PersistAndBuildAnswerStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from datetime import datetime, timezone
from typing import List

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import (
    RuntimeAnswer,
    RouteInfo,
    RuntimeStats,
    ToolCallInfo,
)


class PersistAndBuildAnswerStep(RuntimeStep):
    """
    Persist assistant message into the session and build RuntimeAnswer
    including RouteInfo and RuntimeStats.
    """

    async def run(self, state: RuntimeState) -> None:
        answer_text = state.raw_answer

        # Fallback if answer is empty for any reason
        if not isinstance(answer_text, str) or not answer_text.strip():
            answer_text = (
                str(state.tools_agent_answer)
                if state.tools_agent_answer
                else "[ERROR] Empty answer from runtime."
            )

        session = state.session
        assert session is not None, "Session must be set before persistence."

        assistant_message = ChatMessage(
            role="assistant",
            content=answer_text,
            created_at=datetime.now(timezone.utc).isoformat(),
        )
        await state.context.session_manager.append_message(session.id, assistant_message)

        # Strategy label
        if state.used_rag and state.used_websearch and state.used_tools:
            strategy = "llm_with_rag_websearch_and_tools"
        elif state.used_rag and state.used_tools:
            strategy = "llm_with_rag_and_tools"
        elif state.used_websearch and state.used_tools:
            strategy = "llm_with_websearch_and_tools"
        elif state.used_tools:
            strategy = "llm_with_tools"
        elif state.used_rag and state.used_websearch:
            strategy = "llm_with_rag_and_websearch"
        elif state.used_rag:
            strategy = "llm_with_rag_context_builder"
        elif state.used_websearch:
            strategy = "llm_with_websearch"
        elif state.used_attachments_context:
            strategy = "llm_with_session_attachments"
        elif state.ingestion_results:
            strategy = "llm_only_with_ingestion"
        else:
            strategy = "llm_only"

        route_info = RouteInfo(
            used_rag=state.used_rag and state.context.config.enable_rag,
            used_websearch=state.used_websearch and state.context.config.enable_websearch,
            used_tools=state.used_tools and state.context.config.tools_mode != "off",
            used_user_profile=state.used_user_profile,
            used_user_longterm_memory=state.used_user_longterm_memory and state.context.config.enable_user_longterm_memory,
            strategy=strategy,
            extra={
                "used_attachments_context": bool(state.used_attachments_context),
                "attachments_chunks": int(state.debug_trace.get("attachments_chunks", 0) or 0),
            },
        )

        # Token stats are still placeholders – can be wired from LLM adapter later.
        stats = RuntimeStats(
            total_tokens=None,
            input_tokens=None,
            output_tokens=None,
            rag_tokens=None,
            websearch_tokens=None,
            tool_tokens=None,
            duration_ms=None,
            extra={},
        )

        tool_calls_for_answer: List[ToolCallInfo] = []
        for t in state.tool_traces:
            tool_calls_for_answer.append(
                ToolCallInfo(
                    tool_name=t.get("tool") or "",
                    arguments=t.get("args") or {},
                    result_summary=(
                        t.get("output_preview")
                        if isinstance(t.get("output_preview"), str)
                        else None
                    ),
                    success=not bool(t.get("error")),
                    error_message=t.get("error"),
                    extra={"raw_trace": t},
                )
            )

        # Trace persistence and answer building step.
        state.trace_event(
            component="engine",
            step="persist_and_build_answer",
            message="Assistant answer persisted and RuntimeAnswer built.",
            data={
                "session_id": session.id,
                "strategy": strategy,
                "used_rag": state.used_rag,
                "used_websearch": state.used_websearch,
                "used_tools": state.used_tools,
            },
        )

        state.runtime_answer = RuntimeAnswer(
            answer=answer_text,
            citations=[],
            route=route_info,
            tool_calls=tool_calls_for_answer,
            stats=stats,
            raw_model_output=None,
            debug_trace=state.debug_trace,
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/profile_based_memory_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.profile_based_memory_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=profile_based_memory_step.py
# LINES: 83
# SHA256: efc7a68b87a66308b1dc76d80eb8b1de83c3e1b0b006a36b7a13c663643d1f73
# SYMBOLS:
#   - class ProfileBasedMemoryStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from typing import Optional

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep


class ProfileBasedMemoryStep(RuntimeStep):
    """
    Load profile-based instruction fragments for this request.

    Rules:
        - Use profile memory only if enabled in RuntimeConfig.
        - Do NOT rebuild or cache anything here yet (this is step 1 only).
        - Extract prebuilt 'system_prompt' strings from profile bundles.
        - Store the resulting fragments in RuntimeState so the engine
        can merge them into a system message later.
    """

    async def run(self, state: RuntimeState) -> None:
        session = state.session
        assert session is not None, "Session must exist before memory layer."

        cfg = state.context.config

        user_instr: Optional[str] = None
        org_instr: Optional[str] = None

        # 1) User profile memory (optional)
        if cfg.enable_user_profile_memory:
            user_instr_candidate = await state.context.session_manager.get_user_profile_instructions_for_session(
                session=session
            )
            if isinstance(user_instr_candidate, str):
                stripped = user_instr_candidate.strip()
                if stripped:
                    user_instr = stripped
                    state.used_user_profile = True

        # 2) Organization profile memory (optional)
        if cfg.enable_org_profile_memory:
            org_instr_candidate = await state.context.session_manager.get_org_profile_instructions_for_session(
                session=session
            )
            if isinstance(org_instr_candidate, str):
                stripped = org_instr_candidate.strip()
                if stripped:
                    org_instr = stripped
                    # For now we reuse the same flag to indicate that some profile
                    # (user or organization) has been used.
                    state.used_user_profile = True

        # 3) Store extracted profile instruction fragments in state
        state.profile_user_instructions = user_instr
        state.profile_org_instructions = org_instr


        # 4) Debug info
        state.set_debug_section("memory_layer", {
            "implemented": True,
            "has_user_profile_instructions": bool(user_instr),
            "has_org_profile_instructions": bool(org_instr),
            "enable_user_profile_memory": cfg.enable_user_profile_memory,
            "enable_org_profile_memory": cfg.enable_org_profile_memory,
        })

        # Trace memory layer step.
        state.trace_event(
            component="engine",
            step="memory_layer",
            message="Profile-based instructions loaded for session.",
            data={
                "has_user_profile_instructions": bool(user_instr),
                "has_org_profile_instructions": bool(org_instr),
                "enable_user_profile_memory": cfg.enable_user_profile_memory,
                "enable_org_profile_memory": cfg.enable_org_profile_memory,
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/rag_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.rag_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=rag_step.py
# LINES: 93
# SHA256: c079859fbe8230b688c95afa540ef33a1c0f0d2a434c61a1e18e1f82a5a640ad
# SYMBOLS:
#   - class RagStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations


from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools import format_rag_context, insert_context_before_last_user


class RagStep(RuntimeStep):
    """
    Build RAG layer (if configured) on top of the already constructed conversation history.

    Responsibilities:
      - ensure ContextBuilder result exists (fallback build_context if missing),
      - use RagPromptBuilder to build context messages from retrieved chunks,
      - inject RAG context messages before the last user message,
      - prepare compact RAG text for tools agent (state.tools_context_parts),
      - set debug fields + trace event.
    """

    async def run(self, state: RuntimeState) -> None:
        # Defaults
        state.used_rag = False
        state.set_debug_value("rag_chunks", 0)

        ctx = state.context

        # RAG disabled => no-op
        if not ctx.config.enable_rag:
            return

        if ctx.context_builder is None:
            raise RuntimeError("RAG enabled but ContextBuilder is not configured.")

        # Prefer result from HistoryStep (normal flow)
        built = state.context_builder_result

        # Fallback: build here (should be rare)
        if built is None:
            session = state.session
            assert session is not None, "Session must be set before RAG step."

            built = await ctx.context_builder.build_context(
                session=session,
                request=state.request,
                base_history=state.base_history,
            )
            state.context_builder_result = built

        rag_info = built.rag_debug_info or {}
        state.set_debug_value("rag", rag_info)

        retrieved_chunks = built.retrieved_chunks or []
        state.used_rag = bool(rag_info.get("used", bool(retrieved_chunks)))

        if not state.used_rag:
            state.set_debug_value("rag_chunks", 0)
            return

        if ctx.rag_prompt_builder is None:
            raise RuntimeError("RAG enabled but rag_prompt_builder is not configured.")

        # Build RAG prompt bundle (context messages only)
        bundle = ctx.rag_prompt_builder.build_rag_prompt(built)
        context_messages = bundle.context_messages or []

        # Inject context before last user message
        if context_messages:
            insert_context_before_last_user(state, context_messages)

        # Compact textual form of RAG context for tools agent
        rag_context_text = format_rag_context(retrieved_chunks)
        if rag_context_text:
            state.tools_context_parts.append("RAG CONTEXT:\n" + rag_context_text)

        state.set_debug_value("rag_chunks", len(retrieved_chunks))

        # Trace
        state.trace_event(
            component="engine",
            step="rag",
            message="RAG context built and injected.",
            data={
                "rag_enabled": ctx.config.enable_rag,
                "used_rag": state.used_rag,
                "retrieved_chunks": len(retrieved_chunks),
                "context_messages": len(context_messages),
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/retrieve_attachments_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.retrieve_attachments_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=retrieve_attachments_step.py
# LINES: 104
# SHA256: e58c576f2ca52169ad624613c534f4f487bd2f703e1f56aaeedddcda5b2a202a
# SYMBOLS:
#   - class RetrieveAttachmentsStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from datetime import datetime, timezone

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools import format_rag_context, insert_context_before_last_user


class RetrieveAttachmentsStep(RuntimeStep):
    """
    Retrieve relevant chunks from session-ingested attachments (AttachmentIngestionService)
    and inject them into the LLM context.

    Key requirements:
        - Independent from enable_rag (must work even when enable_rag=False).
        - Reuse existing retrieval components (EmbeddingManager + VectorstoreManager.query).
        - Filter by session_id + user_id (+ tenant/workspace if available).
        - Inject as context messages using _insert_context_before_last_user.
    """

    async def run(self, state: RuntimeState) -> None:
        # Defaults
        state.used_attachments_context = False
        state.set_debug_value("attachments_chunks", 0)

        if state.context.ingestion_service is None:
            state.set_debug_section("attachments", {"used": False, "reason": "ingestion_service_not_configured"})
            return

        session = state.session
        if session is None:
            state.set_debug_section("attachments", {"used": False, "reason": "session_not_initialized"})
            return

        # Retrieval (no coupling to enable_rag)
        res = await state.context.ingestion_service.search_session_attachments(
            query=state.request.message,
            session_id=session.id,
            user_id=state.request.user_id,
            tenant_id=session.tenant_id,
            workspace_id=session.workspace_id,
            top_k=6,
            score_threshold=None,
        )

        dbg = (res or {}).get("debug") or {}
        used = bool((res or {}).get("used"))
        chunks = (res or {}).get("hits") or []

        state.used_attachments_context = used and bool(chunks)
        state.set_debug_section("attachments", {
            **dbg,
            "used": bool(used and chunks),
            "hits_count": len(chunks),
        })
        state.set_debug_value("attachments_chunks", len(chunks))

        if not state.used_attachments_context:
            return

        # Build a single system context message (same injection pattern as RAG).
        attachments_context_text = format_rag_context(chunks)
        if not attachments_context_text.strip():
            # If somehow chunks exist but formatting is empty, treat as unused.
            state.used_attachments_context = False
            state.set_debug_section("attachments", {
                **dbg,
                "used": False,
                "reason": "empty_formatted_context",
            })
            state.set_debug_value("attachments_chunks", 0)
            return

        content = "SESSION ATTACHMENTS (retrieved):\n" + attachments_context_text

        context_messages = [
            ChatMessage(
                role="system",
                content=content,
                created_at=datetime.now(timezone.utc).isoformat(),
            )
        ]

        insert_context_before_last_user(state, context_messages)

        # Provide compact textual form also for tools agent (same pattern as RAG).
        state.tools_context_parts.append("SESSION ATTACHMENTS:\n" + attachments_context_text)

        # Trace step
        state.trace_event(
            component="engine",
            step="attachments_context",
            message="Session attachments retrieval executed and context injected.",
            data={
                "used_attachments_context": state.used_attachments_context,
                "retrieved_chunks": len(chunks),
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/session_and_ingest_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.session_and_ingest_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=session_and_ingest_step.py
# LINES: 117
# SHA256: 3781275bf2fb77787c95e2da8879d86b33cbca52045febf76a898b1302d3644f
# SYMBOLS:
#   - class SessionAndIngestStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from datetime import datetime, timezone
from typing import Any, Dict, List

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep
from intergrax.runtime.drop_in_knowledge_mode.ingestion.ingestion_service import IngestionResult
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest


class SessionAndIngestStep(RuntimeStep):
    """
    Load or create a session, ingest attachments (RAG), append the user
    message and initialize debug_trace.

    IMPORTANT:
        - This step does NOT load conversation history.
        - History is loaded and preprocessed in `_step_build_base_history`.
    """

    async def run(self, state: RuntimeState) -> None:
        req = state.request

        # 1. Load or create session
        session = await state.context.session_manager.get_session(req.session_id)
        if session is None:
            session = await state.context.session_manager.create_session(
                session_id=req.session_id,
                user_id=req.user_id,
                tenant_id=req.tenant_id or state.context.config.tenant_id,
                workspace_id=req.workspace_id or state.context.config.workspace_id,
                metadata=req.metadata,
            )

        # 1a. Ingest attachments into vector store (if any)
        ingestion_results: List[IngestionResult] = []
        if req.attachments:
            if state.context.ingestion_service is None:
                raise ValueError(
                    "Attachments were provided but ingestion_service is not configured. "
                    "Pass ingestion_service explicitly to control where attachments are indexed."
                )
    
            ingestion_results = await state.context.ingestion_service.ingest_attachments_for_session(
                attachments=req.attachments,
                session_id=session.id,
                user_id=req.user_id,
                tenant_id=session.tenant_id,
                workspace_id=session.workspace_id,
            )

        # 2. Append user message to session history
        user_message = self._build_session_message_from_request(req)
        await state.context.session_manager.append_message(session.id, user_message)

        # Reload the session to ensure we have the latest metadata
        session = await state.context.session_manager.get_session(session.id) or session

        # Initialize debug trace – history will be attached later
        debug_trace: Dict[str, Any] = {
            "session_id": session.id,
            "user_id": session.user_id,
        }

        if ingestion_results:
            debug_trace["ingestion"] = [
                {
                    "attachment_id": r.attachment_id,
                    "attachment_type": r.attachment_type,
                    "num_chunks": r.num_chunks,
                    "vector_ids_count": len(r.vector_ids),
                    "metadata": r.metadata,
                }
                for r in ingestion_results
            ]

        # Trace session and ingestion step.
        state.trace_event(
            component="engine",
            step="session_and_ingest",
            message="Session loaded/created and user message appended; attachments ingested.",
            data={
                "session_id": session.id,
                "user_id": session.user_id,
                "tenant_id": session.tenant_id,
                "attachments_count": len(req.attachments or []),
                "ingestion_results_count": len(ingestion_results),
            },
        )

        state.session = session
        state.ingestion_results = ingestion_results
        state.debug_trace = debug_trace
        # NOTE: state.base_history is intentionally left empty here.


    def _build_session_message_from_request(
        self,
        request: RuntimeRequest,
    ) -> ChatMessage:
        """
        Construct a ChatMessage from a RuntimeRequest to be stored in the session.

        Attachments from `request.attachments` are represented at the request level
        and can be linked via metadata if needed.
        """
        return ChatMessage(
            role="user",
            content=request.message,
            created_at=datetime.now(timezone.utc).isoformat(),
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/tools.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=tools.py
# LINES: 96
# SHA256: f6d2ff1e397b1bf9c6fa0f068e9cd61d78e97fdc3948a783fd9b850d7b250e4d
# SYMBOLS:
#   - def insert_context_before_last_user()
#   - def format_rag_context()
#   - def _chunk_text()
#   - def _chunk_meta()
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations
from typing import Any, Iterable, List, Optional

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState


def insert_context_before_last_user(        
        state: RuntimeState,
        context_messages: List[ChatMessage],
    ) -> None:
        """
        Insert context messages right before the last user message.
        If no user message exists, append to the end.
        """
        if not context_messages:
            return

        msgs = state.messages_for_llm
        last_user_idx: Optional[int] = None

        for i in range(len(msgs) - 1, -1, -1):
            if msgs[i].role == "user":
                last_user_idx = i
                break

        if last_user_idx is None:
            msgs.extend(context_messages)
        else:
            state.messages_for_llm = msgs[:last_user_idx] + context_messages + msgs[last_user_idx:]


def format_rag_context(chunks: Iterable[Any], *, max_chars: int = 4000) -> str:
    """
    Best-effort compact formatter that is resilient to different chunk shapes.
    """
    lines: List[str] = []
    total = 0

    for idx, ch in enumerate(chunks, start=1):
        text = _chunk_text(ch)
        meta = _chunk_meta(ch)

        header_parts: List[str] = [f"[{idx}]"]
        src = meta.get("source") or meta.get("url") or meta.get("doc_id") or meta.get("file")
        if src:
            header_parts.append(str(src))
        page = meta.get("page") or meta.get("page_number")
        if page is not None:
            header_parts.append(f"p={page}")

        header = " ".join(header_parts)
        block = header + "\n" + (text.strip() if text else "").strip()

        if not block.strip():
            continue

        # Enforce cap
        if total + len(block) + 2 > max_chars:
            remaining = max_chars - total
            if remaining > 80:
                lines.append(block[:remaining].rstrip() + "…")
            break

        lines.append(block)
        total += len(block) + 2

    return "\n\n".join(lines).strip()


def _chunk_text(ch: Any) -> str:
    for attr in ("text", "content", "page_content", "chunk", "value"):
        v = getattr(ch, attr, None)
        if isinstance(v, str) and v.strip():
            return v
    if isinstance(ch, dict):
        for k in ("text", "content", "page_content", "chunk", "value"):
            v = ch.get(k)
            if isinstance(v, str) and v.strip():
                return v
    return ""


def _chunk_meta(ch: Any) -> dict:
    meta = getattr(ch, "metadata", None)
    if isinstance(meta, dict):
        return meta
    if isinstance(ch, dict):
        m = ch.get("metadata")
        if isinstance(m, dict):
            return m
    return {}

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/tools_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=tools_step.py
# LINES: 157
# SHA256: f15f1281f0e327ebd79e1c776208423ee32b545e4ce4fc49a74e57b48dfcd675
# SYMBOLS:
#   - class ToolsStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

import json
from typing import Any, Dict, List

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.config import ToolsContextScope
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep


class ToolsStep(RuntimeStep):
    """
    Run tools agent (planning + tool calls) if configured.

    Output is stored in RuntimeState:
      - state.used_tools
      - state.tool_traces
      - state.tools_agent_answer

    Core LLM step may decide to reuse tools_agent_answer.
    """

    async def run(self, state: RuntimeState) -> None:
        """
        Run tools agent (planning + tool calls) if configured.

        The tools result is:
          - optionally used as the final answer (when tools_mode != "off"),
          - appended as system context for the core LLM.
        """
        state.used_tools = False
        state.tool_traces = []
        state.tools_agent_answer = None

        use_tools = (
            state.context.config.tools_agent is not None
            and state.context.config.tools_mode != "off"
        )
        if not use_tools:
            return

        tools_context = (
            "\n\n".join(state.tools_context_parts).strip()
            if state.tools_context_parts
            else None
        )

        debug_tools: Dict[str, Any] = {
            "mode": state.context.config.tools_mode,
        }

        try:
            # Decide what to pass as input_data for the tools agent.
            if state.context.config.tools_context_scope == ToolsContextScope.CURRENT_MESSAGE_ONLY:
                agent_input = state.request.message

            elif state.context.config.tools_context_scope == ToolsContextScope.CONVERSATION:
                # Use history built by ContextBuilder if available,
                # otherwise fall back to base_history.
                if state.built_history_messages:
                    agent_input = state.built_history_messages
                else:
                    agent_input = state.base_history

            else:
                # FULL_CONTEXT or any future scope:
                # pass entire message list built so far.
                agent_input = state.messages_for_llm

            tools_result = state.context.config.tools_agent.run(
                input_data=agent_input,
                context=tools_context,
                stream=False,
                tool_choice=None,
                output_model=None,
                run_id=state.run_id,
                llm_usage_tracker = state.llm_usage_tracker
            )

            state.tools_agent_answer = tools_result.get("answer", "") or None
            state.tool_traces = tools_result.get("tool_traces") or []
            state.used_tools = bool(state.tool_traces)

            debug_tools["used_tools"] = state.used_tools
            debug_tools["tool_traces"] = state.tool_traces
            if state.tools_agent_answer:
                debug_tools["agent_answer_preview"] = str(state.tools_agent_answer)[:200]
            if state.context.config.tools_mode == "required" and not state.used_tools:
                debug_tools["warning"] = (
                    "tools_mode='required' but no tools were invoked by the tools_agent."
                )

            # Inject executed tool calls as system context for core LLM.
            if state.tool_traces:
                tool_lines: List[str] = []
                for t in state.tool_traces:
                    name = t.get("tool")
                    args = t.get("args")
                    output = t.get("output")

                    tool_lines.append(f"Tool '{name}' was called.")
                    if args is not None:
                        try:
                            args_str = json.dumps(args, ensure_ascii=False)
                        except Exception:
                            args_str = str(args)
                        tool_lines.append(f"Arguments: {args_str}")

                    if output is not None:
                        if isinstance(output, (dict, list)):
                            try:
                                out_str = json.dumps(output, ensure_ascii=False)
                            except Exception:
                                out_str = str(output)
                        else:
                            out_str = str(output)
                        tool_lines.append("Output:")
                        tool_lines.append(out_str)

                    tool_lines.append("")

                tools_context_for_llm = "\n".join(tool_lines).strip()
                if tools_context_for_llm:
                    insert_at = len(state.messages_for_llm) - 1
                    state.messages_for_llm.insert(
                        insert_at,
                        ChatMessage(
                            role="system",
                            content=(
                                "The following tool calls have been executed. "
                                "Use their results when answering the user.\n\n"
                                + tools_context_for_llm
                            ),
                        ),
                    )

        except Exception as e:
            debug_tools["tools_error"] = str(e)

        state.set_debug_section("tools",  debug_tools)

        # Trace tools step.
        state.trace_event(
            component="engine",
            step="tools",
            message="Tools agent step executed.",
            data={
                "tools_mode": state.context.config.tools_mode,
                "used_tools": state.used_tools,
                "tool_traces_count": len(state.tool_traces),
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/user_longterm_memory_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.user_longterm_memory_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=user_longterm_memory_step.py
# LINES: 123
# SHA256: 98b9a1ee076aa43b0f8b39cf78b2e6469917e4b83abf76ee08a51116b9ab1d99
# SYMBOLS:
#   - class UserLongtermMemoryStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from typing import List

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools import (
    insert_context_before_last_user,
)


class UserLongtermMemoryStep(RuntimeStep):
    """
    Retrieve user long-term memory (LTM) and inject as context messages.

    - Uses state.user_longterm_memory_result cache if available.
    - Uses state.context.session_manager.search_user_longterm_memory(...) for retrieval.
    - Uses state.context.user_longterm_memory_prompt_builder.build_user_longterm_memory_prompt(hits)
      to create context messages.
    """

    async def run(self, state: RuntimeState) -> None:
        state.used_user_longterm_memory = False
        state.set_debug_value("user_longterm_memory_hits", 0)

        # Create a single debug section early and only mutate it later.
        state.set_debug_section("user_longterm_memory", {
            "enabled": bool(state.context.config.enable_user_longterm_memory),
            "used": False,
            "reason": None,
            "hits": 0,
            "retrieval_debug": {},
            "context_blocks_count": 0,
            "context_preview": "",
            "context_preview_chars": 0,
            "hits_preview": [],
        })
        dbg = state.debug_trace["user_longterm_memory"]

        if not state.context.config.enable_user_longterm_memory:
            dbg["reason"] = "disabled"
            return

        built = state.user_longterm_memory_result

        if built is None:
            session = state.session
            assert session is not None, "Session must be set before user long-term memory step."

            query = (state.request.message or "").strip()
            if not query:
                dbg["reason"] = "empty_query"
                dbg["used"] = False
                dbg["hits"] = 0
                state.set_debug_value("user_longterm_memory_hits", 0)
                return

            built = await state.context.session_manager.search_user_longterm_memory(
                user_id=session.user_id,
                query=query,
                top_k=state.context.config.max_longterm_entries_per_query,
                score_threshold=state.context.config.longterm_score_threshold,
            )
            state.user_longterm_memory_result = built

        built = built or {}
        ltm_info = built.get("debug") or {}
        hits = built.get("hits") or []

        # Keep retrieval debug inside the same section
        dbg["retrieval_debug"] = ltm_info

        state.used_user_longterm_memory = bool(ltm_info.get("used", bool(hits)))
        dbg["used"] = state.used_user_longterm_memory
        dbg["hits"] = len(hits)

        if not state.used_user_longterm_memory:
            dbg["reason"] = ltm_info.get("reason") or "no_hits_or_not_used"
            state.set_debug_value("user_longterm_memory_hits", 0)
            return

        bundle = state.context.user_longterm_memory_prompt_builder.build_user_longterm_memory_prompt(hits)
        context_messages = bundle.context_messages or []
        insert_context_before_last_user(state, context_messages)

        # Debug trace (no tools coupling)
        ltm_context_texts: List[str] = []
        for msg in context_messages:
            if msg.content:
                ltm_context_texts.append(msg.content)

        dbg["context_blocks_count"] = len(ltm_context_texts)

        preview = "\n\n".join(ltm_context_texts[:2])  # first 1-2 blocks is enough
        dbg["context_preview"] = preview
        dbg["context_preview_chars"] = len(preview)

        dbg["hits_preview"] = [
            {
                "entry_id": h.entry_id,
                "title": getattr(h, "title", None),
                "kind": getattr(getattr(h, "kind", None), "value", getattr(h, "kind", None)),
                "deleted": bool(getattr(h, "deleted", False)),
            }
            for h in hits[:5]
        ]

        state.set_debug_value("user_longterm_memory_hits", len(hits))

        state.trace_event(
            component="engine",
            step="user_longterm_memory",
            message="User long-term memory step executed.",
            data={
                "ltm_enabled": state.context.config.enable_user_longterm_memory,
                "used_user_longterm_memory": state.used_user_longterm_memory,
                "hits": len(hits),
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/engine/runtime_steps/websearch_step.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.websearch_step
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=websearch_step.py
# LINES: 117
# SHA256: fbc460761d883c3d0bf58a81bcf7ec18fe1cfff8f8fd7af959da2f017be52904
# SYMBOLS:
#   - class WebsearchStep
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from typing import Any, List

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.contract import RuntimeStep
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_steps.tools import insert_context_before_last_user


class WebsearchStep(RuntimeStep):
    """
    Execute websearch (if configured) and inject web context into the LLM prompt.

    Responsibilities:
      - run websearch agent/executor using state.messages_for_llm and tools context
      - build web context messages with websearch_prompt_builder
      - inject web context messages before the last user message
      - append compact web context text into state.tools_context_parts (for tools agent)
      - debug + trace
    """

    async def run(self, state: RuntimeState) -> None:
        state.websearch_debug = {}
        state.used_websearch = False

        if (
            not state.context.config.enable_websearch
            or state.context.websearch_executor is None
            or state.context.websearch_prompt_builder is None
        ):
            return

        try:
            web_results = await state.context.websearch_executor.search_async(
                query=state.request.message,
                top_k=state.context.config.max_docs_per_query,
                language=None,
                top_n_fetch=None,
            )

            state.set_debug_section("websearch", {})
            dbg = state.debug_trace["websearch"]

            raw_preview = []
            for d in (web_results or [])[:5]:
                raw_preview.append(
                    {
                        "type": type(d).__name__,
                        "title": d.title,
                        "url": d.url,
                        "snippet_len": len(d.snippet or ""),
                        "text_len": len(d.text or ""),
                    }
                )

            dbg["raw_results_preview"] = raw_preview

            if not web_results:
                return

            state.used_websearch = True

            bundle = await state.context.websearch_prompt_builder.build_websearch_prompt(
                web_results=web_results,
                user_query=state.request.message,
                run_id=state.run_id,
            )

            context_messages = bundle.context_messages or []
            insert_context_before_last_user(state, context_messages)
            state.websearch_debug.update(bundle.debug_info or {})

            # Debug trace (no tools coupling)
            web_context_texts: List[str] = []
            for msg in context_messages:
                if msg.content:
                    web_context_texts.append(msg.content)

            dbg["context_blocks_count"] = len(web_context_texts)

            # Preview only to avoid bloating trace
            preview = "\n\n".join(web_context_texts[:1])
            dbg["context_preview"] = preview
            dbg["context_preview_chars"] = len(preview)

            # Guardrail signal: did websearch produce any grounded evidence?
            dbg["no_evidence"] = "No answer-relevant evidence extracted" in (preview or "")
            state.websearch_debug["no_evidence"] = dbg["no_evidence"]

            # Optional: doc-level preview (titles/urls only)
            dbg["docs_preview"] = [
                {
                    "title": d.title,
                    "url": d.url,
                }
                for d in (web_results or [])[:5]
            ]

        except Exception as exc:
            state.websearch_debug["error"] = str(exc)

        # Trace web search step.
        state.trace_event(
            component="engine",
            step="websearch",
            message="Web search step executed.",
            data={
                "websearch_enabled": state.context.config.enable_websearch,
                "used_websearch": state.used_websearch,
                "has_error": "error" in (state.websearch_debug or {}),
                "no_evidence": state.websearch_debug.get("no_evidence", False),
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/ingestion/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.ingestion
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/ingestion/attachments.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.ingestion.attachments
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=attachments.py
# LINES: 124
# SHA256: f1f76309b50f858810044e515eb5831eab90919fe622b77456f47e0ada8a12d9
# SYMBOLS:
#   - class AttachmentResolver
#   - class FileSystemAttachmentResolver
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework - proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

"""
Attachment resolution utilities for Drop-In Knowledge Mode.

This module defines:
  - `AttachmentResolver` protocol - an abstraction that knows how to turn
    an `AttachmentRef` into a local `Path` (or raise if it cannot).
  - `FileSystemAttachmentResolver` - a minimal implementation that handles
    local filesystem-based URIs, such as `file:///...`.

The goal is to decouple:
  - how and where attachments are stored (filesystem, DB, object storage),
  - from how the RAG pipeline consumes them (Intergrax document loaders).

In other words:
  Runtime deals with AttachmentRef -> AttachmentResolver -> Path,
  and then passes the resolved Paths to Intergrax RAG components.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Protocol, runtime_checkable
from urllib.parse import urlparse

from intergrax.llm.messages import AttachmentRef


@runtime_checkable
class AttachmentResolver(Protocol):
    """
    Resolves an AttachmentRef into a local file path that can be passed
    to Intergrax document loaders.

    Implementations may:
      - download from object storage (S3, GCS, etc.),
      - fetch from a database and materialize as a temporary file,
      - validate and return local filesystem paths.

    The only hard requirement for the RAG pipeline is that the returned
    object can be consumed by the Intergrax documents loader (typically
    a string path).
    """

    async def resolve_to_path(self, attachment: AttachmentRef) -> Path:
        """
        Resolve the given AttachmentRef into a local filesystem Path.

        Raises:
          - FileNotFoundError if the attachment cannot be found.
          - ValueError for unsupported URI schemes.
        """
        ...


@dataclass
class FileSystemAttachmentResolver:
    """
    Minimal resolver for URIs like:

      - file:///absolute/path/to/file.pdf
      - file://C:/path/to/file.pdf
      - /relative/or/absolute/path (with empty scheme)

    This implementation is intended for:
      - local experiments,
      - Jupyter notebooks,
      - simple on-prem setups.

    In production you are expected to provide additional resolvers
    (e.g. S3AttachmentResolver, DBAttachmentResolver, etc.).
    """

    def _from_file_uri(self, uri: str) -> Path:
        parsed = urlparse(uri)

        # Allow both explicit "file" scheme and an empty scheme (raw path).
        if parsed.scheme not in ("", "file"):
            raise ValueError(f"Unsupported URI scheme for file resolver: {parsed.scheme}")

        # Case 1: raw path without scheme (e.g. "D:/..." or "C:\\...")
        if parsed.scheme == "":
            # `uri` is a plain path string in this branch.
            return Path(uri).expanduser()

        # Case 2: proper file:// URI
        # Example on Windows:
        #   file:///D:/Projekty/intergrax/PROJECT_STRUCTURE.md
        #   -> parsed.path == "/D:/Projekty/intergrax/PROJECT_STRUCTURE.md"
        path_str = parsed.path

        # Fix Windows-style drive letters:
        # If path looks like "/D:/something", strip the leading slash.
        if (
            len(path_str) >= 4
            and path_str[0] == "/"
            and path_str[2] == ":"
        ):
            path_str = path_str[1:]  # "D:/Projekty/..."

        # UNC / netloc case (rare in this context, but we keep it for completeness)
        if parsed.netloc:
            # e.g. file://server/share/path
            # You can adapt this logic if you want UNC support.
            path_str = f"//{parsed.netloc}{path_str}"

        return Path(path_str).expanduser()

    async def resolve_to_path(self, attachment: AttachmentRef) -> Path:
        """
        Resolve the AttachmentRef's URI into an existing filesystem Path.

        This implementation assumes that `attachment.uri` is either:
          - a raw filesystem path, or
          - a `file://` URI pointing to a local file.
        """
        path = self._from_file_uri(attachment.uri)
        if not path.exists():
            raise FileNotFoundError(f"Attachment path does not exist: {path}")
        return path

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/ingestion/ingestion_service.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.ingestion.ingestion_service
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=ingestion_service.py
# LINES: 470
# SHA256: f85b4e1154a7e1a580c573e7e6c55521cc1d8859fe5377a5dddbbef24a383404
# SYMBOLS:
#   - class IngestionResult
#   - class AttachmentIngestionService
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

"""
Attachment ingestion pipeline for Drop-In Knowledge Mode.

This module defines a high-level service that:
  - takes AttachmentRef objects (from sessions/messages),
  - resolves them to loader-compatible paths via AttachmentResolver,
  - loads and splits documents using Intergrax RAG components:
      * IntergraxDocumentsLoader
      * IntergraxDocumentsSplitter
  - embeds them and stores them in a vector database via:
      * IntergraxEmbeddingManager
      * IntergraxVectorstoreManager

The goal is to reuse existing Intergrax RAG building blocks while providing
a clean, runtime-oriented API that operates on AttachmentRef.
"""

from __future__ import annotations

import inspect
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence

from langchain_core.documents import Document

from intergrax.llm.messages import AttachmentRef
from intergrax.rag.documents_loader import DocumentsLoader
from intergrax.rag.documents_splitter import DocumentsSplitter
from intergrax.rag.embedding_manager import EmbeddingManager
from intergrax.rag.vectorstore_manager import VectorstoreManager
from intergrax.runtime.drop_in_knowledge_mode.context.context_builder import RetrievedChunk
from intergrax.runtime.drop_in_knowledge_mode.ingestion.attachments import AttachmentResolver



# ---------------------------------------------------------------------------
# Ingestion result model
# ---------------------------------------------------------------------------

@dataclass
class IngestionResult:
    """
    Summary information about ingestion of a single attachment.
    """

    attachment_id: str
    attachment_type: str
    num_chunks: int
    vector_ids: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


# ---------------------------------------------------------------------------
# Attachment ingestion service (Intergrax-native)
# ---------------------------------------------------------------------------

class AttachmentIngestionService:
    """
    High-level ingestion service for Drop-In Knowledge Mode.

    Responsibilities:
      - Resolve AttachmentRef objects into filesystem Paths (via AttachmentResolver).
      - Load documents using IntergraxDocumentsLoader.load_document(...).
      - Split them into chunks using IntergraxDocumentsSplitter.split_documents(...).
      - Embed chunks (via IntergraxEmbeddingManager).
      - Store vectors (via IntergraxVectorstoreManager).
      - Return a structured IngestionResult per attachment.

    This service does NOT:
      - manage ChatSession objects,
      - perform retrieval or answering.

    It is intended to be called from DropInKnowledgeRuntime or other
    orchestration layers when new attachments are added to a session.
    """

    def __init__(
        self,
        *,
        resolver: AttachmentResolver,
        embedding_manager: EmbeddingManager,
        vectorstore_manager: VectorstoreManager,
        loader: Optional[DocumentsLoader] = None,
        splitter: Optional[DocumentsSplitter] = None,
    ) -> None:
        """
        Args:
            resolver:
                Component that knows how to resolve AttachmentRef.uri into a local Path.
            embedding_manager:
                IntergraxEmbeddingManager used to generate embeddings.
            vectorstore_manager:
                IntergraxVectorstoreManager used to store embeddings + metadata.
            loader:
                Optional custom IntergraxDocumentsLoader instance. If None, a default
                instance is created with conservative settings.
            splitter:
                Optional custom IntergraxDocumentsSplitter instance. If None, a default
                instance is created with standard chunking parameters.
        """
        self._resolver = resolver
        self._embedding_manager = embedding_manager
        self._vectorstore_manager = vectorstore_manager

        # Use provided loader/splitter or fall back to default instances.
        self._loader = loader or DocumentsLoader(verbose=False)
        self._splitter = splitter or DocumentsSplitter(
            verbose=False,
            default_chunk_size=1000,
            default_chunk_overlap=100,
        )

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def ingest_attachments_for_session(
        self,
        attachments: Sequence[AttachmentRef],
        *,
        session_id: str,
        user_id: str,
        tenant_id: Optional[str] = None,
        workspace_id: Optional[str] = None,
    ) -> List[IngestionResult]:
        """
        Ingest all provided attachments in the context of a specific session.

        The session/user/tenant/workspace identifiers are injected as metadata,
        so that RAG retrieval can later filter documents appropriately.
        """

        if self._embedding_manager is None or self._vectorstore_manager is None:
            raise ValueError(
                "Attachment ingestion requires embedding_manager and vectorstore_manager. "
                "Provide them in RuntimeConfig or pass a custom ingestion_service."
            )

        results: List[IngestionResult] = []

        for attachment in attachments:
            result = await self._ingest_single_attachment(
                attachment=attachment,
                session_id=session_id,
                user_id=user_id,
                tenant_id=tenant_id,
                workspace_id=workspace_id,
            )
            results.append(result)

        return results

    
    async def search_session_attachments(
        self,
        *,
        query: str,
        session_id: str,
        user_id: str,
        tenant_id: Optional[str] = None,
        workspace_id: Optional[str] = None,
        top_k: int = 6,
        score_threshold: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        Retrieval over session attachments indexed by this ingestion service.

        Contract:
          - Uses ingestion embedding_manager to embed the user query
          - Uses ingestion vectorstore_manager.query(query_embeddings, top_k, where=...)
          - No getattr/hasattr or signature guessing
        """
        q = (query or "").strip()
        if not q:
            return {
                "used": False,
                "hits": [],
                "scores": [],
                "debug": {"reason": "empty_query"},
            }

        if self._vectorstore_manager is None:
            return {
                "used": False,
                "hits": [],
                "scores": [],
                "debug": {"reason": "vectorstore_manager_not_configured"},
            }

        if self._embedding_manager is None:
            return {
                "used": False,
                "hits": [],
                "scores": [],
                "debug": {"reason": "embedding_manager_not_configured"},
            }

        where: Dict[str, Any] = {
            "session_id": session_id,
            "user_id": user_id,
        }
        if tenant_id is not None:
            where["tenant_id"] = tenant_id
        if workspace_id is not None:
            where["workspace_id"] = workspace_id

        # 1) Embed the query using ingestion embedder.
        # Expected EmbeddingManager API in Intergrax: embed_one / embed_query / embed
        # Use the same call you use in ingestion indexing for chunks, but for single query string.
        query_emb = self._embedding_manager.embed_one(q)        
        if inspect.iscoroutine(query_emb):
            query_emb = await query_emb

        # 2) Vector search in ingestion vectorstore with strict session filters.
        raw = self._vectorstore_manager.query(
            query_embeddings=query_emb,
            top_k=int(top_k),
            where=where,
            include_embeddings=False,
        )
        
        # VectorstoreManager.query() returns a provider-normalized dict:
        # {
        #   "ids": List[List[str]],
        #   "scores": List[List[float]],
        #   "metadatas": List[List[dict]],
        #   "documents": List[List[str]] | None
        # }
        if not isinstance(raw, dict):
            return {
                "used": False,
                "hits": [],
                "scores": [],
                "debug": {"reason": "unexpected_vectorstore_result_type"},
            }

        ids_rows = raw.get("ids") or []
        scores_rows = raw.get("scores") or []
        metas_rows = raw.get("metadatas") or []
        docs_rows = raw.get("documents") or []

        # Batched outputs: take first row for a single query.
        ids_row = ids_rows[0] if isinstance(ids_rows, list) and len(ids_rows) > 0 else []
        scores_row = scores_rows[0] if isinstance(scores_rows, list) and len(scores_rows) > 0 else []
        metas_row = metas_rows[0] if isinstance(metas_rows, list) and len(metas_rows) > 0 else []
        docs_row = docs_rows[0] if isinstance(docs_rows, list) and len(docs_rows) > 0 else []

        hits: List[RetrievedChunk] = []
        scores: List[float] = []

        n = min(len(docs_row), len(metas_row), len(scores_row), len(ids_row) if ids_row else 10**9)

        for i in range(n):
            text = (docs_row[i] or "").strip()
            if not text:
                continue

            md = metas_row[i] or {}
            sc = scores_row[i] if i < len(scores_row) else 0.0
            scv = float(sc) if sc is not None else 0.0

            # Optional: keep id in metadata for traceability
            if ids_row and i < len(ids_row):
                md = dict(md)
                md["vector_id"] = ids_row[i]

            chunk_id = ids_row[i] if (ids_row and i < len(ids_row)) else f"{session_id}:{i}"
            hits.append(RetrievedChunk(chunk_id, text, md, scv))
            scores.append(scv)

        # Optional threshold filtering.
        if score_threshold is not None and hits:
            thr = float(score_threshold)
            filt_hits: List[RetrievedChunk] = []
            filt_scores: List[float] = []
            for h, sc in zip(hits, scores):
                if sc >= thr:
                    filt_hits.append(h)
                    filt_scores.append(sc)
            hits = filt_hits
            scores = filt_scores

        used = bool(hits)

        return {
            "used": used,
            "hits": hits,
            "scores": scores,
            "debug": {
                "used": used,
                "hits_count": len(hits),
                "top_k": int(top_k),
                "score_threshold": score_threshold,
                "where": where,
                "provider": getattr(self._vectorstore_manager, "provider", None),  # optional; can be removed
            },
        }


    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    async def _ingest_single_attachment(
        self,
        attachment: AttachmentRef,
        *,
        session_id: str,
        user_id: str,
        tenant_id: Optional[str],
        workspace_id: Optional[str],
    ) -> IngestionResult:
        """
        End-to-end ingestion pipeline for a single AttachmentRef.
        """
        # 1) Resolve AttachmentRef → Path (or raise FileNotFoundError/ValueError)
        path: Path = await self._resolver.resolve_to_path(attachment)

        # 2) Build base metadata that we want on every chunk
        base_metadata: Dict[str, Any] = {
            "attachment_id": attachment.id,
            "attachment_type": attachment.type,
            "session_id": session_id,
            "user_id": user_id,
            "tenant_id": tenant_id,
            "workspace_id": workspace_id,
        }
        if attachment.metadata:
            base_metadata.update(attachment.metadata)

        # 3) Use IntergraxDocumentsLoader.load_document(...) for a single file
        def _metadata_callback(doc: Document, p: Path) -> Dict[str, Any]:
            """
            Custom metadata callback for the IntergraxDocumentsLoader.

            It receives each loaded Document and its Path, and returns a dict
            merged into doc.metadata. We always inject our base_metadata, but
            we do not override keys that the loader already set (unless they
            are absent).
            """
            merged = dict(base_metadata)
            # Optionally, we could inspect doc.metadata here and adjust.
            return merged

        docs: List[Document] = self._loader.load_document(
            str(path),
            use_default_metadata=True,
            call_custom_metadata=_metadata_callback,
        )

        if not docs:
            return IngestionResult(
                attachment_id=attachment.id,
                attachment_type=attachment.type,
                num_chunks=0,
                vector_ids=[],
                metadata={
                    "reason": "no_documents_loaded",
                    "source_path": str(path),
                },
            )

        # 4) Split into chunks via IntergraxDocumentsSplitter
        chunks: List[Document] = self._splitter.split_documents(docs)

        if not chunks:
            return IngestionResult(
                attachment_id=attachment.id,
                attachment_type=attachment.type,
                num_chunks=0,
                vector_ids=[],
                metadata={
                    "reason": "no_chunks_generated",
                    "source_path": str(path),
                },
            )

        # 5) Embed chunks and store in vectorstore
        #
        # The IntergraxEmbeddingManager / IntergraxVectorstoreManager in your
        # project are currently synchronous. However, to keep this runtime
        # future-proof, we support both sync and async interfaces.
        #
        # Pattern:
        #   result = func(...)
        #   if inspect.iscoroutine(result): await it
        #   else: use it directly

        # 5a) Embeddings
        try:
            # Preferred path: the manager exposes embed_documents(chunks)
            embed_result = self._embedding_manager.embed_documents(chunks)

            if inspect.iscoroutine(embed_result):
                embed_result = await embed_result

            # Normalize result: either (embeddings, docs) or embeddings-only
            if isinstance(embed_result, tuple) and len(embed_result) == 2:
                embeddings, aligned_docs = embed_result
            else:
                embeddings = embed_result
                aligned_docs = chunks

        except AttributeError:
            # Fallback: manager exposes only embed_texts(texts)
            texts = [c.page_content for c in chunks]
            embed_result = self._embedding_manager.embed_texts(texts)

            if inspect.iscoroutine(embed_result):
                embeddings = await embed_result
            else:
                embeddings = embed_result

            aligned_docs = chunks

        # 5b) Enrich metadata on documents with base_metadata
        #
        # This ensures that later retrieval can filter by session/tenant/user/etc.
        for d in aligned_docs:
            d.metadata = {**(d.metadata or {}), **base_metadata}

        # 5c) Generate stable IDs for each stored chunk
        ids = [f"{attachment.id}-{i}" for i in range(len(aligned_docs))]

        # 5d) Store in vectorstore using the current IntergraxVectorstoreManager API.
        #
        # We assume a signature similar to:
        #   add_documents(
        #       documents: Sequence[Document],
        #       embeddings: Optional[Any] = None,
        #       ids: Optional[Sequence[str]] = None,
        #       base_metadata: Optional[Dict[str, Any]] = None,
        #       ...
        #   )
        add_result = self._vectorstore_manager.add_documents(
            documents=aligned_docs,
            embeddings=embeddings,
            ids=ids,
            base_metadata=base_metadata,
        )

        if inspect.iscoroutine(add_result):
            stored_ids = await add_result
        else:
            stored_ids = add_result

        # Normalize stored_ids: if the manager returns None, fall back to local ids
        if stored_ids is None:
            vector_ids = ids
        else:
            vector_ids = list(stored_ids)

        return IngestionResult(
            attachment_id=attachment.id,
            attachment_type=attachment.type,
            num_chunks=len(aligned_docs),
            vector_ids=vector_ids,
            metadata={
                "source_path": str(path),
                "session_id": session_id,
                "user_id": user_id,
                "tenant_id": tenant_id,
                "workspace_id": workspace_id,
            },
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/engine_plan_models.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=engine_plan_models.py
# LINES: 323
# SHA256: 17f0b8c31ab45656bc9190afc89bf021e4e3b77c885f4d65251281722d8d9664
# SYMBOLS:
#   - class EngineNextStep
#   - class PlanIntent
#   - class EnginePlan
#   - class PlannerPromptConfig
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations
from dataclasses import dataclass, field
from enum import Enum
import pprint
from typing import Any, Dict, Literal, Optional


# -----------------------------
# Typed plan schema
# -----------------------------

class EngineNextStep(str, Enum):
    CLARIFY = "clarify"
    WEBSEARCH = "websearch"
    TOOLS = "tools"
    RAG = "rag"
    SYNTHESIZE = "synthesize"
    FINALIZE = "finalize"
    

class PlanIntent(str, Enum):
    GENERIC = "generic"
    FRESHNESS = "freshness"
    PROJECT_ARCHITECTURE = "project_architecture"
    CLARIFY = "clarify"
    

@dataclass(frozen=False)
class EnginePlan:
    version: str
    intent: PlanIntent

    # Debug / trace only
    reasoning_summary: str = ""

    # Clarify only
    ask_clarifying_question: bool = False
    clarifying_question: Optional[str] = None

    # Next action (policy routing for this iteration)
    next_step: Optional[EngineNextStep] = None

    # Soft preferences for this iteration (NOT hard constraints)
    use_websearch: bool = False
    use_user_longterm_memory: bool = False
    use_rag: bool = False
    use_tools: bool = False

    debug: Dict[str, Any] = field(default_factory=dict)


    def print_pretty(self) -> None:
        pprint.pprint({
            "version": self.version,
            "intent": self.intent.value if isinstance(self.intent, Enum) else str(self.intent),
            "reasoning_summary": self.reasoning_summary,
            "ask_clarifying_question": self.ask_clarifying_question,
            "clarifying_question": self.clarifying_question,
            "use_websearch": self.use_websearch,
            "use_user_longterm_memory": self.use_user_longterm_memory,
            "use_rag": self.use_rag,
            "use_tools": self.use_tools,
            "debug": self.debug,
        })
    

@dataclass(frozen=True)
class PlannerPromptConfig:
    version: str = "default"
    system_prompt: Optional[str] = None


BASE_PLANNER_SYSTEM_PROMPT = """You are EnginePlanner for Intergrax Drop-In Knowledge Runtime.
Return a SINGLE JSON object only. No prose. No markdown. No comments.
The JSON MUST match the provided JSON Schema EXACTLY (no extra keys).
Do NOT include chain-of-thought. Put a short high-level note in reasoning_summary.

Hard constraints:
- If a capability is unavailable, its corresponding use_* flag MUST be false.
- If intent is 'clarify', ask_clarifying_question MUST be true and clarifying_question MUST be a single question.
- If intent is not 'clarify', ask_clarifying_question MUST be false and clarifying_question MUST be null.

Intent definitions:
- generic: general answer using internal knowledge; no external retrieval needed.
- freshness: requires up-to-date info; prefer websearch if available.
- project_architecture: depends on user's project history/preferences; prefer user long-term memory if available.
- clarify: question is ambiguous/missing info; ask exactly one clarifying question.

Tools policy (STRICT):
- Default: use_tools=false.
- Set use_tools=true ONLY if the user explicitly requests tool usage or the task requires an external action/data source (e.g., web lookup, calling tools, operating on user resources).

Examples:
- Q: 'Explain async retry in Python' -> use_tools=false
- Q: 'What are the most recent changes to the OpenAI Responses API? Provide dates.' -> use_websearch=true (if available), use_tools=false
- Q: 'Search the web and summarize the latest changes to the OpenAI Responses API' -> use_websearch=true (if available), use_tools=true ONLY if your tools system is the websearch tool.
"""


# DEFAULT_PLANNER_SYSTEM_PROMPT = """
#         You are EnginePlanner for Intergrax Drop-In Knowledge Runtime.
#         Return a SINGLE JSON object only. No prose. No markdown. No comments.
#         The JSON MUST match the provided JSON Schema EXACTLY (no extra keys).
#         Do NOT include chain-of-thought. Put a short high-level note in reasoning_summary.

#         Hard constraints:
#         - If a capability is unavailable, its corresponding use_* flag MUST be false.
#         - If intent is 'clarify', ask_clarifying_question MUST be true and clarifying_question MUST be a single question.
#         - If intent is not 'clarify', ask_clarifying_question MUST be false and clarifying_question MUST be null.

#         Intent definitions:
#         - generic: general answer using internal knowledge; no external retrieval needed.
#         - freshness: requires up-to-date info; prefer websearch if available.
#         - project_architecture: depends on user's project history/preferences; prefer user long-term memory if available.
#         - clarify: question is ambiguous/missing info; ask exactly one clarifying question.

#         Tools policy (STRICT):
#         - Default: use_tools=false.
#         - Set use_tools=true in TWO cases only:
#         (A) External action/data source is required AND is handled by the tools pipeline (NOT websearch).
#         (B) Deterministic transformation/extraction is required with a strict output contract, e.g.:
#             - "only output JSON", "exact list", "sorted", "unique", "no prose", "return only ..."
#             - parsing/transforming provided JSON/CSV/XML/text into a precise structured output
#         - If use_tools=true: next_step MUST be "tools".

#         Examples:
#         - Q: 'Explain async retry in Python' -> use_tools=false
#         - Q: 'What are the most recent changes to the OpenAI Responses API? Provide dates.' -> use_websearch=true (if available), use_tools=false
#         - Q: 'Search the web and summarize the latest changes to the OpenAI Responses API' -> use_websearch=true (if available), use_tools=true ONLY if your tools system is the websearch tool.

#         Intent field constraints (STRICT):
#         - intent MUST be EXACTLY one of: "generic", "freshness", "project_architecture", "clarify".
#         - Do NOT output any other intent value (e.g., "compare", "choose", "optimize", "decision").

#         User long-term memory policy (STRICT, HARD RULE):
#         - use_user_longterm_memory MUST be true ONLY when intent is exactly "project_architecture" AND the capability is available.
#         - For intents "generic", "freshness", and "clarify": use_user_longterm_memory MUST be false.

#         Websearch vs Tools policy (STRICT, HARD RULE):
#         - In this runtime, websearch is NOT part of tools. Websearch is a separate pipeline.
#         - Therefore, if use_websearch=true, then use_tools MUST be false.
#         - Set use_tools=true ONLY for non-websearch external actions handled by the tools pipeline.

#         Clarify policy (STRICT, HARD RULE):
#         - Use intent="clarify" when the user's request is ambiguous OR missing required details to answer safely/correctly.
#         - Missing-required-info triggers (NON-NEGOTIABLE). If ANY trigger matches => intent MUST be "clarify":
#         - user mentions an exception/error/bug but provides no traceback/stack trace
#         - no error message, no reproduction steps, no environment/version, no minimal example when needed
#         - "it doesn't work" / "I got an error" without the actual error text
#         - For missing traceback specifically: clarifying_question MUST ask for the full traceback and minimal repro in one sentence.
#         - If intent="clarify": next_step MUST be "clarify", ask_clarifying_question MUST be true, and clarifying_question MUST be exactly one question.
#         - If intent!="clarify": ask_clarifying_question MUST be false and clarifying_question MUST be null.

#         RAG policy (STRICT, HARD RULE):
#         - If intent is "generic" or "freshness" or "clarify": use_rag MUST be false.
#         - use_rag MAY be true only when intent is exactly "project_architecture" AND capability is available.
#         """


# DEFAULT_PLANNER_SYSTEM_PROMPT = """
# You are EnginePlanner for Intergrax Drop-In Knowledge Runtime.

# Return a SINGLE JSON object only. No prose. No markdown. No comments.
# The JSON MUST match the provided JSON Schema EXACTLY (no extra keys).
# Do NOT include chain-of-thought. Put a short high-level note in reasoning_summary.

# Hard constraints:
# - If a capability is unavailable, its corresponding use_* flag MUST be false.
# - If intent is 'clarify': ask_clarifying_question MUST be true and clarifying_question MUST be a single question.
# - If intent is not 'clarify': ask_clarifying_question MUST be false and clarifying_question MUST be null.
# - If use_tools=true: next_step MUST be "tools".
# - If use_websearch=true: next_step MUST be "websearch".

# Intent definitions:
# - generic: general answer using internal knowledge; no external retrieval needed; no strict output contract.
# - freshness: requires up-to-date info; prefer websearch if available.
# - project_architecture: depends on user's project and preferences; can use user long-term memory and project RAG if available.
# - clarify: question is ambiguous/missing required info; ask exactly one clarifying question.

# Decision procedure (follow in order):
# 1) CLARIFY gate:
#    If the request is ambiguous OR missing required details to answer safely/correctly => intent="clarify".
# 2) FRESHNESS gate:
#    If the request requires recent/dated information not safely answerable from general knowledge => intent="freshness".
# 3) PROJECT_ARCHITECTURE gate:
#    If the request depends on the user's project/codebase/history/preferences => intent="project_architecture".
# 4) Otherwise => intent="generic".

# Tools policy (STRICT):
# - Default: use_tools=false.
# - Set use_tools=true ONLY in TWO cases:
#   (A) External action/data source is required AND is handled by the tools pipeline (NOT websearch).
#   (B) The user requires a deterministic transformation/extraction with a strict output contract (machine-readable output),
#       where format correctness matters more than prose quality.

# Strict output contract signals (non-exhaustive; treat as strong indicators for (B)):
# - The user explicitly demands format constraints such as:
#   "only output ...", "no prose", "no explanation", "return only", "exact", "verbatim", "must be valid JSON",
#   "JSON array only", "schema", "sorted", "unique", "deduplicate", "extract fields", "parse", "transform".
# - The input includes structured data (JSON/CSV/XML/logs) and the task is to compute/transform it precisely.
# If these signals are present, prefer use_tools=true even if the transformation seems simple.

# Websearch vs Tools policy (STRICT, HARD RULE):
# - In this runtime, websearch is NOT part of tools. Websearch is a separate pipeline.
# - Therefore, if use_websearch=true, then use_tools MUST be false.
# - Set use_tools=true ONLY for non-websearch external actions handled by the tools pipeline,
#   or for deterministic transformations under the strict output contract rule (B).

# User long-term memory policy (STRICT, HARD RULE):
# - use_user_longterm_memory MUST be true ONLY when intent is exactly "project_architecture" AND the capability is available.
# - For intents "generic", "freshness", and "clarify": use_user_longterm_memory MUST be false.

# RAG policy (STRICT, HARD RULE):
# - If intent is "generic" or "freshness" or "clarify": use_rag MUST be false.
# - use_rag MAY be true only when intent is exactly "project_architecture" AND capability is available.

# Clarify policy (STRICT, HARD RULE):
# - Missing-required-info triggers (NON-NEGOTIABLE). If ANY trigger matches => intent MUST be "clarify":
#   - user mentions an exception/error/bug but provides no traceback/stack trace
#   - no error message, no reproduction steps, no environment/version, no minimal example when needed
#   - "it doesn't work" / "I got an error" without the actual error text
# - For missing traceback specifically:
#   clarifying_question MUST ask for the full traceback and a minimal repro in one sentence.
# - If intent="clarify": next_step MUST be "clarify", ask_clarifying_question MUST be true,
#   and clarifying_question MUST be exactly one question.
# - If intent!="clarify": ask_clarifying_question MUST be false and clarifying_question MUST be null.

# Intent field constraints (STRICT):
# - intent MUST be EXACTLY one of: "generic", "freshness", "project_architecture", "clarify".
# - Do NOT output any other intent value.

# Examples (illustrative, not exhaustive):
# - Explanation request without strict output contract -> intent="generic", use_tools=false.
# - Up-to-date changes with dates -> intent="freshness", use_websearch=true (if available), use_tools=false.
# - Question about user's runtime/codebase -> intent="project_architecture", use_rag/use_user_longterm_memory as available.
# - "I have an error but no traceback" -> intent="clarify", next_step="clarify".
# - "Return only valid JSON / exact list / sorted unique values from provided data" -> use_tools=true, next_step="tools".
# """


DEFAULT_PLANNER_SYSTEM_PROMPT = """
You are EnginePlanner for Intergrax Drop-In Knowledge Runtime.

Return a SINGLE JSON object only. No prose. No markdown. No comments.
The JSON MUST match the provided JSON Schema EXACTLY (no extra keys).
Do NOT include chain-of-thought. Put a short high-level note in reasoning_summary.

Core idea:
- intent describes the user's request type.
- use_* flags describe which capabilities are likely needed to answer well.
- next_step selects the FIRST execution step for StepPlanner (subsequent steps may be planned iteratively later).

Hard constraints:
- If a capability is unavailable, its corresponding use_* flag MUST be false.
- If intent is 'clarify': ask_clarifying_question MUST be true and clarifying_question MUST be a single question.
- If intent is not 'clarify': ask_clarifying_question MUST be false and clarifying_question MUST be null.
- If next_step is "tools": use_tools MUST be true.
- If next_step is "websearch": use_websearch MUST be true.
- If next_step is "clarify": intent MUST be "clarify".

Intent definitions (choose ONE):
- generic: can be answered from general knowledge or provided context; no requirement for up-to-date facts.
- freshness: the user explicitly asks for latest/recent/current info OR correctness depends on post-cutoff changes.
- project_architecture: the answer depends on the user's specific project/runtime/codebase/conventions/preferences
  (including Intergrax/Mooff details), not just general knowledge.
- clarify: essential information is missing to answer safely/correctly (for the user's stated goal).

Planning procedure (do NOT use rigid gates; evaluate needs):
1) Determine whether the user wants (a) diagnosis of a concrete situation OR (b) general guidance.
   - If diagnosis requires missing essentials => intent="clarify".
2) Determine whether the question requires up-to-date facts (post-training) => set intent="freshness".
3) Determine whether the question depends on the user's project specifics => set intent="project_architecture".
4) Otherwise => intent="generic".
Note: If both (2) and (3) apply, prefer intent="project_architecture" (because the answer must fit the user's system),
but still set use_websearch/use_rag/use_user_longterm_memory as needed.

Capability selection (truthful, flexible):
- use_user_longterm_memory:
  Set true when user preferences/history/project conventions are helpful to answer correctly or in the expected style,
  AND capability is available. This can apply to any intent except "clarify" (where you must first ask).
- use_rag:
  Set true when project/docs knowledgebase is likely needed to answer accurately (APIs, code, internal docs, specs),
  AND capability is available. This can apply to generic or project_architecture intents.
- use_websearch:
  Set true when the answer depends on current/up-to-date external information, AND capability is available.
- use_tools:
  Set true when the answer requires:
  (A) non-websearch external actions/data sources handled by tools pipeline, OR
  (B) high-reliability deterministic transformation/validation where format correctness is critical.

Deterministic transformation rule (STRONG):
- If the user provides structured input (JSON/CSV/XML/logs) AND asks to compute/transform it with exactness
  (e.g., "exact", "only output", "sorted", "unique", "deduplicate", "extract fields", "parse", "transform"),
  then set use_tools=true and next_step="tools" (unless tools capability is unavailable).
- Treat "Only output the JSON array ..." + "sorted/unique/exact" as a decisive signal for tools.
- Do NOT set use_tools=true for "please answer in JSON" if no transformation/validation is required.

Websearch and Tools relationship:
- Websearch is a separate pipeline from tools.
- They are NOT logically mutually exclusive in the overall solution.
- However, next_step MUST choose only one FIRST step. If both may be needed, pick the one that should happen first:
  - Prefer websearch first when external freshness is needed.
  - Prefer tools first when deterministic transformation/validation is needed on provided data.

Clarify policy:
- Use intent="clarify" ONLY when missing information blocks the user's stated goal.
- If the user reports an error and asks for diagnosis but provides no traceback/logs, ask for:
  full traceback + minimal repro + environment/version in ONE question.
- If the user asks for general debugging guidance (not diagnosis), do NOT force clarify.

Choosing next_step:
- If intent="clarify": next_step="clarify".
- Else if use_websearch=true and freshness is a key requirement: next_step="websearch".
- Else if use_tools=true and deterministic transformation/validation is the primary requirement: next_step="tools".
- Else: next_step="synthesize" (or whatever your schema uses for normal answering).

In reasoning_summary (short):
- State why the intent was chosen and which capability is needed first.
- If both websearch and tools are likely needed, mention the expected sequence in one short phrase.
"""

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/engine_planner.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.engine_planner
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=engine_planner.py
# LINES: 385
# SHA256: b52a6588b3ad2ec98298ba1a394c310c574ee8a5ed9444801dab1ee88fa7f377
# SYMBOLS:
#   - class EnginePlanner
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

import inspect
import json
from typing import List, Optional
from intergrax.llm.messages import ChatMessage
from intergrax.llm_adapters.llm_adapter import LLMAdapter
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import DEFAULT_PLANNER_SYSTEM_PROMPT, EngineNextStep, EnginePlan, PlanIntent, PlannerPromptConfig
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest



class EnginePlanner:
    """
    LLM-based planner that outputs a typed EnginePlan.

    IMPORTANT:
    - No heuristics: the LLM decides based on PlannerInput + capabilities.
    - Output must be JSON only (we parse & validate).
    """

    def __init__(self, *, llm_adapter: LLMAdapter) -> None:
        self._llm = llm_adapter

    async def plan(
        self,
        *,
        req: RuntimeRequest,
        state: RuntimeState,
        config: RuntimeConfig,
        prompt_config: Optional[PlannerPromptConfig] = None,
        run_id: Optional[str] = None,
    ) -> EnginePlan:
        messages = self._build_planner_messages(
            req=req, 
            state=state, 
            config=config,
            prompt_config=prompt_config,
        )

        raw = self._llm.generate_messages(messages, run_id=run_id)
        if inspect.iscoroutine(raw):
            raw = await raw
        if not isinstance(raw, str):
            raw = str(raw)

        # Single source of truth: parsing extracts JSON and loads it
        plan = self._parse_plan(raw)

        # Capability clamp
        plan = self._validate_against_capabilities(plan=plan, state=state)

        # Merge debug (do not overwrite parser debug)
        if plan.debug is None:
            plan.debug = {}

        plan.debug.update(
            {
                "planner_raw_len": len(raw),
                "planner_raw_preview": raw[:400],
                "planner_raw_tail_preview": raw[-400:],
            }
        )

        return plan
    

    def _validate_against_capabilities(self, *, plan: EnginePlan, state: RuntimeState) -> EnginePlan:
        """
        Hard capability clamp. No heuristics, no intent changes.
        Only disables flags that are not available in the current runtime.
        """

        if plan.use_websearch and not state.cap_websearch_available:
            plan.use_websearch = False

        if plan.use_user_longterm_memory and not state.cap_user_ltm_available:
            plan.use_user_longterm_memory = False

        if plan.use_rag and not state.cap_rag_available:
            plan.use_rag = False

        if plan.use_tools and not state.cap_tools_available:
            plan.use_tools = False

        # Optional: record clamp info for debugging
        if plan.debug is None:
            plan.debug = {}

        plan.debug["capability_clamp"] = {
            "websearch_available": state.cap_websearch_available,
            "user_ltm_available": state.cap_user_ltm_available,
            "rag_available": state.cap_rag_available,
            "tools_available": state.cap_tools_available,
            "use_websearch": plan.use_websearch,
            "use_user_longterm_memory": plan.use_user_longterm_memory,
            "use_rag": plan.use_rag,
            "use_tools": plan.use_tools,
        }

        return plan

    # -----------------------------
    # Prompting
    # -----------------------------

    def _build_planner_messages(
        self,
        *,
        req: RuntimeRequest,
        state: RuntimeState,
        config: RuntimeConfig,
        prompt_config: Optional[PlannerPromptConfig] = None,
    ) -> List[ChatMessage]:
        """
        Build minimal, low-variance planner messages.

        The model must output a SINGLE JSON object that matches the schema exactly.
        The output is parsed into EnginePlan (simplified).
        """

        # Capabilities are hard constraints (runtime will clamp again as a safety net).
        caps = {
            "websearch_available": state.cap_websearch_available,
            "user_ltm_available": state.cap_user_ltm_available,
            "rag_available": state.cap_rag_available,
            "tools_available": state.cap_tools_available,
            "attachments_present": req.attachments and len(req.attachments or []) > 0,
        }

        # Strict JSON schema (minimal surface area, no extra keys).
        schema = {
            "type": "object",
            "additionalProperties": False,
            "required": [
                "version",
                "intent",
                "next_step",
                "reasoning_summary",
                "ask_clarifying_question",
                "clarifying_question",
                "use_websearch",
                "use_user_longterm_memory",
                "use_rag",
                "use_tools",
            ],
            "properties": {
                "version": {"type": "string"},
                "intent": {
                    "type": "string",
                    "enum": ["generic", "freshness", "project_architecture", "clarify"],
                },
                "next_step": {
                    "type": "string",
                    "enum": [
                        "clarify",
                        "websearch",
                        "tools",
                        "rag",
                        "synthesize",
                        "finalize",
                    ],
                },
                "reasoning_summary": {"type": "string"},
                "ask_clarifying_question": {"type": "boolean"},
                "clarifying_question": {"type": ["string", "null"]},
                "use_websearch": {"type": "boolean"},
                "use_user_longterm_memory": {"type": "boolean"},
                "use_rag": {"type": "boolean"},
                "use_tools": {"type": "boolean"},
            },
        }

        system_prompt = DEFAULT_PLANNER_SYSTEM_PROMPT

        if prompt_config is not None and prompt_config.system_prompt:
            system_prompt = prompt_config.system_prompt.strip()

        # User message: provide only needed context + schema.
        user_lines: List[str] = []
        user_lines.append("CAPABILITIES (hard constraints):")
        user_lines.append(json.dumps(caps, ensure_ascii=False))
        user_lines.append("")

        user_lines.append("USER QUERY:")
        user_lines.append((req.message or "").strip())
        user_lines.append("")

        # Minimal, explicit rules for next_step to reduce ambiguity.
        user_lines.append("RULES FOR next_step:")
        user_lines.append('- If intent == "clarify": next_step MUST be "clarify".')
        user_lines.append('- If intent != "clarify": next_step MUST NOT be "clarify".')
        user_lines.append('Clarify intent should be used ONLY when the user request is ambiguous or missing critical information.')
        user_lines.append('Do NOT choose clarify if you can answer with a reasonable technical/general response without asking follow-ups.')
        user_lines.append('Do NOT choose clarify for broad/open questions; answer them as GENERIC and use next_step="synthesize".')

        user_lines.append('- Choose exactly one next_step for THIS iteration.')
        user_lines.append('- Use "websearch" for freshness/external information.')
        user_lines.append('- Use "rag" for internal documents or user long-term memory.')
        user_lines.append('- Use "tools" only when tool execution is required.')
        user_lines.append('- Use "synthesize" when you have enough information to draft an answer.')
        user_lines.append('- Use "finalize" only when you can return the final answer now.')
        user_lines.append("")

        user_lines.append("JSON SCHEMA:")
        user_lines.append(json.dumps(schema, ensure_ascii=False))
        user_lines.append("")
        user_lines.append("OUTPUT JSON:")

        return [
            ChatMessage(role="system", content=system_prompt),
            ChatMessage(role="user", content="\n".join(user_lines)),
        ]




    # -----------------------------
    # Parsing & validation
    # -----------------------------

    def _extract_json_object(self, s: str) -> str:
        """
        Minimal safety: extract the first {...} block.
        This is not a heuristic planner; it's just robust parsing for LLM outputs.
        """
        start = s.find("{")
        end = s.rfind("}")
        if start < 0 or end < 0 or end <= start:
            raise ValueError("Planner did not return a JSON object.")
        return s[start : end + 1]

    def _parse_plan(self, raw: str) -> EnginePlan:
        """
        Parse strict JSON (as specified by _build_planner_messages) into EnginePlan.

        Expected keys:
        version, intent, next_step, reasoning_summary, ask_clarifying_question, clarifying_question,
        use_websearch, use_user_longterm_memory, use_rag, use_tools
        """
        js = self._extract_json_object(raw)

        try:
            data = json.loads(js)
        except Exception as e:
            raise ValueError(f"Invalid JSON from LLM: {e}") from e

        if not isinstance(data, dict):
            raise ValueError("Planner output must be a JSON object.")

        def req_bool(k: str) -> bool:
            if k not in data:
                raise ValueError(f"Missing required key: {k}")
            v = data[k]
            if isinstance(v, bool):
                return v
            raise ValueError(f"Key '{k}' must be boolean.")

        def req_str(k: str) -> str:
            if k not in data:
                raise ValueError(f"Missing required key: {k}")
            v = data[k]
            if isinstance(v, str):
                return v
            raise ValueError(f"Key '{k}' must be string.")

        def req_str_or_null(k: str) -> Optional[str]:
            if k not in data:
                raise ValueError(f"Missing required key: {k}")
            v = data[k]
            if v is None:
                return None
            if isinstance(v, str):
                vv = v.strip()
                return vv
            raise ValueError(f"Key '{k}' must be string or null.")

        def opt_version_str(k: str, default: str = "1.0") -> str:
            """
            Version is metadata. Be tolerant: accept string/number/null/missing.
            Always return a non-empty string.
            """
            if k not in data:
                return default

            v = data[k]
            if v is None:
                return default

            if isinstance(v, str):
                s = v.strip()
                return s or default

            if isinstance(v, (int, float)):
                return str(v)

            raise ValueError(f"Key '{k}' must be string, number, or null.")

        version = opt_version_str("version", default="1.0")

        intent_raw = req_str("intent").strip()
        try:
            intent = PlanIntent(intent_raw)
        except Exception:
            raise ValueError(
                f"Invalid intent '{intent_raw}'. Allowed: generic|freshness|project_architecture|clarify."
            )

        # next_step -> EngineNextStep (Enum)
        next_step_raw = req_str("next_step").strip()
        try:
            next_step = EngineNextStep(next_step_raw)
        except Exception:
            # Tolerant: keep None and let fallback decide
            next_step = None

        reasoning_summary = req_str("reasoning_summary").strip()

        ask_clarify = req_bool("ask_clarifying_question")
        clar_q = req_str_or_null("clarifying_question")

        use_web = req_bool("use_websearch")
        use_ltm = req_bool("use_user_longterm_memory")
        use_rag = req_bool("use_rag")
        use_tools = req_bool("use_tools")

        # Deterministic consistency rules (independent from model compliance)
        if intent == PlanIntent.CLARIFY:
            ask_clarify = True
            if not clar_q:
                # If missing, force a safe generic clarifier.
                clar_q = "Could you clarify what you mean and what outcome you want?"
            # In clarify mode, retrieval is always off
            use_web = use_ltm = use_rag = use_tools = False
            next_step = EngineNextStep.CLARIFY
        else:
            # Non-clarify must not ask clarifying question
            if ask_clarify:
                # If model set it true incorrectly, force clarify intent
                intent = PlanIntent.CLARIFY
                if not clar_q:
                    clar_q = "Could you clarify what you mean and what outcome you want?"
                use_web = use_ltm = use_rag = use_tools = False
                next_step = EngineNextStep.CLARIFY
            else:
                clar_q = None
                # In non-clarify, next_step must not be clarify
                if next_step == EngineNextStep.CLARIFY:
                    next_step = None

        # Deterministic fallback for next_step (never leave None for runtime loop)
        if next_step is None:
            if use_web:
                next_step = EngineNextStep.WEBSEARCH
            elif use_tools:
                next_step = EngineNextStep.TOOLS
            elif use_rag or use_ltm:
                next_step = EngineNextStep.RAG
            else:
                next_step = EngineNextStep.SYNTHESIZE

        return EnginePlan(
            version=version,
            intent=intent,
            reasoning_summary=reasoning_summary,
            ask_clarifying_question=ask_clarify,
            clarifying_question=clar_q,
            next_step=next_step,
            use_websearch=use_web,
            use_user_longterm_memory=use_ltm,
            use_rag=use_rag,
            use_tools=use_tools,
            debug={
                "raw_json": data,
                "planner_json_len": len(js),
                "next_step_raw": next_step_raw,
            },
        )



# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/plan_builder_helper.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.plan_builder_helper
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=plan_builder_helper.py
# LINES: 77
# SHA256: e386380d18e438e1196a8888c5061ab4d9da72db845f94bc6171512f722269c5
# SYMBOLS:
#   - async def build_plan()
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

from typing import Optional, List

from intergrax.llm.messages import ChatMessage
from intergrax.llm_adapters.llm_usage_track import LLMUsageTracker
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest
from intergrax.runtime.drop_in_knowledge_mode.planning.engine_planner import EnginePlanner
from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import EnginePlan


async def build_plan(
    *,
    config: RuntimeConfig,
    message: str,
    user_id: str,
    session_id: Optional[str] = None,
    run_id: Optional[str] = None,
    instructions: Optional[str] = None,
    attachments: Optional[list] = None,
    base_history: Optional[List[ChatMessage]] = None,
    profile_user_instructions: Optional[str] = None,
    profile_org_instructions: Optional[str] = None
) -> EnginePlan:
    """
    Minimal helper:
      EnginePlanner.plan() -> EnginePlan
    """

    inner_session_id = session_id or "plan_builder_helper_session"
    inner_run_id = run_id or "plan_builder_helper_run_id"

    planner = EnginePlanner(llm_adapter=config.llm_adapter)

    # 1. Request
    req = RuntimeRequest(
        user_id=user_id,
        session_id=inner_session_id,
        message=message,
        instructions=instructions,
        attachments=attachments or [],
    )

    # 2. State
    state = RuntimeState(
        request=req,
        run_id=inner_run_id,
        llm_usage_tracker=LLMUsageTracker(run_id=inner_run_id),
    )

    state.llm_usage_tracker.register_adapter(config.llm_adapter, label="core_adapter")

    state.base_history = base_history or []

    state.profile_user_instructions = profile_user_instructions
    state.profile_org_instructions = profile_org_instructions

    state.cap_rag_available = config.enable_rag
    state.cap_user_ltm_available = config.enable_user_longterm_memory
    state.cap_attachments_available = bool(attachments and len(attachments)>0)
    state.cap_websearch_available = config.enable_websearch
    state.cap_tools_available = config.tools_mode != "off"

    # 3. Engine plan
    engine_plan = await planner.plan(
        req=req,
        state=state,
        config=config,
        run_id=inner_run_id,
    )

    return engine_plan

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/runtime_step_handlers.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.runtime_step_handlers
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=runtime_step_handlers.py
# LINES: 136
# SHA256: 3fc4960f4dc0b36dbcae6c863cecc4063863f604042ea01f874ad7574d409ee1
# SYMBOLS:
#   - class RuntimeStep
#   - def _now_iso()
#   - def _ok_result()
#   - def _failed_result()
#   - class RuntimeStepBinding
#   - def make_runtime_step_handler()
#   - def build_runtime_step_registry()
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Awaitable, Callable, Dict, Optional

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import ExecutionStep, StepAction, StepId
from intergrax.runtime.drop_in_knowledge_mode.planning.step_executor_models import (
    StepError,
    StepErrorCode,
    StepExecutionContext,
    StepExecutionResult,
    StepHandler,
    StepHandlerRegistry,
    StepStatus,
)

# Your existing RuntimeStep protocol
from typing import Protocol


class RuntimeStep(Protocol):
    async def run(self, state: RuntimeState) -> None:
        ...


# -----------------------------
# Result helpers (minimal + deterministic)
# -----------------------------

def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def _ok_result(*, step: ExecutionStep, attempts: int = 1) -> StepExecutionResult:
    # Executor overwrites telemetry anyway (sequence/timestamps/duration) when it wraps results,
    # but StepExecutionResult requires these fields, so provide deterministic placeholders.
    ts = _now_iso()
    return StepExecutionResult(
        step_id=step.step_id,
        action=step.action,
        status=StepStatus.OK,
        sequence=0,
        started_at_utc=ts,
        ended_at_utc=ts,
        duration_ms=0,
        validated_params=None,
        output=None,
        meta=None,
        error=None,
        attempts=attempts,
    )


def _failed_result(*, step: ExecutionStep, message: str, details: Optional[dict] = None, attempts: int = 1) -> StepExecutionResult:
    ts = _now_iso()
    return StepExecutionResult(
        step_id=step.step_id,
        action=step.action,
        status=StepStatus.FAILED,
        sequence=0,
        started_at_utc=ts,
        ended_at_utc=ts,
        duration_ms=0,
        validated_params=None,
        output=None,
        meta=None,
        error=StepError(
            code=StepErrorCode.HANDLER_EXCEPTION,
            message=message,
            details=details,
        ),
        attempts=attempts,
    )


# -----------------------------
# RuntimeStep -> StepHandler adapter
# -----------------------------

@dataclass(frozen=True)
class RuntimeStepBinding:
    """
    One explicit binding:
      StepAction -> factory that returns a RuntimeStep instance
    """
    action: StepAction
    factory: Callable[[], RuntimeStep]


def make_runtime_step_handler(*, action: StepAction, factory: Callable[[], RuntimeStep]) -> StepHandler:
    """
    Adapter: ExecutionStep + ctx -> executes RuntimeStep.run(state)
    """
    async def _handler(step: ExecutionStep, ctx: StepExecutionContext) -> StepExecutionResult:
        # Defensive: executor already validates action/step_id consistency, but keep it tight.
        if step.action != action:
            return _failed_result(
                step=step,
                message="Handler called with unexpected action.",
                details={"expected": action.value, "got": step.action.value},
            )

        # Instantiate step (fresh instance to keep handlers stateless by default)
        runtime_step = factory()

        # Run
        try:
            await runtime_step.run(ctx.state)
            return _ok_result(step=step, attempts=1)
        except Exception as e:
            return _failed_result(
                step=step,
                message=f"RuntimeStep exception: {type(e).__name__}: {e}",
                details={"step_id": step.step_id.value, "action": step.action.value},
                attempts=1,
            )

    return _handler


def build_runtime_step_registry(*, bindings: Dict[StepAction, Callable[[], RuntimeStep]]) -> StepHandlerRegistry:
    """
    Explicit, strongly-typed mapping StepAction -> handler.

    bindings: Dict[StepAction, factory()]
    """
    handlers: Dict[StepAction, StepHandler] = {}
    for action, factory in bindings.items():
        handlers[action] = make_runtime_step_handler(action=action, factory=factory)

    return StepHandlerRegistry(handlers=handlers)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/step_executor.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.step_executor
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=step_executor.py
# LINES: 352
# SHA256: b07fcb4b4aaf9da5dfc8d4b9d5acdbd1dba69224552d065fbf258ba569026ac7
# SYMBOLS:
#   - class _DefaultExecContext
#   - class StepExecutor
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

import asyncio
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Mapping, Optional

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import (
    ExecutionPlan,
    ExecutionStep,
    FailurePolicyKind,
    StepAction,
    StepId,
)
from intergrax.runtime.drop_in_knowledge_mode.planning.step_executor_models import (
    PlanExecutionReport,
    ReplanCode,
    ReplanReason,
    StepError,
    StepErrorCode,
    StepExecutionContext,
    StepExecutionResult,
    StepExecutorConfig,
    StepHandlerRegistry,
    StepReplanRequested,
    StepStatus,
)


@dataclass
class _DefaultExecContext(StepExecutionContext):
    _state: RuntimeState
    _results: Dict[StepId, StepExecutionResult]
    _ordered: List[StepExecutionResult]
    _current_step: Optional[ExecutionStep] = None

    @property
    def state(self) -> RuntimeState:
        return self._state

    @property
    def current_step(self) -> Optional[ExecutionStep]:
        return self._current_step

    def set_current_step(self, step: Optional[ExecutionStep]) -> None:
        self._current_step = step

    @property
    def results(self) -> Mapping[StepId, StepExecutionResult]:
        return self._results

    @property
    def ordered_results(self) -> List[StepExecutionResult]:
        return self._ordered

    def set_result(self, result: StepExecutionResult) -> None:
        self._results[result.step_id] = result
        self._ordered.append(result)


class StepExecutor:
    """
    Executes an ExecutionPlan deterministically.

    The executor:
      - does NOT know runtime internals,
      - uses injected handlers (StepAction -> handler),
      - maintains a StepId -> StepExecutionResult store for dependencies.
    """

    def __init__(
        self,
        *,
        registry: StepHandlerRegistry,
        cfg: Optional[StepExecutorConfig] = None,
    ) -> None:
        self._registry = registry
        self._cfg = cfg or StepExecutorConfig()


    async def execute(self, *, plan: ExecutionPlan, state: RuntimeState) -> PlanExecutionReport:
        results: Dict[StepId, StepExecutionResult] = {}
        ordered: List[StepExecutionResult] = []
        ctx = _DefaultExecContext(_state=state, _results=results, _ordered=ordered)
        
        step_order: List[StepId] = [s.step_id for s in plan.steps]
        executed_order: List[StepId] = []

        replan_reason: Optional[str] = None
        plan_started_dt = datetime.now(timezone.utc)
        plan_started_at = plan_started_dt.isoformat()
        sequence: int = 0

        for step in plan.steps:
            ctx.set_current_step(step)
            executed_order.append(step.step_id)

            sequence += 1
            step_started_dt = datetime.now(timezone.utc)
            step_started_at = step_started_dt.isoformat()

            try:
                if not step.enabled:
                    step_ended_dt = datetime.now(timezone.utc)
                    res = StepExecutionResult(
                        step_id=step.step_id,
                        action=step.action,
                        status=StepStatus.SKIPPED,
                        sequence=sequence,
                        started_at_utc=step_started_at,
                        ended_at_utc=step_ended_dt.isoformat(),
                        duration_ms=int((step_ended_dt - step_started_dt).total_seconds() * 1000),
                        validated_params=None,
                        output=None,
                        meta=None,
                        error=None,
                        attempts=1,
                    )
                    ctx.set_result(res)
                    continue

                # Dependency gate: if any dep FAILED/REPLAN -> propagate stop
                if not self._deps_ok(step=step, results=results):
                    step_ended_dt = datetime.now(timezone.utc)
                    res = StepExecutionResult(
                        step_id=step.step_id,
                        action=step.action,
                        status=StepStatus.SKIPPED,
                        sequence=sequence,
                        started_at_utc=step_started_at,
                        ended_at_utc=step_ended_dt.isoformat(),
                        duration_ms=int((step_ended_dt - step_started_dt).total_seconds() * 1000),
                        validated_params=None,
                        output=None,
                        meta=None,
                        error=StepError(
                            code=StepErrorCode.DEPENDENCY_FAILED,
                            message="Skipped due to failed dependency.",
                            details={"depends_on": [d.value for d in step.depends_on]},
                        ),
                        attempts=1,
                    )
                    ctx.set_result(res)
                    if self._cfg.fail_fast:
                        break
                    continue

                try:
                    res = await self._run_step_with_policy(step=step, ctx=ctx)

                    step_ended_dt = datetime.now(timezone.utc)

                    final_res = StepExecutionResult(
                        step_id=res.step_id,
                        action=res.action,
                        status=res.status,
                        sequence=sequence,
                        started_at_utc=step_started_at,
                        ended_at_utc=step_ended_dt.isoformat(),
                        duration_ms=int((step_ended_dt - step_started_dt).total_seconds() * 1000),
                        validated_params=res.validated_params,
                        output=res.output,
                        meta=res.meta,
                        error=res.error,
                        attempts=res.attempts,
                    )

                    ctx.set_result(final_res)

                    if final_res.status in (StepStatus.FAILED, StepStatus.REPLAN_REQUESTED) and self._cfg.fail_fast:
                        if final_res.status == StepStatus.REPLAN_REQUESTED:
                            replan_reason = (final_res.error.message if final_res.error else None) or "replan_requested"
                        break

                except StepReplanRequested as e:
                    step_ended_dt = datetime.now(timezone.utc)
                    res = StepExecutionResult(
                        step_id=step.step_id,
                        action=step.action,
                        status=StepStatus.REPLAN_REQUESTED,
                        sequence=sequence,
                        started_at_utc=step_started_at,
                        ended_at_utc=step_ended_dt.isoformat(),
                        duration_ms=int((step_ended_dt - step_started_dt).total_seconds() * 1000),
                        validated_params=None,
                        output=None,
                        meta=None,
                        error=StepError(
                            code=StepErrorCode.REPLAN,
                            message=f"{e.code.value}: {e.reason.value}",
                            details={
                                "replan_code": e.code.value,
                                "replan_reason": e.reason.value,
                                "replan_details": e.details,
                            },
                        ),
                        attempts=1,
                    )
                    ctx.set_result(res)
                    continue

            finally:
                ctx.set_current_step(None)


        # Final output: last OK output if any (runtime can override this later)
        final_output: Optional[Any] = None
        final_step = next(
            (s for s in reversed(plan.steps) if s.action == StepAction.FINALIZE_ANSWER),
            None,
        )
        if final_step is not None:
            r = results.get(final_step.step_id)
            if r is not None and r.status == StepStatus.OK:
                final_output = r.output

        has_failed = any(r.status == StepStatus.FAILED for r in results.values())
        has_replan = any(r.status == StepStatus.REPLAN_REQUESTED for r in results.values())
        has_skipped_error = any(
            (r.status == StepStatus.SKIPPED and r.error is not None) for r in results.values()
        )

        ok = (replan_reason is None) and (not has_failed) and (not has_replan) and (not has_skipped_error)

        # If plan includes FINALIZE_ANSWER, require it to be OK for ok=True
        final_step = next(
            (s for s in reversed(plan.steps) if s.action == StepAction.FINALIZE_ANSWER),
            None,
        )
        if ok and final_step is not None:
            fr = results.get(final_step.step_id)
            if fr is None or fr.status != StepStatus.OK:
                ok = False

        plan_ended_dt = datetime.now(timezone.utc)
        plan_ended_at = plan_ended_dt.isoformat()
        plan_duration_ms = int((plan_ended_dt - plan_started_dt).total_seconds() * 1000)

        return PlanExecutionReport(
            plan_id=plan.plan_id,
            ok=ok,
            step_results=results,
            final_output=final_output,
            replan_reason=replan_reason,
            step_order=step_order,
            executed_order=executed_order,
            started_at_utc=plan_started_at,
            ended_at_utc=plan_ended_at,
            duration_ms=plan_duration_ms,
        )

    def _deps_ok(self, *, step: ExecutionStep, results: Mapping[StepId, StepExecutionResult]) -> bool:
        for dep in step.depends_on:
            r = results.get(dep)
            if r is None:
                raise RuntimeError(
                    f"Missing dependency result: step={step.step_id.value} depends_on={dep.value}"
                )
            if r.status in (StepStatus.FAILED, StepStatus.REPLAN_REQUESTED):
                return False
        return True


    async def _run_step_with_policy(self, *, step: ExecutionStep, ctx: StepExecutionContext) -> StepExecutionResult:
        policy = step.on_failure.policy
        max_retries = int(step.on_failure.max_retries)
        backoff_ms = int(step.on_failure.retry_backoff_ms)

        # attempts = 1 + retries, but hard capped
        allowed_attempts = min(1 + max_retries, int(self._cfg.max_attempts_hard_cap))

        last_err: Optional[StepError] = None

        for attempt in range(1, allowed_attempts + 1):
            try:
                handler = self._registry.get(step.action)
                res = await handler(step, ctx)

                if res is None:
                    raise RuntimeError("StepHandler returned None (expected StepExecutionResult).")

                if res.step_id != step.step_id:
                    raise RuntimeError(
                        f"StepHandler returned mismatched step_id: got={res.step_id.value} expected={step.step_id.value}"
                    )

                if res.action != step.action:
                    raise RuntimeError(
                        f"StepHandler returned mismatched action: got={res.action.value} expected={step.action.value}"
                    )

                # normalize attempts
                res.attempts = attempt
                return res

            except StepReplanRequested:
                # bubble up; executor will record it
                raise

            except Exception as e:
                last_err = StepError(
                    code=StepErrorCode.HANDLER_EXCEPTION,
                    message=str(e),
                    details={"action": step.action.value, "step_id": step.step_id.value},
                )

                if attempt < allowed_attempts and policy == FailurePolicyKind.RETRY:
                    if backoff_ms > 0:
                        await asyncio.sleep(backoff_ms / 1000.0)
                    continue

                # terminal based on policy
                if policy == FailurePolicyKind.SKIP:
                    return StepExecutionResult(
                        step_id=step.step_id,
                        action=step.action,
                        status=StepStatus.SKIPPED,
                        output=None,
                        error=last_err,
                        attempts=attempt,
                    )

                if policy == FailurePolicyKind.REPLAN:
                    raise StepReplanRequested(
                        code=ReplanCode.STEP_POLICY_REPLAN,
                        reason=ReplanReason.INTERNAL_ERROR,
                        details={"step_id": step.step_id.value, "action": step.action.value}
                    )

                # default FAIL
                return StepExecutionResult(
                    step_id=step.step_id,
                    action=step.action,
                    status=StepStatus.FAILED,
                    output=None,
                    error=last_err,
                    attempts=attempt,
                )

        # should never reach, but keep safe
        return StepExecutionResult(
            step_id=step.step_id,
            action=step.action,
            status=StepStatus.FAILED,
            output=None,
            error=last_err or StepError(code=StepErrorCode.UNKNOWN, message="Unknown failure"),
            attempts=allowed_attempts,
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/step_executor_models.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.step_executor_models
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=step_executor_models.py
# LINES: 160
# SHA256: c06fb5917829f1ad7a826114830b86ad9d8d98b0adad3aebaa8514e7f7366dd3
# SYMBOLS:
#   - class StepStatus
#   - class ReplanCode
#   - class ReplanReason
#   - class StepErrorCode
#   - class StepError
#   - class StepExecutionResult
#   - class PlanExecutionReport
#   - class StepReplanRequested
#   - class StepExecutionContext
#   - class StepExecutorConfig
#   - class StepHandlerRegistry
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import Any, Awaitable, Callable, Dict, Mapping, Optional, Protocol, Sequence

from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState
from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import (
    ExecutionStep,
    StepAction,
    StepId,
)


class StepStatus(str, Enum):
    OK = "ok"
    SKIPPED = "skipped"
    FAILED = "failed"
    REPLAN_REQUESTED = "replan_requested"
    

class ReplanCode(str, Enum):
    VERIFICATION_FAILED = "verification_failed"
    TOOL_SCHEMA_MISMATCH = "tool_schema_mismatch"
    MISSING_CAPABILITY = "missing_capability"
    PLANNER_INVARIANT_VIOLATION = "planner_invariant_violation"
    STEP_POLICY_REPLAN = "step_policy_replan"
    UNKNOWN = "unknown"


class ReplanReason(str, Enum):
    RETRY_EXHAUSTED = "retry_exhausted"
    INVALID_OUTPUT = "invalid_output"
    DEPENDENCY_FAILED = "dependency_failed"
    USER_CONSTRAINT_VIOLATION = "user_constraint_violation"
    INTERNAL_ERROR = "internal_error"
    UNSPECIFIED = "unspecified"


class StepErrorCode(str, Enum):
    HANDLER_EXCEPTION = "handler_exception"
    INVALID_HANDLER_RESULT = "invalid_handler_result"
    DEPENDENCY_MISSING = "dependency_missing"
    DEPENDENCY_FAILED = "dependency_failed"
    RETRY_EXHAUSTED = "retry_exhausted"
    REPLAN = "replan"
    INTERNAL = "internal"
    UNKNOWN = "unknown"



@dataclass(frozen=True)
class StepError:
    code: StepErrorCode
    message: str
    details: Optional[Dict[str, Any]] = None


@dataclass(frozen=False)
class StepExecutionResult:
    step_id: StepId
    action: StepAction
    status: StepStatus

    # --- new: production telemetry ---
    sequence: int
    started_at_utc: str
    ended_at_utc: str
    duration_ms: int

    # --- new: structured data for audit/debug ---
    validated_params: Optional[Dict[str, Any]] = None
    output: Optional[Any] = None
    meta: Optional[Dict[str, Any]] = None

    error: Optional[StepError] = None
    attempts: int = 1


@dataclass(frozen=True)
class PlanExecutionReport:
    plan_id: str
    ok: bool

    step_results: Dict[StepId, StepExecutionResult]
    final_output: Optional[Any] = None
    replan_reason: Optional[str] = None

    started_at_utc: str = ""
    ended_at_utc: str = ""
    duration_ms: int = 0

    step_order: Optional[list[StepId]] = None
    executed_order: Optional[list[StepId]] = None


class StepReplanRequested(RuntimeError):
    def __init__(
        self,
        *,
        code: ReplanCode,
        reason: ReplanReason,
        details: Optional[Dict[str, Any]] = None,
    ) -> None:
        super().__init__(f"{code.value}: {reason.value}")
        self.code = code
        self.reason = reason
        self.details = details



class StepExecutionContext(Protocol):
    @property
    def state(self) -> RuntimeState: ...

    @property
    def current_step(self) -> Optional[ExecutionStep]: ...

    @property
    def results(self) -> Mapping[StepId, StepExecutionResult]: ...

    @property
    def ordered_results(self) -> Sequence[StepExecutionResult]: ...

    def set_current_step(self, step: Optional[ExecutionStep]) -> None: ...

    def set_result(self, result: StepExecutionResult) -> None: ...


StepHandler = Callable[[ExecutionStep, StepExecutionContext], Awaitable[StepExecutionResult]]


@dataclass(frozen=True)
class StepExecutorConfig:
    """
    Executor-level behavior. Keep deterministic.
    """
    # Global hard cap safety (even if step says retry many times)
    max_attempts_hard_cap: int = 3

    # Whether to stop execution immediately on FAILED step (even if more steps exist)
    fail_fast: bool = True


@dataclass(frozen=True)
class StepHandlerRegistry:
    """
    Explicit mapping StepAction -> handler.
    This replaces getattr/reflection and makes wiring obvious and testable.
    """
    handlers: Dict[StepAction, StepHandler]

    def get(self, action: StepAction) -> StepHandler:
        h = self.handlers.get(action)
        if h is None:
            raise KeyError(f"No handler registered for action={action.value}")
        return h

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/step_planner.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.step_planner
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=step_planner.py
# LINES: 820
# SHA256: 920e42eea374cfec97bb2edc85a52ed7a6fdf6b93a6c346680bbe131d25fd0f2
# SYMBOLS:
#   - class StepPlannerConfig
#   - class StepPlanner
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional
import uuid

from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import EngineNextStep, EnginePlan
from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import (
    EngineHints,
    ExecutionPlan,
    ExecutionStep,
    ExpectedOutputType,
    FailurePolicy,
    FailurePolicyKind,
    OutputFormat,
    PlanBudgets,
    PlanBuildMode,
    PlanIntent,
    PlanMode,
    RationaleType,
    StepAction,
    StepBudgets,
    StepId,
    StopConditions,
    VerifyCriterion,
    VerifySeverity,
    WebSearchStrategy,
)


@dataclass(frozen=True)
class StepPlannerConfig:
    """
    Rule-based step planner configuration.
    Keep it deterministic; no LLM prompting here.
    """

    # Output style
    final_answer_style: str = "concise_technical"
    final_format: OutputFormat = OutputFormat.MARKDOWN

    # Default per-step budgets
    step_max_chars: int = 2000

    web_top_k: int = 5
    web_max_results: int = 5
    web_recency_days: int = 30
    web_strategy: WebSearchStrategy = WebSearchStrategy.HYBRID

    # Plan-level budgets
    max_total_steps: int = 6
    max_total_tool_calls: int = 3
    max_total_web_queries: int = 5
    max_total_chars_context: int = 12000
    max_total_tokens_output: Optional[int] = None


class StepPlanner:
    """
    Deterministic planner that builds an ExecutionPlan from:
      - user_message
      - engine_hints (e.g. from engine_planner.py)

    Important: ExecutionStep.params MUST be a dict (per stepplan_models.ExecutionStep).
    """

    def __init__(self, cfg: Optional[StepPlannerConfig] = None):
        self._cfg = cfg or StepPlannerConfig()

    # -----------------------------
    # Public API
    # -----------------------------

    def build_from_engine_plan(
        self,
        *,
        user_message: str,
        engine_plan: EnginePlan,
        plan_id: Optional[str] = None,
        build_mode: PlanBuildMode = PlanBuildMode.STATIC,
    ) -> ExecutionPlan:
        """
        Adapter entrypoint: EnginePlanner -> StepPlanner.

        STATIC:
        - build full sequence using EngineHints (web/ltm/rag/tools -> draft -> verify -> finalize)

        DYNAMIC:
        - build a single-step plan based on engine_plan.next_step (ready for planning loop)
        """
        if engine_plan is None:
            raise ValueError("engine_plan is required")

        msg = (user_message or "").strip()
        pid = (plan_id or self._new_plan_id()).strip() or self._new_plan_id()

        hints = self._hints_from_engine_plan(engine_plan)
        intent = hints.intent or PlanIntent.GENERIC

        # Preserve upstream clarifying question if provided
        if intent == PlanIntent.CLARIFY:
            q = (engine_plan.clarifying_question or "").strip()
            if not q:
                q = self._clarifying_question(msg)

            if build_mode == PlanBuildMode.STATIC:
                # STATIC must be an EXECUTE plan and end with FINAL
                return self._plan_clarify_execute_with_question(msg, plan_id=pid, question=q)

            # DYNAMIC must be ITERATE and can be single-step
            return self._plan_clarify_iterate_with_question(msg, plan_id=pid, question=q)

        if build_mode == PlanBuildMode.STATIC:
            return self.build_from_hints(
                user_message=msg,
                engine_hints=hints,
                plan_id=pid,
            )
        
        # DYNAMIC: one-step plan based on engine_plan.next_step
        ns = engine_plan.next_step
        if ns is None:
            # Fallback: if model didn't provide next_step, behave like STATIC
            return self.build_from_hints(
                user_message=msg,
                engine_hints=hints,
                plan_id=pid,
            )

        steps: List[ExecutionStep]

        if ns == EngineNextStep.WEBSEARCH:
            steps = [
                self._step_websearch(
                    step_id=StepId.WEBSEARCH,
                    depends_on=[],
                    query=self._web_query(msg, intent=intent),
                )
            ]

        elif ns == EngineNextStep.TOOLS:
            steps = [
                self._step_tools(
                    step_id=StepId.TOOLS,
                    depends_on=[],
                    tool_input={"query": msg, "intent": str(intent)},
                    max_tool_calls=1,
                )
            ]

        elif ns == EngineNextStep.RAG:
            steps = [
                self._step_rag_retrieval(
                    query=msg,
                    step_id=StepId.RAG,
                    depends_on=[],
                    top_k=6,
                )
            ]

        elif ns == EngineNextStep.SYNTHESIZE:
            # Use existing synth builder (do NOT call non-existent _step_synthesize_draft)
            steps = [
                self._step_synthesize(
                    step_id=StepId.DRAFT,
                    depends_on=[],
                    instructions=msg,
                )
            ]

        elif ns == EngineNextStep.FINALIZE:
            # Use existing finalize builder (do NOT call non-existent _step_finalize_answer)
            steps = [
                self._step_finalize(
                    depends_on=[],
                    instructions=msg,
                )
            ]

        else:
            # ns == CLARIFY was handled above via intent == CLARIFY
            # but keep a safe fallback:
            steps = [
                self._step_synthesize(
                    step_id=StepId.DRAFT,
                    depends_on=[],
                    instructions=msg,
                )
            ]

        # DYNAMIC plans do NOT have to end with FINALIZE_ANSWER.
        return self._wrap(
            intent=intent,
            mode=PlanMode.ITERATE,
            steps=steps,
            plan_id=pid,
            enforce_finalize=False,
        )



    def _hints_from_engine_plan(self, plan: EnginePlan) -> EngineHints:
        return EngineHints(
            enable_websearch=bool(plan.use_websearch),
            enable_ltm=bool(plan.use_user_longterm_memory),
            enable_rag=bool(plan.use_rag),
            enable_tools=bool(plan.use_tools),
            intent=plan.intent,
            intent_reason=(plan.reasoning_summary or None),
        )
    

    def _chain_pre_steps(self, steps: List[ExecutionStep]) -> List[ExecutionStep]:
        """
        Ensure deterministic sequential execution for pre-steps:
        steps[0].depends_on stays as-is (expected empty),
        steps[i].depends_on = [steps[i-1].step_id] for i>0.
        """
        if not steps:
            return steps

        # First step: enforce no deps (pre-steps start the chain)
        steps[0].depends_on = []

        for i in range(1, len(steps)):
            steps[i].depends_on = [steps[i - 1].step_id]

        return steps
    
    def _plan_clarify_execute_with_question(self, msg: str, *, plan_id: str, question: str) -> ExecutionPlan:
        steps: List[ExecutionStep] = [
            self._step_clarify(step_id=StepId.CLARIFY, depends_on=[], question=question),
            # FINAL ensures a consistent "output step" and keeps invariant "plan ends with FINAL"
            self._step_finalize(depends_on=[StepId.CLARIFY], instructions=question),
        ]
        return self._wrap(
            plan_id=plan_id,
            intent=PlanIntent.CLARIFY,
            mode=PlanMode.EXECUTE,
            steps=steps,
            enforce_finalize=True,
        )

    def _plan_clarify_iterate_with_question(self, msg: str, *, plan_id: str, question: str) -> ExecutionPlan:
        steps: List[ExecutionStep] = [
            self._step_clarify(step_id=StepId.CLARIFY, depends_on=[], question=question),
        ]
        return self._wrap(
            plan_id=plan_id,
            intent=PlanIntent.CLARIFY,
            mode=PlanMode.ITERATE,
            steps=steps,
            enforce_finalize=False,
        )



    def build_from_hints(
        self,
        *,
        user_message: str,
        engine_hints: Optional[EngineHints] = None,
        plan_id: Optional[str] = None,
    ) -> ExecutionPlan:
        msg = (user_message or "").strip()
        hints = engine_hints or EngineHints()
        pid = (plan_id or "stepplan-001").strip() or "stepplan-001"

        # If no message -> clarify (hard deterministic)
        if not msg:
            return self._plan_clarify(msg, plan_id=pid)

        # PRIMARY: upstream route decides.
        intent = hints.intent or PlanIntent.GENERIC

        if intent == PlanIntent.CLARIFY:
            return self._plan_clarify(msg, plan_id=pid)

        if intent == PlanIntent.FRESHNESS:
            # If upstream asked for freshness but websearch disabled -> degrade safely
            if hints.enable_websearch:
                return self._plan_freshness_with_hints(msg, plan_id=pid, hints=hints)
            return self._plan_generic_with_hints(msg, plan_id=pid, hints=hints)

        if intent == PlanIntent.PROJECT_ARCHITECTURE:
            if hints.enable_ltm:
                return self._plan_project_with_hints(msg, plan_id=pid, hints=hints)
            return self._plan_generic_with_hints(msg, plan_id=pid, hints=hints)


        # GENERIC default
        return self._plan_generic_with_hints(msg, plan_id=pid, hints=hints)


    def _build_execute_tail(
        self,
        *,
        msg: str,
        depends_on: List[StepId],
    ) -> List[ExecutionStep]:
        """
        Standard EXECUTE tail: DRAFT -> VERIFY -> FINAL.
        `depends_on` defines what DRAFT depends on (can be empty).
        """
        steps: List[ExecutionStep] = [
            self._step_synthesize(step_id=StepId.DRAFT, depends_on=depends_on, instructions=msg),
            self._step_verify(depends_on=[StepId.DRAFT], criteria=self._default_verify_criteria(msg), strict=True),
            self._step_finalize(depends_on=[StepId.VERIFY], instructions=msg),
        ]
        return steps


    # -----------------------------
    # Plan builders
    # -----------------------------

    def _plan_freshness_with_hints(self, msg: str, *, plan_id: str, hints: EngineHints) -> ExecutionPlan:
        pre_steps: List[ExecutionStep] = []

        # 1) WEBSEARCH must be first for freshness (if enabled)
        if hints.enable_websearch:
            pre_steps.append(
                self._step_websearch(
                    step_id=StepId.WEBSEARCH,
                    depends_on=[],
                    query=self._web_query(msg, intent=PlanIntent.FRESHNESS),
                )
            )

        # 2) Optional RAG (after websearch, before draft)
        if hints.enable_rag:
            pre_steps.append(
                self._step_rag_retrieval(
                    query=msg,
                    step_id=StepId.RAG,
                    depends_on=[],
                    top_k=6,
                )
            )

        # 3) Optional TOOLS
        if hints.enable_tools:
            pre_steps.append(
                self._step_tools(
                    step_id=StepId.TOOLS,
                    depends_on=[],
                    tool_input={"query": msg, "intent": str(PlanIntent.FRESHNESS)},
                    max_tool_calls=1,
                )
            )

        pre_steps = self._chain_pre_steps(pre_steps)
        draft_deps = [pre_steps[-1].step_id] if pre_steps else []
        steps = pre_steps + self._build_execute_tail(msg=msg, depends_on=draft_deps)

        return self._wrap(plan_id=plan_id, intent=PlanIntent.FRESHNESS, mode=PlanMode.EXECUTE, steps=steps)


    def _plan_generic_with_hints(self, msg: str, *, plan_id: str, hints: EngineHints) -> ExecutionPlan:
        pre_steps: List[ExecutionStep] = []

        # Deterministic, conservative ordering for pre-draft:
        # RAG -> TOOLS (web/ltm are handled by dedicated plans)
        if hints.enable_rag:
            pre_steps.append(
                self._step_rag_retrieval(query=msg, step_id=StepId.RAG, depends_on=[], top_k=6)
            )

        if hints.enable_tools:
            pre_steps.append(
                self._step_tools(
                    step_id=StepId.TOOLS,
                    depends_on=[],
                    tool_input={"query": msg, "intent": str(PlanIntent.GENERIC)},
                    max_tool_calls=1,
                )
            )

        pre_steps = self._chain_pre_steps(pre_steps)
        draft_deps = [pre_steps[-1].step_id] if pre_steps else []
        steps = pre_steps + self._build_execute_tail(msg=msg, depends_on=draft_deps)

        return self._wrap(plan_id=plan_id, intent=PlanIntent.GENERIC, mode=PlanMode.EXECUTE, steps=steps)


    def _plan_project_with_hints(self, msg: str, *, plan_id: str, hints: EngineHints) -> ExecutionPlan:
        pre_steps: List[ExecutionStep] = []

        # 1) LTM must be first for project architecture (if enabled)
        if hints.enable_ltm:
            pre_steps.append(
                self._step_ltm(
                    step_id=StepId.LTM_SEARCH,
                    depends_on=[],
                    query=self._ltm_query(msg, intent=PlanIntent.PROJECT_ARCHITECTURE),
                )
            )

        # 2) Optional RAG
        if hints.enable_rag:
            pre_steps.append(
                self._step_rag_retrieval(
                    query=msg,
                    step_id=StepId.RAG,
                    depends_on=[],
                    top_k=6,
                )
            )

        # 3) Optional TOOLS
        if hints.enable_tools:
            pre_steps.append(
                self._step_tools(
                    step_id=StepId.TOOLS,
                    depends_on=[],
                    tool_input={"query": msg, "intent": str(PlanIntent.PROJECT_ARCHITECTURE)},
                    max_tool_calls=1,
                )
            )

        pre_steps = self._chain_pre_steps(pre_steps)
        draft_deps = [pre_steps[-1].step_id] if pre_steps else []
        steps = pre_steps + self._build_execute_tail(msg=msg, depends_on=draft_deps)

        return self._wrap(plan_id=plan_id, intent=PlanIntent.PROJECT_ARCHITECTURE, mode=PlanMode.EXECUTE, steps=steps)


    def _plan_generic(self, msg: str, *, plan_id: str) -> ExecutionPlan:
        return self._plan_generic_with_hints(msg, plan_id=plan_id, hints=EngineHints())


    def _plan_freshness(self, msg: str, *, plan_id: str) -> ExecutionPlan:
        return self._plan_freshness_with_hints(msg, plan_id=plan_id, hints=EngineHints(enable_websearch=True, intent=PlanIntent.FRESHNESS))


    def _plan_project(self, msg: str, *, plan_id: str) -> ExecutionPlan:
        # Backward-compatible wrapper: "classic" project plan = LTM only.
        return self._plan_project_with_hints(
            msg,
            plan_id=plan_id,
            hints=EngineHints(enable_ltm=True, intent=PlanIntent.PROJECT_ARCHITECTURE),
        )


    def _plan_clarify(self, msg: str, *, plan_id: str) -> ExecutionPlan:
        q = self._clarifying_question(msg)
        return self._plan_clarify_execute_with_question(msg, plan_id=plan_id, question=q)


    

    def _plan_budgets(self) -> PlanBudgets:
        """
        Plan-level budgets. Deterministic defaults.
        Keep these small and stable; engine/runtime can override if needed.
        """
        return PlanBudgets(
            max_total_steps=self._cfg.max_total_steps,
            max_total_tool_calls=self._cfg.max_total_tool_calls,
            max_total_web_queries=self._cfg.max_total_web_queries,
            max_total_chars_context=self._cfg.max_total_chars_context,
            max_total_tokens_output=self._cfg.max_total_tokens_output,
        )


    def _stop_conditions(self, mode: PlanMode) -> StopConditions:
        if mode == PlanMode.ITERATE:
        # In iterative mode, we execute a single step and return to the planning loop.
            return StopConditions(
                max_iterations=1,
                stop_on_verifier_pass=False,
                stop_on_no_progress=True,
            )

        # EXECUTE (static full plan)
        return StopConditions(
            max_iterations=20,
            stop_on_verifier_pass=True,
            stop_on_no_progress=True,
        )


    def _web_query(self, msg: str, *, intent: PlanIntent) -> str:
        """
        Deterministic web query builder.
        IMPORTANT: routing decision (czy web w ogóle) nie jest tutaj.
        Tu tylko budujemy query, jeśli upstream już zdecydował, że websearch jest potrzebny.
        """
        q = (msg or "").strip()
        if not q:
            return "OpenAI Responses API changes"

        # Intent-specific normalization (no heuristics, just formatting)
        if intent == PlanIntent.FRESHNESS:
            # Keep it close to user text, but nudge toward changelog/release notes.
            return f"{q} changelog release notes dates"

        return q


    def _ltm_query(self, msg: str, *, intent: PlanIntent) -> str:
        """
        Deterministic LTM query builder.
        Again: no routing here; only build the query when LTM retrieval is already allowed.
        """
        q = (msg or "").strip()
        if not q:
            return "Intergrax architecture decisions"

        if intent == PlanIntent.PROJECT_ARCHITECTURE:
            # Keep stable prefix to improve retrieval consistency
            return f"Intergrax architecture: {q}"

        return q

    # -----------------------------
    # Step factories (IMPORTANT: params MUST be dict)
    # -----------------------------

    def _step_synthesize(self, *, step_id: StepId, depends_on: List[StepId], instructions: str) -> ExecutionStep:
        return ExecutionStep(
            step_id=step_id,
            action=StepAction.SYNTHESIZE_DRAFT,
            enabled=True,
            depends_on=depends_on,
            budgets=StepBudgets(top_k=0, max_chars=self._cfg.step_max_chars, max_tool_calls=0, max_web_queries=0),
            inputs={},
            params={
                "instructions": instructions,
                "must_include": [],
                "avoid": [],
            },
            expected_output_type=ExpectedOutputType.DRAFT,
            rationale_type=RationaleType.PRODUCE_DRAFT,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.RETRY,
                max_retries=1,
                retry_backoff_ms=0,
                replan_reason=None,
            ),
        )

    def _step_verify(
        self,
        *,
        depends_on: List[StepId],
        criteria: List[VerifyCriterion],
        strict: bool,
    ) -> ExecutionStep:
        if not criteria:
            criteria = [VerifyCriterion(id="non_empty", description="Answer is non-empty", severity=VerifySeverity.ERROR)]

        return ExecutionStep(
            step_id=StepId.VERIFY,
            action=StepAction.VERIFY_ANSWER,
            enabled=True,
            depends_on=depends_on or [],
            budgets=StepBudgets(top_k=0, max_chars=1000, max_tool_calls=0, max_web_queries=0),
            inputs={},
            params={
                # ExecutionStep expects dict; models will validate/normalize.
                "criteria": [c.model_dump() for c in criteria],
                "strict": bool(strict),
            },
            expected_output_type=ExpectedOutputType.VERIFIED,            
            rationale_type=RationaleType.VERIFY_QUALITY,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.REPLAN,
                max_retries=0,
                retry_backoff_ms=0,
                replan_reason="Verification failed",
            ),
        )

    def _step_finalize(self, *, depends_on: List[StepId], instructions: str) -> ExecutionStep:
        return ExecutionStep(
            step_id=StepId.FINAL,
            action=StepAction.FINALIZE_ANSWER,
            enabled=True,
            depends_on=depends_on,
            budgets=StepBudgets(top_k=0, max_chars=self._cfg.step_max_chars, max_tool_calls=0, max_web_queries=0),
            inputs={},
            params={
                "instructions": instructions,
                "format": self._cfg.final_format.value,  # OutputFormat -> string for params model
            },
            expected_output_type=ExpectedOutputType.FINAL,
            rationale_type=RationaleType.FINALIZE,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.FAIL,
                max_retries=0,
                retry_backoff_ms=0,
                replan_reason=None,
            ),
        )

    def _step_websearch(self, *, step_id: StepId, depends_on: List[StepId], query: str) -> ExecutionStep:
        return ExecutionStep(
            step_id=step_id,
            action=StepAction.USE_WEBSEARCH,
            enabled=True,
            depends_on=depends_on,
            budgets=StepBudgets(top_k=self._cfg.web_top_k, max_chars=5000, max_tool_calls=0, max_web_queries=1),
            inputs={},
            params={
                "query": query,
                "recency_days": int(self._cfg.web_recency_days),
                "max_results": int(self._cfg.web_max_results),
                "strategy": self._cfg.web_strategy.value,  # WebSearchStrategy -> string for params model
                "domains_allowlist": None,
            },
            expected_output_type=ExpectedOutputType.SEARCH_RESULTS,
            rationale_type=RationaleType.RETRIEVE_WEB,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.REPLAN,
                max_retries=0,
                retry_backoff_ms=0,
                replan_reason="Web search failed",
            ),
        )

    def _step_ltm(self, *, step_id: StepId, depends_on: List[StepId], query: str) -> ExecutionStep:
        return ExecutionStep(
            step_id=step_id,
            action=StepAction.USE_USER_LONGTERM_MEMORY_SEARCH,
            enabled=True,
            depends_on=depends_on,
            budgets=StepBudgets(top_k=5, max_chars=2000, max_tool_calls=0, max_web_queries=0),
            inputs={},
            params={
                "query": query,
                "top_k": 5,
                "score_threshold": None,
                "include_debug": False,
            },
            expected_output_type=ExpectedOutputType.LTM_RESULTS,
            rationale_type=RationaleType.RETRIEVE_LTM,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.RETRY,
                max_retries=1,
                retry_backoff_ms=0,
                replan_reason=None,
            ),
        )

    def _step_clarify(self, *, step_id: StepId, depends_on: List[StepId], question: str) -> ExecutionStep:
        # Clarify mode requires first step action ASK_CLARIFYING_QUESTION.
        return ExecutionStep(
            step_id=step_id,
            action=StepAction.ASK_CLARIFYING_QUESTION,
            enabled=True,
            depends_on=depends_on,
            budgets=StepBudgets(top_k=0, max_chars=300, max_tool_calls=0, max_web_queries=0),
            inputs={},
            params={
                "question": question,
                "choices": None,
                "must_answer_to_continue": True,
            },
            expected_output_type=ExpectedOutputType.CLARIFYING_QUESTION,
            rationale_type=RationaleType.ASK_CLARIFICATION,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.FAIL,
                max_retries=0,
                retry_backoff_ms=0,
                replan_reason=None,
            ),
        )
    
    def _step_rag_retrieval(
        self,
        *,
        query: str,
        step_id: StepId = StepId.RAG,
        depends_on: Optional[List[StepId]] = None,
        top_k: int = 6,
    ) -> ExecutionStep:
        """
        Retrieve context from RAG vectorstore (project / docs KB).
        Output: ExpectedOutputType.RAG_RESULTS
        """
        q = (query or "").strip()
        deps = depends_on or []

        k = int(top_k) if int(top_k) > 0 else 6

        return ExecutionStep(
            step_id=step_id,
            action=StepAction.USE_RAG_RETRIEVAL,
            enabled=True,
            depends_on=deps,
            budgets=StepBudgets(top_k=k, max_chars=5000, max_tool_calls=0, max_web_queries=0),
            inputs={},
            params={
                # Must match RagRetrievalParams exactly (extra=forbid): query + top_k only.
                "query": q,
                "top_k": k,
            },
            expected_output_type=ExpectedOutputType.RAG_RESULTS,
            rationale_type=RationaleType.RETRIEVE_RAG,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.REPLAN,
                max_retries=0,
                retry_backoff_ms=0,
                replan_reason="RAG retrieval failed",
            ),
        )

    def _step_tools(
        self,
        *,
        tool_input: Dict[str, Any],
        step_id: StepId = StepId.TOOLS,
        depends_on: Optional[List[StepId]] = None,
        max_tool_calls: int = 1,
    ) -> ExecutionStep:
        """
        Execute tool calling step via tools_agent.
        Output: ExpectedOutputType.TOOLS_RESULTS
        """
        deps = depends_on or []

        mtc = int(max_tool_calls) if int(max_tool_calls) > 0 else 1

        return ExecutionStep(
            step_id=step_id,
            action=StepAction.USE_TOOLS,
            enabled=True,
            depends_on=deps,
            budgets=StepBudgets(
                top_k=0,
                max_chars=5000,
                max_tool_calls=mtc,
                max_web_queries=0,
            ),
            inputs={},
            params={
                # Keep schema stable: executor/tools_agent will interpret this payload.
                "input": tool_input or {},
            },
            expected_output_type=ExpectedOutputType.TOOLS_RESULTS,
            rationale_type=RationaleType.RETRIEVE_TOOLS,
            on_failure=FailurePolicy(
                policy=FailurePolicyKind.REPLAN,
                max_retries=0,
                retry_backoff_ms=0,
                replan_reason="Tools execution failed",
            ),
        )


    # -----------------------------
    # Wrapping helpers
    # -----------------------------

    def _wrap(
        self,
        *,
        intent: PlanIntent,
        mode: PlanMode,
        steps: List[ExecutionStep],
        plan_id: Optional[str],
        enforce_finalize: bool = True,
    ) -> ExecutionPlan:
        pid = plan_id or self._new_plan_id()

        # Validate steps count early
        if len(steps) > self._cfg.max_total_steps:
            raise ValueError(
                f"StepPlanner bug: steps_count={len(steps)} exceeds max_total_steps={self._cfg.max_total_steps} "
                f"for intent={intent.value}"
            )

        # Execute plans must have at least one step.
        if mode == PlanMode.EXECUTE and not steps:
            raise ValueError("StepPlanner bug: execute plan has no steps.")

        # Execute plans MUST end with FINALIZE_ANSWER only when we enforce completeness.
        if mode == PlanMode.EXECUTE and enforce_finalize:
            if steps[-1].action != StepAction.FINALIZE_ANSWER:
                raise ValueError(
                    f"StepPlanner bug: execute plan must end with FINALIZE_ANSWER; "
                    f"got last_action={steps[-1].action.value} for intent={intent.value}"
                )

        return ExecutionPlan(
            plan_id=pid,
            intent=intent,
            mode=mode,
            steps=steps,
            budgets=self._plan_budgets(),
            stop_conditions=self._stop_conditions(mode),
            final_answer_style=self._cfg.final_answer_style,
            notes=None,
        )



    # -----------------------------
    # Classification rules (simple + deterministic)
    # -----------------------------

    def _clarifying_question(self, msg: str) -> str:
        return (
            "What exactly should the planner decide or output in your case "
            "(steps/actions/budgets), and what constraints must it follow?"
        )

    def _default_verify_criteria(self, msg: str) -> List[VerifyCriterion]:        
        return [
            VerifyCriterion(id="non_empty", description="Final answer is non-empty", severity=VerifySeverity.ERROR),
            VerifyCriterion(id="no_emojis", description="No emojis in technical output/code", severity=VerifySeverity.WARN),
        ]
    
    def _new_plan_id(self):
        return uuid.uuid4().hex
    

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/planning/stepplan_models.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=stepplan_models.py
# LINES: 411
# SHA256: ab165f6c4c46893e9ae1280bfca5d5ccadda89607b79b9be08001a452249c8b9
# SYMBOLS:
#   - class StepAction
#   - class FailurePolicyKind
#   - class WebSearchStrategy
#   - class OutputFormat
#   - class PlanMode
#   - class PlanBuildMode
#   - class StepId
#   - class ExpectedOutputType
#   - class RationaleType
#   - class VerifySeverity
#   - class FailurePolicy
#   - class StepBudgets
#   - class PlanBudgets
#   - class StopConditions
#   - class VerifyCriterion
#   - class AskClarifyingParams
#   - class LtmSearchParams
#   - class AttachmentsRetrievalParams
#   - class RagRetrievalParams
#   - class WebSearchParams
#   - class ToolsParams
#   - class SynthesizeDraftParams
#   - class VerifyAnswerParams
#   - class FinalizeAnswerParams
#   - class ExecutionStep
#   - class ExecutionPlan
#   - class EngineHints
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator

from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import PlanIntent


# -----------------------------
# Enums
# -----------------------------

class StepAction(str, Enum):
    ASK_CLARIFYING_QUESTION = "ASK_CLARIFYING_QUESTION"
    USE_USER_LONGTERM_MEMORY_SEARCH = "USE_USER_LONGTERM_MEMORY_SEARCH"
    USE_ATTACHMENTS_RETRIEVAL = "USE_ATTACHMENTS_RETRIEVAL"
    USE_RAG_RETRIEVAL = "USE_RAG_RETRIEVAL"
    USE_WEBSEARCH = "USE_WEBSEARCH"
    USE_TOOLS = "USE_TOOLS"
    SYNTHESIZE_DRAFT = "SYNTHESIZE_DRAFT"
    VERIFY_ANSWER = "VERIFY_ANSWER"
    FINALIZE_ANSWER = "FINALIZE_ANSWER"


class FailurePolicyKind(str, Enum):
    FAIL = "fail"
    SKIP = "skip"
    RETRY = "retry"
    REPLAN = "replan"


class WebSearchStrategy(str, Enum):
    SNIPPETS = "snippets"
    OPEN_PAGES = "open_pages"
    HYBRID = "hybrid"


class OutputFormat(str, Enum):
    PLAIN_TEXT = "plain_text"
    MARKDOWN = "markdown"


class PlanMode(str, Enum):
    CLARIFY = "clarify"
    EXECUTE = "execute"
    ITERATE = "iterate"


class PlanBuildMode(str, Enum):
    STATIC = "static"
    DYNAMIC = "dynamic"
    
    
class StepId(str, Enum):
    WEBSEARCH = "websearch"
    LTM_SEARCH = "ltm_search"
    RAG = "rag"
    DRAFT = "draft"
    VERIFY = "verify"
    FINAL = "final"
    CLARIFY = "clarify"
    TOOLS = "tools"


class ExpectedOutputType(str, Enum):
    DRAFT = "draft"
    VERIFIED = "verified"
    FINAL = "final"
    SEARCH_RESULTS = "search_results"
    LTM_RESULTS = "ltm_results"
    CLARIFYING_QUESTION = "clarifying_question"
    RAG_RESULTS = "rag_results" 
    TOOLS_RESULTS = "tools_results"


class RationaleType(str, Enum):
    PRODUCE_DRAFT = "produce_draft"
    VERIFY_QUALITY = "verify_quality"
    FINALIZE = "finalize"
    RETRIEVE_WEB = "retrieve_web"
    RETRIEVE_LTM = "retrieve_ltm"
    ASK_CLARIFICATION = "ask_clarification"
    RETRIEVE_RAG = "retrieve_rag"
    RETRIEVE_TOOLS = "retrieve_tools" 


class VerifySeverity(str, Enum):
    ERROR = "error"
    WARN = "warn"


# -----------------------------
# Shared small models
# -----------------------------

class FailurePolicy(BaseModel):
    model_config = ConfigDict(extra="forbid")

    policy: FailurePolicyKind
    max_retries: int = Field(ge=0)
    retry_backoff_ms: int = Field(ge=0)
    replan_reason: Optional[str] = None


class StepBudgets(BaseModel):
    """
    Step-level budgets.
    HARD: Only these keys are allowed.
    """
    model_config = ConfigDict(extra="forbid")

    top_k: int = Field(default=0, ge=0)
    max_chars: int = Field(default=0, ge=0)
    max_tool_calls: int = Field(default=0, ge=0)
    max_web_queries: int = Field(default=0, ge=0)


class PlanBudgets(BaseModel):
    """
    Plan-level budgets.
    """
    model_config = ConfigDict(extra="forbid")

    max_total_steps: int = Field(default=6, ge=1)
    max_total_tool_calls: int = Field(default=0, ge=0)
    max_total_web_queries: int = Field(default=0, ge=0)
    max_total_chars_context: int = Field(default=12000, ge=0)
    max_total_tokens_output: Optional[int] = Field(default=None, ge=0)


class StopConditions(BaseModel):
    model_config = ConfigDict(extra="forbid")

    max_iterations: int = 20
    stop_on_no_progress: bool = True

    stop_on_clarifying_question_answered: bool = True
    stop_on_verifier_pass: bool = True
    stop_on_budget_exhausted: bool = True


class VerifyCriterion(BaseModel):
    model_config = ConfigDict(extra="forbid")

    id: str = Field(min_length=1)
    description: str = Field(min_length=1)
    severity: VerifySeverity


# -----------------------------
# Params per action
# -----------------------------

class AskClarifyingParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    question: str = Field(min_length=1)
    # Optional UX helpers (allowed but not required)
    choices: Optional[List[str]] = None
    must_answer_to_continue: bool = True


class LtmSearchParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    query: str = Field(min_length=1)
    top_k: int = Field(default=3, ge=1)
    score_threshold: Optional[float] = None
    include_debug: bool = False


class AttachmentsRetrievalParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    query: str = Field(min_length=1)
    top_k: int = Field(default=5, ge=1)


class RagRetrievalParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    query: str = Field(min_length=1)
    top_k: int = Field(default=5, ge=1)


class WebSearchParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    query: str = Field(min_length=1)
    recency_days: Optional[int] = Field(default=None, ge=0)
    max_results: int = Field(default=5, ge=1)
    strategy: WebSearchStrategy = WebSearchStrategy.HYBRID
    domains_allowlist: Optional[List[str]] = None


class ToolsParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    input: Dict[str, Any] = Field(default_factory=dict)


class SynthesizeDraftParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    instructions: str = Field(min_length=1)
    must_include: List[str] = Field(default_factory=list)
    avoid: List[str] = Field(default_factory=list)


class VerifyAnswerParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    criteria: List[VerifyCriterion] = Field(min_length=1)
    strict: bool = True


class FinalizeAnswerParams(BaseModel):
    model_config = ConfigDict(extra="forbid")

    instructions: str = Field(min_length=1)
    format: OutputFormat = OutputFormat.MARKDOWN


# Mapping for deterministic validation
_ACTION_TO_PARAMS_MODEL = {
    StepAction.ASK_CLARIFYING_QUESTION: AskClarifyingParams,
    StepAction.USE_USER_LONGTERM_MEMORY_SEARCH: LtmSearchParams,
    StepAction.USE_ATTACHMENTS_RETRIEVAL: AttachmentsRetrievalParams,
    StepAction.USE_RAG_RETRIEVAL: RagRetrievalParams,
    StepAction.USE_WEBSEARCH: WebSearchParams,
    StepAction.USE_TOOLS: ToolsParams,
    StepAction.SYNTHESIZE_DRAFT: SynthesizeDraftParams,
    StepAction.VERIFY_ANSWER: VerifyAnswerParams,
    StepAction.FINALIZE_ANSWER: FinalizeAnswerParams,
}


_RETRIEVAL_OR_TOOLS_ACTIONS = {
    StepAction.USE_USER_LONGTERM_MEMORY_SEARCH,
    StepAction.USE_ATTACHMENTS_RETRIEVAL,
    StepAction.USE_RAG_RETRIEVAL,
    StepAction.USE_WEBSEARCH,
    StepAction.USE_TOOLS,
}


# -----------------------------
# Execution Step
# -----------------------------

class ExecutionStep(BaseModel):
    model_config = ConfigDict(extra="forbid")

    step_id: StepId
    action: StepAction
    enabled: bool = True
    depends_on: List[StepId]

    budgets: StepBudgets = Field(default_factory=StepBudgets)
    inputs: Dict[str, Any] = Field(default_factory=dict)

    # NOTE: LLM returns dict; we validate and coerce into a typed params model in validator.
    params: Dict[str, Any] = Field(default_factory=dict)

    expected_output_type: ExpectedOutputType
    rationale_type: RationaleType

    on_failure: FailurePolicy = Field(default_factory=lambda: FailurePolicy(
        policy=FailurePolicyKind.RETRY, max_retries=1, retry_backoff_ms=0, replan_reason=None
    ))

    @field_validator("depends_on", mode="before")
    @classmethod
    def _depends_on_to_list(cls, v: Any) -> List[StepId]:
        if v is None:
            return []
        if isinstance(v, list):
            out: List[StepId] = []
            for it in v:
                if it is None:
                    continue
                # allow StepId or str
                out.append(it if isinstance(it, StepId) else StepId(str(it)))
            return out
        # single item
        return [v if isinstance(v, StepId) else StepId(str(v))]

    @model_validator(mode="after")
    def _validate_params_match_action(self) -> ExecutionStep:
        """
        Validate 'params' dict shape by action and reject missing/extra keys.
        """
        model = _ACTION_TO_PARAMS_MODEL.get(self.action)
        if model is None:
            raise ValueError(f"Unsupported action: {self.action}")

        # Validate and normalize params via corresponding params model
        try:
            parsed = model.model_validate(self.params)
        except Exception as e:
            raise ValueError(f"Invalid params for action={self.action}: {e}") from e

        # Replace dict with normalized dict (still JSON-serializable)
        self.params = parsed.model_dump()

        return self


# -----------------------------
# Execution Plan
# -----------------------------

class ExecutionPlan(BaseModel):
    model_config = ConfigDict(extra="forbid")

    plan_id: str = Field(min_length=1)
    intent: PlanIntent
    mode: PlanMode
    steps: List[ExecutionStep] = Field(min_length=1)

    budgets: PlanBudgets
    stop_conditions: StopConditions
    final_answer_style: str = Field(default="concise_technical", min_length=1)

    notes: Optional[str] = None

    @model_validator(mode="after")
    def _validate_plan(self) -> "ExecutionPlan":
        # 1) Max steps budget
        if self.budgets and self.budgets.max_total_steps:
            if len(self.steps) > int(self.budgets.max_total_steps):
                raise ValueError("Number of steps exceeds max_total_steps budget.")

        # 2) Step ids must be unique
        ids = [s.step_id for s in self.steps]
        if len(ids) != len(set(ids)):
            raise ValueError("Duplicate step_id in steps.")

        # 3) depends_on must reference existing step_ids and not create forward references
        seen = set()
        for s in self.steps:
            for dep in s.depends_on:
                if dep not in ids:
                    raise ValueError(f"Step '{s.step_id}' depends_on unknown step_id '{dep}'.")
                # forward dep check: dep must already be seen in traversal order
                if dep not in seen:
                    raise ValueError(f"Step '{s.step_id}' has forward/invalid dependency '{dep}'.")
            seen.add(s.step_id)

        # 4) Mode constraints        
        if self.mode == PlanMode.EXECUTE:
            if self.steps[-1].action != StepAction.FINALIZE_ANSWER:
                raise ValueError("Execute mode must end with FINALIZE_ANSWER.")

        elif self.mode == PlanMode.ITERATE:
            # Iteration plans may contain 1+ steps and do NOT have to end with FINALIZE_ANSWER.
            # They are intended for dynamic planning loops.
            pass

        elif self.mode == PlanMode.CLARIFY:
            if self.steps[0].action != StepAction.ASK_CLARIFYING_QUESTION:
                raise ValueError("Clarify mode must start with ASK_CLARIFYING_QUESTION.")
            for s in self.steps:
                if s.action in _RETRIEVAL_OR_TOOLS_ACTIONS:
                    raise ValueError("Clarify mode cannot include retrieval/tools steps.")
        else:
            raise ValueError(f"Unsupported mode: {self.mode}")


        # 5) Websearch ordering: if present, it must occur before first SYNTHESIZE_DRAFT
        web_idx = None
        first_draft_idx = None
        for i, s in enumerate(self.steps):
            if web_idx is None and s.action == StepAction.USE_WEBSEARCH:
                web_idx = i
            if first_draft_idx is None and s.action == StepAction.SYNTHESIZE_DRAFT:
                first_draft_idx = i
        if web_idx is not None and first_draft_idx is not None:
            if web_idx > first_draft_idx:
                raise ValueError("USE_WEBSEARCH must appear before the first SYNTHESIZE_DRAFT.")

        return self


@dataclass(frozen=True)
class EngineHints:
    """
    Engine-level decision output: what capabilities are available/required for this run.

    Notes:
    - This is NOT a plan. It's only gating + intent hints.
    - StepPlanner must be deterministic given (user_message, hints).
    """
    enable_websearch: bool = False
    enable_ltm: bool = False
    enable_rag: bool = False
    enable_tools: bool = False

    # Routing decision from EnginePlanner
    intent: Optional[PlanIntent] = None

    # Optional debug
    intent_reason: Optional[str] = None



# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/prompts/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.prompts
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/prompts/history_prompt_builder.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.prompts.history_prompt_builder
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=history_prompt_builder.py
# LINES: 89
# SHA256: 7a99ef869b924ae8b98afb7409f51ccddb42fba28a79488896d4eefa17552286
# SYMBOLS:
#   - class HistorySummaryPromptBundle
#   - class HistorySummaryPromptBuilder
#   - class DefaultHistorySummaryPromptBuilder
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Protocol

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import (
    RuntimeRequest,
    HistoryCompressionStrategy,
)


@dataclass
class HistorySummaryPromptBundle:
    """
    Container for prompt elements related to history optimization
    (mainly summarization of older conversation turns).

    For now it only carries a single system_prompt string, but this
    structure allows us to extend it later (e.g. additional guardrails,
    style hints, etc.) without changing the interface.
    """

    system_prompt: str


class HistorySummaryPromptBuilder(Protocol):
    """
    Strategy interface for building the history-summary-related part
    of the prompt.

    You can provide a custom implementation and pass it to
    DropInKnowledgeRuntime to fully control:

    - the exact system prompt text used when summarizing older history,
    - how the request / strategy / message splits influence that prompt.
    """

    def build_history_summary_prompt(
        self,
        *,
        request: RuntimeRequest,
        strategy: HistoryCompressionStrategy,
        older_messages: List[ChatMessage],
        tail_messages: List[ChatMessage],
    ) -> HistorySummaryPromptBundle:
        ...


class DefaultHistorySummaryPromptBuilder(HistorySummaryPromptBuilder):
    """
    Default prompt builder for history summarization in Drop-In Knowledge Mode.

    Responsibilities:
    - Provide a safe, generic system prompt for summarizing older
      conversation turns into an information-dense summary.
    - Ignore request / strategy / messages for now (but the signature
      allows future, more advanced implementations).
    """

    def __init__(self, config: RuntimeConfig) -> None:
        self._config = config

    def build_history_summary_prompt(
        self,
        *,
        request: RuntimeRequest,
        strategy: HistoryCompressionStrategy,
        older_messages: List[ChatMessage],
        tail_messages: List[ChatMessage],
    ) -> HistorySummaryPromptBundle:
        # For now we return a static, default prompt. Later we can use
        # fields from `request` or `config` (e.g. domain, language,
        # user preferences) to customize the text.
        system_prompt = (
            "You are a summarization assistant.\n"
            "Summarize the following conversation history into a short, "
            "factual bullet list that preserves key decisions, key facts, "
            "and open questions.\n"
            "Do not invent new facts. Do not change the meaning.\n"
            "Keep the summary compact and information-dense."
        )

        return HistorySummaryPromptBundle(system_prompt=system_prompt)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/prompts/rag_prompt_builder.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.prompts.rag_prompt_builder
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=rag_prompt_builder.py
# LINES: 105
# SHA256: 487e17f834e16f6b28434c4e88072d4f318281f1d26952e1f4d9ac75a6bb422a
# SYMBOLS:
#   - class RagPromptBundle
#   - class RagPromptBuilder
#   - class DefaultRagPromptBuilder
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Protocol

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.runtime.drop_in_knowledge_mode.context.context_builder import (
    RetrievedChunk,
    BuiltContext,
)


@dataclass
class RagPromptBundle:
    """
    Prompt elements related to RAG:

    - context_messages: system-level messages injecting retrieved document context.
    """
    context_messages: List[ChatMessage]


class RagPromptBuilder(Protocol):
    """
    Strategy interface for building the RAG-related part of the prompt.

    You can provide a custom implementation and pass it to
    DropInKnowledgeRuntime to fully control:

    - the exact system prompt text,
    - how retrieved chunks are formatted and injected as messages.
    """

    def build_rag_prompt(self, built: BuiltContext) -> RagPromptBundle:
        ...


class DefaultRagPromptBuilder(RagPromptBuilder):
    """
    Default prompt builder for Drop-In Knowledge Mode.

    Responsibilities:
    - Inject retrieved chunks into system-level context messages.
    - Global system instructions are owned by the runtime.
    """

    def __init__(self, config: RuntimeConfig) -> None:
        self._config = config

    def build_rag_prompt(self, built: BuiltContext) -> RagPromptBundle:
        context_messages: List[ChatMessage] = []

        if built.retrieved_chunks:
            rag_context_text = self._format_rag_context(built.retrieved_chunks)
            context_messages.append(
                ChatMessage(
                    role="user",
                    content=(
                        "The following excerpts were retrieved from the user's "
                        "documents. Use them as factual context when answering "
                        "the user's question.\n\n"
                        f"{rag_context_text}"
                    ),
                )
            )

        return RagPromptBundle(
            context_messages=context_messages,
        )


    def _format_rag_context(self, chunks: List[RetrievedChunk]) -> str:
        """
        Build a compact, model-friendly text block from retrieved chunks.

        Design goals:
        - Provide enough semantic context.
        - Avoid internal markers ([CTX ...], scores, ids) that the model
          could copy into the final answer.
        - Keep format simple and natural.
        """
        if not chunks:
            return ""

        lines: List[str] = []

        for ch in chunks:
            source_name = (
                ch.metadata.get("source_name")
                or ch.metadata.get("attachment_id")
                or "document"
            )
            lines.append(f"Source: {source_name}")
            lines.append("Excerpt:")
            lines.append(ch.text)
            lines.append("")  # blank line separator

        # Optional: add truncation based on config (e.g. max chars)
        # For now we keep full text and rely on upstream chunking.
        return "\n".join(lines)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/prompts/user_longterm_memory_prompt_builder.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.prompts.user_longterm_memory_prompt_builder
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=user_longterm_memory_prompt_builder.py
# LINES: 117
# SHA256: 8608ee04efafc18df98ae70b097e4ddc92d0cc63a28c151fa8deac5e3568f0ad
# SYMBOLS:
#   - class UserLongTermMemoryPromptBundle
#   - class UserLongTermMemoryPromptBuilder
#   - class DefaultUserLongTermMemoryPromptBuilder
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Protocol, Optional

from intergrax.llm.messages import ChatMessage, MessageRole
from intergrax.memory.user_profile_memory import UserProfileMemoryEntry


@dataclass
class UserLongTermMemoryPromptBundle:
    """
    Prompt-ready bundle built from retrieved long-term memory entries.

    Design goals:
      - deterministic formatting (no LLM inference here),
      - compact but traceable,
      - safe: contains only retrieved entries, never the full profile.
    """
    context_messages: List[ChatMessage] = field(default_factory=list)


class UserLongTermMemoryPromptBuilder(Protocol):
    """
    Builds prompt messages to inject retrieved user long-term memory
    into the LLM context (similar role as RagPromptBuilder, but for LTM).

    NOTE: This is pure prompt construction only.
    Retrieval / embeddings / ranking live in UserProfileManager.
    """

    def build_user_longterm_memory_prompt(
        self,
        retrieved_entries: List[UserProfileMemoryEntry],
    ) -> UserLongTermMemoryPromptBundle:
        ...


class DefaultUserLongTermMemoryPromptBuilder(UserLongTermMemoryPromptBuilder):
    """
    Default deterministic LTM prompt builder.

    Output strategy:
      - single SYSTEM message containing compact bullet list,
      - includes entry_id and optional session_id for traceability,
      - avoids any inferred claims (just retrieved content).
    """

    def __init__(
        self,
        max_entries: int = 12,
        max_chars: int = 3000,
        title: str = "USER LONG-TERM MEMORY",
    ) -> None:
        self._max_entries = max_entries
        self._max_chars = max_chars
        self._title = title

    def build_user_longterm_memory_prompt(
        self,
        retrieved_entries: List[UserProfileMemoryEntry],
    ) -> UserLongTermMemoryPromptBundle:
        if not retrieved_entries:
            return UserLongTermMemoryPromptBundle(context_messages=[])

        # Filter deleted entries defensively (should already be handled upstream).
        entries = [e for e in retrieved_entries if not e.deleted]
        if not entries:
            return UserLongTermMemoryPromptBundle(context_messages=[])

        # Limit count.
        entries = entries[: self._max_entries]

        lines: List[str] = []
        lines.append(f"{self._title} (retrieved)")
        lines.append("Use these as factual user memory only if relevant to the question.")
        lines.append("If not relevant, ignore them.")
        lines.append("")

        # Build bullet list with traceable IDs.
        for e in entries:
            entry_id = (e.entry_id or "").strip()
            session_id = (e.session_id or "").strip() if e.session_id else ""
            kind = e.kind.value
            importance = e.importance.value

            meta_bits: List[str] = []
            if entry_id:
                meta_bits.append(f"id={entry_id}")
            if session_id:
                meta_bits.append(f"session={session_id}")
            if kind:
                meta_bits.append(f"kind={kind}")
            if importance:
                meta_bits.append(f"importance={importance}")

            meta = ", ".join(meta_bits)
            content = (e.content or "").strip()

            # Keep deterministic structure.
            if meta:
                lines.append(f"- [{meta}] {content}")
            else:
                lines.append(f"- {content}")

        text = "\n".join(lines).strip()

        # Hard char limit (deterministic truncation).
        if self._max_chars and len(text) > self._max_chars:
            text = text[: self._max_chars].rstrip() + "\n[...truncated]"

        msg = ChatMessage(role="user", content=text)
        return UserLongTermMemoryPromptBundle(context_messages=[msg])

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/prompts/websearch_prompt_builder.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.prompts.websearch_prompt_builder
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=websearch_prompt_builder.py
# LINES: 112
# SHA256: b51b50c3d26a1ea36fdc242caa532be5890502e64437369cd9fdb0ac1700b85f
# SYMBOLS:
#   - class WebSearchPromptBundle
#   - class WebSearchPromptBuilder
#   - class DefaultWebSearchPromptBuilder
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Protocol, Any, Optional

from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig
from intergrax.websearch.schemas.web_search_result import WebSearchResult
from intergrax.websearch.service.websearch_config import WebSearchConfig
from intergrax.websearch.service.websearch_context_generator import create_websearch_context_generator




@dataclass
class WebSearchPromptBundle:
    """
    Container for prompt elements related to web search:

    - context_messages: system-level messages injecting web search results.
    - debug_info: structured metadata for debug traces (URLs, counts, errors).
    """
    context_messages: List[ChatMessage]
    debug_info: Dict[str, Any]


class WebSearchPromptBuilder(Protocol):
    """
    Strategy interface for building the web search part of the prompt.

    You can provide a custom implementation and pass it to
    DropInKnowledgeRuntime to fully control:

    - how web documents are summarized,
    - how many results are injected,
    - the exact wording of the system messages.
    """

    async def build_websearch_prompt(
        self,
        web_results: List[WebSearchResult],
        *,
        user_query: Optional[str] = None,
        run_id: Optional[str] = None,
    ) -> WebSearchPromptBundle:
        ...


class DefaultWebSearchPromptBuilder(WebSearchPromptBuilder):
    """
    Default prompt builder for web search results in Drop-In Knowledge Mode.

    Responsibilities:
    - Take a list of typed WebSearchResult returned by WebSearchExecutor.
    - Delegate to websearch module context generator (strategy-based).
    - Wrap the generated grounded context into a single system message.
    - Provide debug info: number of docs, top URLs, strategy debug.
    """

    def __init__(self, config: RuntimeConfig) -> None:
        self._config = config

    async def build_websearch_prompt(
        self,
        web_results: List[WebSearchResult],
        *,
        user_query: Optional[str] = None,
        run_id: Optional[str] = None,
    ) -> WebSearchPromptBundle:
        debug_info: Dict[str, Any] = {}

        if not web_results:
            return WebSearchPromptBundle(
                context_messages=[],
                debug_info=debug_info,
            )

        debug_info["num_docs"] = len(web_results)
        debug_info["top_urls"] = [d.url for d in web_results[:3] if (d.url or "").strip()]

        # Use dedicated websearch configuration if present; otherwise fallback.
        cfg: Optional[WebSearchConfig] = self._config.websearch_config
        if cfg is None:
            # Fallback to previous behavior limits, but with safe defaults.
            # Prefer an explicit WebSearchConfig default rather than re-implementing logic here.
            cfg = WebSearchConfig()

        cfg.run_id = run_id

        gen = create_websearch_context_generator(cfg)
        result = await gen.generate(web_results, user_query=user_query)

        # IMPORTANT: context injection should be a system message
        context_messages = [
            ChatMessage(
                role="system",
                content=result.context_text,
            )
        ]

        # Merge generator debug info into builder debug info
        for k, v in result.debug_info.items():
            debug_info[k] = v

        return WebSearchPromptBundle(
            context_messages=context_messages,
            debug_info=debug_info,
        )

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/responses/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.responses
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/responses/response_schema.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.responses.response_schema
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=response_schema.py
# LINES: 182
# SHA256: 0edf2ebbb1b90e51e017a5d9b3de57ddb5cd7fb67f7594e34c1b181d3398ba3f
# SYMBOLS:
#   - class Citation
#   - class RouteInfo
#   - class ToolCallInfo
#   - class RuntimeStats
#   - class HistoryCompressionStrategy
#   - class RuntimeRequest
#   - class RuntimeAnswer
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

"""
Request and response data models for the Drop-In Knowledge Mode runtime.

These dataclasses define the high-level contract between applications
(FastAPI, Streamlit, CLI, MCP, etc.) and the DropInKnowledgeRuntime.

They intentionally hide low-level implementation details while keeping
enough structure to expose citations, routing information, tool calls,
and basic statistics.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional
from intergrax.llm.messages import AttachmentRef
from intergrax.llm_adapters.llm_usage_track import LLMUsageReport


@dataclass
class Citation:
    """
    Represents a single citation/reference used in the final answer.

    This can point to:
      - a document chunk in a vector store,
      - a specific file and location,
      - a web page,
      - an internal knowledge base entry.
    """

    source_id: str
    source_type: str  # e.g. "vectorstore", "file", "web", "db"
    source_label: Optional[str] = None  # human-readable label
    url: Optional[str] = None
    score: Optional[float] = None
    extra: Dict[str, Any] = field(default_factory=dict)


@dataclass
class RouteInfo:
    """
    Describes how the runtime decided to answer the question.

    Useful for debugging, observability, and UI explanations.
    """

    used_rag: bool = False
    used_websearch: bool = False
    used_tools: bool = False
    used_user_profile: bool = False
    used_user_longterm_memory: bool = False
    strategy: Optional[str] = None  # e.g. "simple", "agentic", "fallback_websearch"
    extra: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ToolCallInfo:
    """
    Describes a single tool call executed during the runtime request.
    """

    tool_name: str
    arguments: Dict[str, Any] = field(default_factory=dict)
    result_summary: Optional[str] = None
    success: bool = True
    error_message: Optional[str] = None
    extra: Dict[str, Any] = field(default_factory=dict)


@dataclass
class RuntimeStats:
    """
    Basic statistics about a runtime call.

    This is intentionally simple and can be extended over time.
    """

    total_tokens: Optional[int] = None
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    rag_tokens: Optional[int] = None
    websearch_tokens: Optional[int] = None
    tool_tokens: Optional[int] = None
    duration_ms: Optional[int] = None
    extra: Dict[str, Any] = field(default_factory=dict)


class HistoryCompressionStrategy(Enum):
    """
    Strategy for compressing the conversation history before sending it
    to the LLM.

    - OFF
        Do not modify or compress history at all.
        (Risk: context window overflow for very long conversations.)

    - TRUNCATE_OLDEST:
        Drop the oldest messages until the history fits into the budget.
    - SUMMARIZE_OLDEST:
        Summarize the oldest portion of the history into a compact
        synthetic message and keep more recent turns verbatim.
    - HYBRID:
        Combine truncation and summarization, e.g. truncate very old noise
        and summarize the remaining older block.
    """

    OFF = "off"
    TRUNCATE_OLDEST = "truncate_oldest"
    SUMMARIZE_OLDEST = "summarize_oldest"
    HYBRID = "hybrid"
    

@dataclass
class RuntimeRequest:
    """
    High-level request structure for the Drop-In Knowledge runtime.

    This object is built by the application layer and passed into the
    DropInKnowledgeRuntime. It can be created directly or via helper
    functions/wrappers in web frameworks.
    """

    user_id: str
    session_id: str
    message: str

    attachments: List[AttachmentRef] = field(default_factory=list)

    # Optional tenant/workspace scoping
    tenant_id: Optional[str] = None
    workspace_id: Optional[str] = None

    # Optional UI / app metadata (channel, app name, etc.)
    metadata: Dict[str, Any] = field(default_factory=dict)

    # User-provided instructions (ChatGPT/Gemini-style)
    instructions: Optional[str] = None


    # Strategy used to keep the conversation history within the model
    # context window for THIS request.
    #
    # If you don't specify anything when constructing the request,
    # TRUNCATE_OLDEST will be used as a reasonable default.
    history_compression_strategy: HistoryCompressionStrategy = HistoryCompressionStrategy.TRUNCATE_OLDEST

    # Maximum number of output tokens for a single model response
    # for THIS request.
    #
    # If None, the runtime/adapter will use its own internal default.
    #
    # NOTE:
    # This is *not* the context window size. The maximum context window
    # is defined by the underlying LLM adapter (context_window_tokens).
    max_output_tokens: Optional[int] = None


@dataclass
class RuntimeAnswer:
    """
    High-level response structure returned by the Drop-In Knowledge runtime.

    This contains the final answer, along with citations, routing info,
    tool call summaries, and basic statistics.
    """

    answer: str
    citations: List[Citation] = field(default_factory=list)
    route: RouteInfo = field(default_factory=RouteInfo)
    tool_calls: List[ToolCallInfo] = field(default_factory=list)
    stats: RuntimeStats = field(default_factory=RuntimeStats)
    llm_usage_report: Optional[LLMUsageReport] = None

    # Optional raw model output or intermediate artifacts
    raw_model_output: Optional[Any] = None
    debug_trace: Dict[str, Any] = field(default_factory=dict)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/session/__init__.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.session
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=__init__.py
# LINES: 0
# SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
# SYMBOLS:
#   - <none>
# ======================================================================


# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/session/chat_session.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.session.chat_session
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=chat_session.py
# LINES: 145
# SHA256: 3b849ae2b512c86002b303a4ac748ad8b9624d2a04a2eb1108972deb2354779d
# SYMBOLS:
#   - class SessionStatus
#   - class SessionCloseReason
#   - class ChatSession
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
from intergrax.llm.messages import AttachmentRef
from enum import Enum


class SessionStatus(str, Enum):
    """
    Domain-level status of a chat session.

    Kept as `str` + `Enum` so that the value is JSON-serializable and
    safe to store in DB or metadata.

    Typical lifecycle:
      - OPEN   → session is active and can receive messages
      - CLOSED → session was finalized and should not be appended to
    """

    OPEN = "open"
    CLOSED = "closed"


class SessionCloseReason(str, Enum):
    """
    Optional domain-level reason for closing a chat session.

    This is intentionally minimal for now; additional categories can
    be added when the domain evolves (e.g. timeout, user_logout, etc.)
    """

    # The session was explicitly closed (default)
    EXPLICIT = "explicit"

    # Could be triggered by inactivity / timeout (future use)
    TIMEOUT = "timeout"

    # Session closed because the tenant/user context changed (future use)
    CONTEXT_SWITCH = "context_switch"

    # Generic fallback reason
    UNKNOWN = "unknown"


@dataclass
class ChatSession:
    """
    Domain model describing a single chat session.

    Important:
      - This object does NOT store messages. The single source of truth for
        conversation history is maintained by session-level storage
        (e.g. ConversationalMemory, database, Redis, etc.).
      - This model is intentionally I/O-free. It should not talk directly
        to any storage backend. All persistence is handled by a manager
        or storage component above it.
    """

    # Stable session identifier used throughout the runtime.
    id: str

    # Optional identifiers for user, tenant and workspace.
    user_id: Optional[str] = None
    tenant_id: Optional[str] = None     # can be used as organization/tenant identifier
    workspace_id: Optional[str] = None  # workspace/project/context within a tenant

    # Timestamps for auditing and retention policies.
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

    # Optional per-session attachments (not tied directly to a single message).
    attachments: List[AttachmentRef] = field(default_factory=list)

    # Core domain state (typed)
    status: SessionStatus = SessionStatus.OPEN  # "open" / "closed" – można kiedyś zamienić na Enum
    closed_reason: Optional[SessionCloseReason] = None

    user_turns: int = 0

    # Consolidation-related state
    last_consolidated_at: Optional[datetime] = None
    last_consolidated_reason: Optional[str] = None  # wartości z SessionConsolidationReason.value
    last_consolidated_turn: Optional[int] = None
    last_consolidation_debug: Optional[Dict[str, Any]] = None

    # Per-session instructions snapshot + refresh flag
    user_profile_instructions: Optional[str] = None
    org_profile_instructions: Optional[str] = None
    needs_user_instructions_refresh: bool = False

    # Arbitrary metadata (could contain tags, profile instruction cache, counters, etc.).
    metadata: Dict[str, Any] = field(default_factory=dict)

    # ------------------------------------------------------------------
    # Domain helpers (no I/O)
    # ------------------------------------------------------------------

    def touch(self) -> None:
        """
        Refresh modification timestamp.

        Managers / storage components should call this before/after
        mutating the session, but this method itself does not persist
        anything.
        """
        self.updated_at = datetime.now(timezone.utc)

    @property
    def is_closed(self) -> bool:
        """
        Return True if this session is marked as closed at the domain level.
        """
        return self.status == SessionStatus.CLOSED

    def mark_closed(self, reason: SessionCloseReason = None) -> None:
        """
        Mark this session as closed at the domain level.

        This method does not persist changes. The caller is responsible
        for saving the session via the session manager / storage.
        """
        self.status = SessionStatus.CLOSED        
        self.closed_reason = reason or SessionCloseReason.EXPLICIT

        self.touch()

    def increment_user_turns(self) -> int:
        """
        Increment and return the per-session counter of user turns.

        This is useful for heuristics like "synthesize memory every N user
        messages". The counter is stored in user_turns.

        This method updates in-memory state only. Persistence is the
        responsibility of the manager / storage layer.
        """        
        self.user_turns += 1
        self.touch()
        return self.user_turns

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/session/in_memory_session_storage.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=in_memory_session_storage.py
# LINES: 181
# SHA256: 5d9d175f44122ddc9b86a9e41403c6c0070b94fe09c1692a8c6ad69bd5fbd769
# SYMBOLS:
#   - class InMemorySessionStorage
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

import uuid
from dataclasses import replace
from typing import Dict, List, Optional

from intergrax.llm.messages import ChatMessage
from intergrax.memory.conversational_memory import ConversationalMemory
from intergrax.runtime.drop_in_knowledge_mode.session.chat_session import ChatSession
from intergrax.runtime.drop_in_knowledge_mode.session.session_storage import SessionStorage


class InMemorySessionStorage(SessionStorage):
    """
    In-memory implementation of SessionStorage.

    Responsibilities:
      - Keep ChatSession metadata in an in-process dictionary.
      - Maintain per-session conversation history using ConversationalMemory.
      - Apply a simple FIFO trimming policy via ConversationalMemory's
        max_messages setting.

    This implementation is suitable for:
      - development,
      - tests,
      - single-process / single-node setups.

    It is NOT intended for production use in distributed or long-lived
    environments. For production you should implement a persistent
    SessionStorage based on a database, Redis, or another durable backend.
    """

    def __init__(
        self,
        *,
        max_history_messages: Optional[int] = None,
    ) -> None:
        # Metadata storage (chat sessions registry).
        self._sessions: Dict[str, ChatSession] = {}

        # Internal conversational memory storage (one per session).
        self._conv_memory: Dict[str, ConversationalMemory] = {}

        # Maximum number of messages to keep before trimming FIFO-style.
        self._max_history_messages = max_history_messages

    # ------------------------------------------------------------------
    # Session metadata CRUD
    # ------------------------------------------------------------------

    async def get_session(self, session_id: str) -> Optional[ChatSession]:
        """
        Return the session metadata if it exists, else None.

        A shallow copy of the session is returned so that callers cannot
        accidentally mutate the internal storage without calling save_session().
        """
        session = self._sessions.get(session_id)
        if session is None:
            return None
        return replace(session)

    async def create_session(
        self,
        session_id: Optional[str] = None,
        *,
        user_id: Optional[str] = None,
        tenant_id: Optional[str] = None,
        workspace_id: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> ChatSession:
        """
        Create and persist a new ChatSession.

        If session_id is None, a new UUID4-based id is generated.
        """
        if session_id is None:
            session_id = str(uuid.uuid4())

        if metadata is None:
            metadata = {}

        session = ChatSession(
            id=session_id,
            user_id=user_id,
            tenant_id=tenant_id,
            workspace_id=workspace_id,
            metadata=dict(metadata),
        )

        self._sessions[session_id] = session
        return replace(session)

    async def save_session(self, session: ChatSession) -> None:
        """
        Persist changes to an existing ChatSession.

        For the in-memory implementation this simply updates
        the internal dictionary.
        """
        self._sessions[session.id] = session

    async def list_sessions_for_user(
        self,
        user_id: str,
        *,
        limit: Optional[int] = None,
    ) -> List[ChatSession]:
        """
        List recent sessions for a given user, ordered by recency.
        """
        sessions = [s for s in self._sessions.values() if s.user_id == user_id]
        sessions.sort(key=lambda s: s.updated_at, reverse=True)

        if limit is not None and limit > 0:
            sessions = sessions[:limit]

        # Return copies so callers cannot mutate internal state directly.
        return [replace(s) for s in sessions]

    # ------------------------------------------------------------------
    # Conversation history operations
    # ------------------------------------------------------------------

    async def append_message(
        self,
        session_id: str,
        message: ChatMessage,
    ) -> ChatMessage:
        """
        Append a single message to the conversation history of a session.

        If the session does not exist, a KeyError is raised.
        If no ConversationalMemory exists yet for the session, it is created.
        """
        session = self._sessions.get(session_id)
        if session is None:
            raise KeyError(f"Session '{session_id}' does not exist")

        memory = self._conv_memory.get(session_id)

        # Safety fallback: create a new conversational memory if missing.
        if memory is None:
            memory = ConversationalMemory(
                session_id=session_id,
                max_messages=self._max_history_messages,
            )
            self._conv_memory[session_id] = memory

        # Delegate trimming / retention policy to ConversationalMemory.
        memory.add_message(message)

        # Update session recency.
        session.touch()
        self._sessions[session_id] = session

        return message

    async def get_history(
        self,
        session_id: str,
        *,
        native_tools: bool = False,
    ) -> List[ChatMessage]:
        """
        Return the ordered conversation history for a given session id.

        Trimming logic (max history size, FIFO) is handled internally by
        the underlying ConversationalMemory instance.
        """
        memory = self._conv_memory.get(session_id)
        if memory is None:
            return []

        # Still uses ConversationalMemory under the hood, but the storage
        # interface does not talk about LLMs or prompts.
        return memory.get_for_model(native_tools=native_tools)

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/session/session_manager.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.session.session_manager
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=session_manager.py
# LINES: 651
# SHA256: 1f580d02a008d582afe828ea0be42ab5821c6608e8f3f1fb7e4103bad734510d
# SYMBOLS:
#   - class SessionConsolidationReason
#   - class SessionManager
# ======================================================================
from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional, Sequence
from datetime import datetime, timezone

from intergrax.globals.settings import GLOBAL_SETTINGS
from intergrax.llm.messages import ChatMessage
from intergrax.memory.user_profile_manager import UserProfileManager
from intergrax.runtime.drop_in_knowledge_mode.session.chat_session import (
    ChatSession,
    SessionCloseReason,
)
from intergrax.runtime.drop_in_knowledge_mode.session.session_storage import (
    SessionStorage,
)
from intergrax.runtime.organization.organization_profile_manager import (
    OrganizationProfileManager,
)
from intergrax.runtime.user_profile.session_memory_consolidation_service import (
    SessionMemoryConsolidationService,
)


class SessionConsolidationReason(str, Enum):
    """
    Enumeration of session consolidation triggers.
    Keeping this as `str` + `Enum` ensures that the value
    is safe to serialize into metadata and logs.
    """

    MID_SESSION = "mid_session"
    CLOSE_SESSION = "close_session"


class SessionManager:
    """
    High-level manager for chat sessions.

    Responsibilities:
      - Orchestrate session lifecycle on top of a SessionStorage backend.
      - Provide a stable API for the runtime engine (DropInKnowledgeRuntime).
      - Integrate with user/organization profile managers to expose
        prompt-ready system instructions per session.
      - Optionally trigger long-term user memory consolidation for a session.

    This class should be the *only* component that the runtime engine
    talks to when it comes to sessions and their metadata/history.
    """

    def __init__(
        self,
        storage: SessionStorage,
        *,
        user_profile_manager: Optional[UserProfileManager] = None,
        organization_profile_manager: Optional[OrganizationProfileManager] = None,
        session_memory_consolidation_service: Optional[
            SessionMemoryConsolidationService
        ] = None,
        user_turns_consolidation_interval: Optional[
            int
        ] = GLOBAL_SETTINGS.default_user_turns_consolidation_interval,
        consolidation_cooldown_seconds: Optional[
            int
        ] = GLOBAL_SETTINGS.default_consolidation_cooldown_seconds,
    ) -> None:
        """
        Initialize a new SessionManager instance.

        Args:
            storage:
                Low-level session + history storage backend (in-memory, DB, etc.).
            user_profile_manager:
                Optional manager used to resolve user-level system instructions
                and to write long-term user profile memory.
            organization_profile_manager:
                Optional manager used to resolve organization-level
                system instructions (per tenant / org).
            session_memory_consolidation_service:
                Optional service responsible for consolidating a single session
                into long-term user profile memory entries and refreshing
                user-level system instructions.
            user_turns_consolidation_interval:
                Interval (in user turns) for mid-session consolidation.
                If None or non-positive, mid-session consolidation is disabled.
            consolidation_cooldown_seconds:
                Cooldown (in seconds) between mid-session consolidations for a
                single session. If None or non-positive, no cooldown is applied.
        """
        # Low-level storage backend (in-memory, DB, Redis, etc.).
        self._storage = storage

        # High-level managers for profile-based instructions (optional).
        self._user_profile_manager = user_profile_manager
        self._organization_profile_manager = organization_profile_manager

        # Optional service that can consolidate a single session into
        # long-term user profile memory entries and refresh user-level
        # system instructions.
        self._session_memory_consolidation_service = (
            session_memory_consolidation_service
        )

        # Resolve the effective interval for mid-session consolidation.
        # The value is interpreted as:
        #   - > 0  → consolidate every N-th user message,
        #   - <= 0 → mid-session consolidation disabled.
        if (
            user_turns_consolidation_interval is not None
            and user_turns_consolidation_interval > 0
        ):
            effective_interval = user_turns_consolidation_interval
        else:
            effective_interval = 0

        self._user_turns_consolidation_interval: int = effective_interval

        # Effective cooldown in seconds between mid-session consolidations.
        # The value is interpreted as:
        #   - > 0  → enforce cooldown,
        #   - <= 0 → no cooldown (only the interval is applied).
        if (
            consolidation_cooldown_seconds is not None
            and consolidation_cooldown_seconds > 0
        ):
            effective_cooldown = consolidation_cooldown_seconds
        else:
            effective_cooldown = 0

        self._consolidation_cooldown_seconds: int = effective_cooldown


    async def get_history(self, session_id: str) -> List[ChatMessage]:
        return await self._storage.get_history(session_id=session_id)

    # ------------------------------------------------------------------
    # Session lifecycle (metadata)
    # ------------------------------------------------------------------

    async def get_session(self, session_id: str) -> Optional[ChatSession]:
        """
        Return ChatSession metadata if it exists, else None.
        """
        return await self._storage.get_session(session_id)

    async def create_session(
        self,
        session_id: Optional[str] = None,
        *,
        user_id: Optional[str] = None,
        tenant_id: Optional[str] = None,
        workspace_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> ChatSession:
        """
        Create and persist a new ChatSession via the underlying storage.

        Notes:
          - If session_id is None, the storage may generate a new identifier.
          - This method only encapsulates construction + basic defaults;
            all persistence is delegated to SessionStorage.
        """
        return await self._storage.create_session(
            session_id=session_id,
            user_id=user_id,
            tenant_id=tenant_id,
            workspace_id=workspace_id,
            metadata=metadata,
        )
    
    async def get_or_create_session(
        self,
        *,
        user_id: str,
        session_id: str,
    ) -> ChatSession:
        session = await self.get_session(            
            session_id=session_id,
        )

        if session is not None:
            return session

        session = await self.create_session(
            user_id=user_id,
            session_id=session_id
        )

        await self._storage.save_session(session)
        return session


    async def save_session(self, session: ChatSession) -> None:
        """
        Persist changes to an existing ChatSession.

        The manager refreshes the modification timestamp and delegates
        the actual persistence to the storage backend.
        """
        session.touch()
        await self._storage.save_session(session)

    async def close_session(
        self,
        session_id: str,
        *,
        reason: Optional[SessionCloseReason] = None,
        run_id: Optional[str] = None,
    ) -> None:
        """
        Mark a session as closed at the domain level and, if configured,
        trigger long-term memory consolidation for this session.

        Behavior:
          - Mark the ChatSession as closed and persist it.
          - If a SessionMemoryConsolidationService is available and the
            session has an associated user_id:
              * load the conversation history for this session,
              * call consolidate_session(user_id, session_id, messages)
                to extract long-term memory entries and update the
                user's system_instructions (side-effect).

        Args:
            session_id:
                Identifier of the session to close.
            reason:
                Optional domain-level reason. If None, a default
                SessionCloseReason.EXPLICIT is used.
        """
        session = await self._storage.get_session(session_id)
        if session is None:
            return

        # Decide which close reason to apply. If caller did not provide one,
        # we use EXPLICIT as the default semantic.
        effective_reason = reason or SessionCloseReason.EXPLICIT

        # 1) Domain-level close (no deletion of messages).
        #    ChatSession is responsible for updating its own status and
        #    closed_reason according to the enum value.
        session.mark_closed(reason=effective_reason)
        await self._storage.save_session(session)

        # 2) Optional: consolidate this session into long-term user memory.
        #    We only do this if:
        #      - the service is configured, and
        #      - the session is associated with a user_id.
        if (
            self._session_memory_consolidation_service is not None
            and session.user_id
        ):
            # Fetch full conversation history for this session. This allows the
            # consolidation service to decide how much to trim and which parts
            # to keep, based on its own config (max messages, char budget, etc.).
            messages = await self.get_history_for_session(session_id)

            # If there's no history, there is nothing to consolidate.
            if messages:
                stored_entries = (
                    await self._session_memory_consolidation_service.consolidate_session(
                        user_id=session.user_id,
                        session_id=session_id,
                        messages=messages,
                        run_id=run_id,
                    )
                )

                debug_info = self._build_consolidation_debug_info(stored_entries)

                # Mark that this session has been consolidated as part of close_session.
                await self._mark_session_consolidated(
                    session,
                    reason=SessionConsolidationReason.CLOSE_SESSION,
                    # Store the final user_turns value to indicate at which
                    # point the last consolidation happened.
                    turn=session.user_turns,
                    debug_info=debug_info,
                )

    async def list_sessions_for_user(
        self,
        user_id: str,
        *,
        limit: Optional[int] = None,
    ) -> List[ChatSession]:
        """
        List recent sessions for a given user, ordered by recency.
        """
        return await self._storage.list_sessions_for_user(user_id, limit=limit)

    # ------------------------------------------------------------------
    # Conversation history
    # ------------------------------------------------------------------

    async def append_message(
        self,
        session_id: str,
        message: ChatMessage,
    ) -> ChatMessage:
        """
        Append a single message to the conversation history of a session.

        Domain rules:
          - For user messages, increment the per-session "user_turns" counter
            stored in ChatSession.user_turns.
          - Optionally, every N-th user message, trigger mid-session
            long-term memory consolidation for this session.
          - Trimming / retention policies for history are implemented by the
            underlying storage (e.g. ConversationalMemory).

        Note:
          - This method intentionally keeps domain logic (user_turns and
            consolidation hooks) at the manager level, while the storage
            remains responsible only for persisting sessions and their history.
        """
        # Try to load the session so we can apply domain-level updates
        # (user_turns counter, timestamps, etc.).
        session = await self._storage.get_session(session_id)

        # Increment user_turns only for user messages and only if the
        # session exists. If the session is missing, we delegate error
        # handling to the storage.append_message call below.
        if session is not None and message.role == "user":
            # This updates in-memory state and timestamps; persistence is
            # delegated to save_session().
            user_turns = session.increment_user_turns()
            await self.save_session(session)

            # Decide whether to trigger mid-session consolidation.
            if (
                self._session_memory_consolidation_service is not None
                and session.user_id
            ):
                interval = self._user_turns_consolidation_interval
                # Only trigger if:
                #   - the interval is positive,
                #   - we reached an exact multiple (e.g. 8, 16, 24...),
                #   - and the cooldown since the last consolidation has passed.
                if (
                    interval > 0
                    and (user_turns % interval) == 0
                    and self._is_mid_session_consolidation_allowed(session)
                ):
                    # Fetch the current conversation history for this session.
                    # The consolidation service is responsible for trimming
                    # or summarizing as needed based on its own config.
                    messages = await self.get_history_for_session(session_id)

                    if messages:
                        stored_entries = (
                            await self._session_memory_consolidation_service.consolidate_session(
                                user_id=session.user_id,
                                session_id=session_id,
                                messages=messages,
                            )
                        )

                        # Build a small debug payload based on the stored entries.
                        debug_info = self._build_consolidation_debug_info(
                            stored_entries
                        )

                        # Record consolidation metadata for debugging and future heuristics.
                        await self._mark_session_consolidated(
                            session,
                            reason=SessionConsolidationReason.MID_SESSION,
                            turn=user_turns,
                            debug_info=debug_info,
                        )

        # Delegate message persistence to the storage backend. The storage
        # may apply its own retention/trimming logic (FIFO, max_messages, etc.).
        return await self._storage.append_message(session_id, message)

    async def get_history_for_session(
        self,
        session_id: str,
        *,
        native_tools: bool = False,
    ) -> List[ChatMessage]:
        """
        Convenience helper: return conversation history by session id.

        This is the primary method to use from higher-level components
        (e.g. memory consolidation services) when they need the full
        conversation for a given session.
        """
        return await self._storage.get_history(
            session_id=session_id,
            native_tools=native_tools,
        )

    # ------------------------------------------------------------------
    # User profile memory – prompt-level instructions (per session)
    # ------------------------------------------------------------------

    async def get_user_profile_instructions_for_session(
        self,
        session: ChatSession,
    ) -> Optional[str]:
        """
        Return a prompt-ready user profile instruction string for this session.

        Behavior:
          - If a cached value is present in session.user_profile_instructions
            and the session is not marked as requiring a refresh, it is
            returned (after stripping whitespace).
          - Otherwise this method delegates to UserProfileManager, calling
            `get_system_instructions_for_user(user_id)` which returns the
            effective user-level system instructions (already including any
            internal fallbacks), caches the resulting string on the session,
            and saves the updated session.

        Semantics:
          - Instructions are effectively *snapshotted per session*.
            If the underlying user profile changes (e.g. after consolidation),
            the session can be marked as requiring a refresh and will then
            re-resolve instructions on the next call.
        """
        # No associated user or no profile manager → no instructions.
        if not session.user_id:
            return None
        if self._user_profile_manager is None:
            return None

        needs_refresh = session.needs_user_instructions_refresh

        # 1) Try cached instructions from the session.
        cached = session.user_profile_instructions
        if not needs_refresh and isinstance(cached, str):
            stripped = cached.strip()
            if stripped:
                return stripped

        # 2) Fallback: resolve from the user profile manager.
        # The manager encapsulates all logic of:
        #   - using profile.system_instructions if set,
        #   - or falling back to a deterministic summary if not.
        instructions = await self._user_profile_manager.get_system_instructions_for_user(
            session.user_id
        )
        if not isinstance(instructions, str):
            return None

        stripped = instructions.strip()
        if not stripped:
            return None

        # 3) Cache on the session and persist.
        session.user_profile_instructions = stripped
        session.needs_user_instructions_refresh = False

        await self.save_session(session)

        return stripped


    async def search_user_longterm_memory(
        self,
        user_id: str,
        query: str,
        *,
        top_k: Optional[int] = None,
        score_threshold: Optional[float] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        Delegate long-term memory retrieval to UserProfileManager (if available).
        Engine should not know how the profile manager implements retrieval.
        """
        if self._user_profile_manager is None:
            return None

        return await self._user_profile_manager.search_longterm_memory(
            user_id=user_id,
            query=query,
            top_k=top_k,
            score_threshold=score_threshold,
        )


    # ------------------------------------------------------------------
    # Organization profile memory – prompt-level instructions (per session)
    # ------------------------------------------------------------------

    async def get_org_profile_instructions_for_session(
        self,
        session: ChatSession,
    ) -> Optional[str]:
        """
        Return a prompt-ready organization profile instruction string
        for this session.

        Behavior:
          - If a cached value is present in session.org_profile_instructions,
            it is returned (after stripping whitespace).
          - Otherwise this method delegates to OrganizationProfileManager, calling
            `get_system_instructions_for_organization(organization_id, ...)`,
            caches the resulting string on the session, and saves the updated
            session.

        Note:
          - This method no longer uses prompt bundles; it works purely on
            the final system-instructions string exposed by the manager.
          - The organization identifier is derived from session.tenant_id.
        """
        # No associated tenant or no organization profile manager → no instructions.
        if not session.tenant_id:
            return None
        if self._organization_profile_manager is None:
            return None

        # 1) Try cached instructions from the session.
        cached = session.org_profile_instructions
        if isinstance(cached, str):
            stripped = cached.strip()
            if stripped:
                return stripped

        # 2) Fallback: resolve from the organization profile manager.
        instructions = (
            await self._organization_profile_manager.get_system_instructions_for_organization(
                organization_id=session.tenant_id
            )
        )
        if not isinstance(instructions, str):
            return None

        stripped = instructions.strip()
        if not stripped:
            return None

        # 3) Cache on the session and persist.
        session.org_profile_instructions = stripped
        await self.save_session(session)

        return stripped

    # ------------------------------------------------------------------
    # Consolidation helpers
    # ------------------------------------------------------------------

    @staticmethod
    def _build_consolidation_debug_info(
        entries: Sequence[Any],
    ) -> Dict[str, Any]:
        """
        Build a lightweight debug payload describing the outcome of a
        consolidation run.

        The structure is intentionally simple and JSON-serializable so it can
        be safely stored in session.last_consolidation_debug.
        """
        total = len(entries)

        # Try to infer entry types in a defensive way. We deliberately avoid
        # using getattr(...) here. Instead:
        #   - if the object exposes an 'entry_type' attribute, we use it,
        #   - otherwise we fallback to the literal "unknown".
        type_counts: Dict[str, int] = {}
        for e in entries:
            if hasattr(e, "entry_type"):
                # We ignore type-checker complaints here because not all
                # objects in the list are guaranteed to have this attribute.
                entry_type = e.entry_type  # type: ignore[attr-defined]
            else:
                entry_type = "unknown"

            key = str(entry_type)
            type_counts[key] = type_counts.get(key, 0) + 1

        return {
            "entries_count": total,
            "entry_types": type_counts,
        }

    async def _mark_session_consolidated(
        self,
        session: ChatSession,
        *,
        reason: SessionConsolidationReason,
        turn: Optional[int] = None,
        debug_info: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Mark the given session as having been consolidated into long-term
        user memory.

        Side effects:
          - Updates typed consolidation fields on the ChatSession:
              last_consolidated_at
              last_consolidated_reason
              last_consolidated_turn
              last_consolidation_debug
              needs_user_instructions_refresh
          - Persists the updated session via save_session().

        The debug payload is intentionally small and JSON-serializable so it
        can be logged or inspected by tooling without additional parsing.
        """
        # When using typed fields we keep the timestamp as a proper datetime
        # object in UTC. If string serialization is needed (e.g. for DB),
        # the storage backend is responsible for that conversion.
        now_utc = datetime.now(timezone.utc)

        session.last_consolidated_at = now_utc
        session.last_consolidated_reason = reason.value

        # Mark that the underlying user profile may have changed
        # (new memory entries, regenerated system_instructions).
        # Existing sessions should refresh their cached instructions
        # on the next call to get_user_profile_instructions_for_session().
        session.needs_user_instructions_refresh = True

        if turn is not None:
            session.last_consolidated_turn = int(turn)

        if debug_info is not None:
            session.last_consolidation_debug = debug_info

        # Persist the updated consolidation metadata (and refresh modification
        # timestamp via save_session()).
        await self.save_session(session)

    def _is_mid_session_consolidation_allowed(self, session: ChatSession) -> bool:
        """
        Check whether we are allowed to run a mid-session consolidation
        for the given session based on a simple cooldown.

        Logic:
          - If cooldown <= 0 → always allowed.
          - If there is no last_consolidated_at on the session → allowed.
          - Otherwise, only allowed if at least `cooldown` seconds have
            passed since the last consolidation.
        """
        cooldown = self._consolidation_cooldown_seconds

        if cooldown <= 0:
            return True

        last_dt = session.last_consolidated_at
        if last_dt is None:
            return True

        # Ensure we are working with an aware UTC datetime.
        if last_dt.tzinfo is None:
            last_dt = last_dt.replace(tzinfo=timezone.utc)

        now = datetime.now(timezone.utc)
        elapsed_seconds = (now - last_dt).total_seconds()

        return elapsed_seconds >= cooldown

# ======================================================================
# FILE: intergrax/runtime/drop_in_knowledge_mode/session/session_storage.py
# MODULE: intergrax.runtime.drop_in_knowledge_mode.session.session_storage
# MODULE_GROUP: runtime
# TAGS:
#   - scope=folder=intergrax/runtime/drop_in_knowledge_mode/
#   - module_group=runtime
#   - file=session_storage.py
# LINES: 129
# SHA256: 135a750dd88598fc774666a50b5adc868df52bfe8e5f70271f58329aef89f4b4
# SYMBOLS:
#   - class SessionStorage
# ======================================================================
# © Artur Czarnecki. All rights reserved.
# Intergrax framework – proprietary and confidential.
# Use, modification, or distribution without written permission is prohibited.

from __future__ import annotations

from typing import Protocol, Optional, List
from intergrax.llm.messages import ChatMessage
from intergrax.runtime.drop_in_knowledge_mode.session.chat_session import ChatSession


class SessionStorage(Protocol):
    """
    Low-level storage interface for chat sessions and their conversation
    history.

    Responsibilities:
      - Persist and load ChatSession objects.
      - Persist and load conversation history (ChatMessage sequences)
        for a given session.

    This interface is intentionally minimal and does not contain any
    higher-level domain logic (no profile instructions, no memory
    synthesis, no counters). Those responsibilities belong to the
    SessionManager layer built on top of this storage.
    """

    # ------------------------------------------------------------------
    # Session metadata CRUD
    # ------------------------------------------------------------------

    async def get_session(self, session_id: str) -> Optional[ChatSession]:
        """
        Load a session by its identifier.

        Returns:
            ChatSession if found, otherwise None.
        """
        ...

    async def create_session(
        self,
        session_id: Optional[str] = None,
        *,
        user_id: Optional[str] = None,
        tenant_id: Optional[str] = None,
        workspace_id: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> ChatSession:
        """
        Create and persist a new ChatSession.

        If session_id is None, the storage is allowed to generate its own
        identifier (e.g. UUID4).
        """
        ...
        

    async def save_session(self, session: ChatSession) -> None:
        """
        Persist changes to an existing ChatSession.

        This should be called by the SessionManager whenever session
        metadata (including timestamps) is updated.
        """
        ...

    async def list_sessions_for_user(
        self,
        user_id: str,
        *,
        limit: Optional[int] = None,
    ) -> List[ChatSession]:
        """
        List recent sessions for a given user, ordered by recency
        (e.g. updated_at descending).

        Parameters:
            user_id:
                Identifier of the user.
            limit:
                Maximum number of sessions to return.

        Returns:
            A list of ChatSession objects (may be empty).
        """
        ...

    # ------------------------------------------------------------------
    # Conversation history operations
    # ------------------------------------------------------------------

    async def append_message(
        self,
        session_id: str,
        message: ChatMessage,
    ) -> ChatMessage:
        """
        Append a single message to the conversation history of a session.

        Implementations may apply trimming or other retention policies,
        but the general contract is:
          - message is persisted,
          - the stored message (possibly enriched with additional data)
            is returned.
        """
        ...

    async def get_history(
        self,
        session_id: str,
        *,
        native_tools: bool = False,
    ) -> List[ChatMessage]:
        """
        Return the ordered conversation history for a given session id.

        Parameters:
            session_id:
                Identifier of the session.
            native_tools:
                If True, return messages with native tool-calls preserved
                (depending on the underlying history implementation).

        Returns:
            A list of ChatMessage objects in chronological order
            (may be empty if no history exists for the session).
        """
        ...

