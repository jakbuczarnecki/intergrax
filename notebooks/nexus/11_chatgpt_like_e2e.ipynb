{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70da1f1",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dada617",
   "metadata": {},
   "source": [
    "# Notebook 11 — ChatGPT-like E2E (behavior-level)\n",
    "\n",
    "This notebook is an **integration / behavior** test that exercises the nexus Runtime end-to-end, in a way that resembles a real ChatGPT usage pattern.\n",
    "\n",
    "## What we test (behavior)\n",
    "- Multi-session behavior (A/B/C) with **isolated session history** and **shared user LTM**\n",
    "- User LTM persistence + recall across sessions\n",
    "- Session-level consolidation (history → summary) without cross-session leakage\n",
    "- RAG ingestion + Q&A over a document\n",
    "- Websearch as a context layer that affects the final answer\n",
    "- Tools execution (tool + LLM) without breaking the **user-last invariant**\n",
    "- Reasoning enabled for observability, but **not persisted into user-visible history**\n",
    "\n",
    "## What we do NOT test (intentionally out of scope for this notebook)\n",
    "- Retry / fallback logic for empty LLM outputs\n",
    "- Formal “adapter contract” beyond `generate_messages(...) -> str`\n",
    "- Tools contract v1 (final answer vs context-only)\n",
    "- Prompt tuning, quality scoring, reranking\n",
    "- Debug refactors (e.g., removing getattr) unless strictly required to run the notebook\n",
    "\n",
    "## Hard invariants\n",
    "- **User-last invariant:** the last message sent to the core LLM must always be `role=\"user\"`\n",
    "- No empty assistant answers in the final output\n",
    "- No memory leakage between sessions (history isolation)\n",
    "\n",
    "## Requirements\n",
    "This notebook expects your local environment to provide:\n",
    "- Intergrax sources on PYTHONPATH\n",
    "- LLM credentials (e.g., `OPENAI_API_KEY`) if using OpenAI adapters\n",
    "- Vectorstore backend deps (Chroma/Qdrant) if enabling RAG/LTM vector retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27ca231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8005ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_CSE_API_KEY\"] = os.getenv(\"GOOGLE_CSE_API_KEY\") or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intergrax.runtime.nexus.responses.response_schema import RuntimeAnswer\n",
    "from intergrax.runtime.nexus.tracing.trace_query import TraceQuery\n",
    "from intergrax.runtime.nexus.tracing.adapters.llm_usage_snapshot import LLMUsageSnapshotDiag\n",
    "from intergrax.runtime.nexus.tracing.adapters.llm_usage_finalize import LLMUsageFinalizeDiag\n",
    "\n",
    "\n",
    "def get_llm_usage_snapshot(answer: RuntimeAnswer) -> LLMUsageSnapshotDiag:\n",
    "    q = TraceQuery.from_iter(answer.trace_events)\n",
    "    snap = q.first_payload(LLMUsageSnapshotDiag)\n",
    "    assert snap is not None, \"Missing LLMUsageSnapshotDiag in trace_events.\"\n",
    "    return snap\n",
    "\n",
    "\n",
    "def get_llm_usage_finalize(answer: RuntimeAnswer) -> LLMUsageFinalizeDiag:\n",
    "    q = TraceQuery.from_iter(answer.trace_events)\n",
    "    fin = q.first_payload(LLMUsageFinalizeDiag)\n",
    "    assert fin is not None, \"Missing LLMUsageFinalizeDiag in trace_events.\"\n",
    "    return fin\n",
    "\n",
    "\n",
    "def assert_llm_usage_basic(answer: RuntimeAnswer) -> LLMUsageSnapshotDiag:\n",
    "    snap = get_llm_usage_snapshot(answer)\n",
    "\n",
    "    assert snap.calls >= 1, f\"Expected calls >= 1, got {snap.to_dict()}\"\n",
    "    assert snap.total_tokens > 0, f\"Expected total_tokens > 0, got {snap.to_dict()}\"\n",
    "    assert snap.total_tokens == snap.input_tokens + snap.output_tokens, (\n",
    "        f\"Expected total_tokens == input_tokens + output_tokens, got {snap.to_dict()}\"\n",
    "    )\n",
    "    assert snap.errors >= 0, f\"Expected errors >= 0, got {snap.to_dict()}\"\n",
    "\n",
    "    return snap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c26e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS\\AppData\\Local\\Temp\\ipykernel_24076\\2916059931.py:30: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  RUN_ID = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=rag_docs_20251226_091323_5de869a1\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=user_ltm_20251226_091323_5de869a1\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "BOOTSTRAP OK\n",
      "RUN_ID: 20251226_091323_5de869a1\n",
      "ARTIFACTS_DIR: D:\\Projekty\\intergrax\\notebooks\\nexus\\_artifacts\\notebook_11_chatgpt_like\\20251226_091323_5de869a1\n",
      "USER_ID: user_chatgpt_like_001\n",
      "SESSION_A: sess_chatgpt_like_A\n",
      "SESSION_B: sess_chatgpt_like_B\n",
      "SESSION_C: sess_chatgpt_like_C\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.runtime.nexus.config import RuntimeConfig\n",
    "from intergrax.runtime.nexus.engine.runtime import RuntimeEngine\n",
    "from intergrax.runtime.nexus.engine.runtime_context import RuntimeContext\n",
    "from intergrax.runtime.nexus.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.nexus.session.session_manager import SessionManager\n",
    "from intergrax.memory.user_profile_manager import UserProfileManager\n",
    "from intergrax.memory.stores.in_memory_user_profile_store import InMemoryUserProfileStore\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "from intergrax.runtime.user_profile.session_memory_consolidation_service import SessionMemoryConsolidationService\n",
    "from intergrax.runtime.user_profile.user_profile_instructions_service import UserProfileInstructionsService\n",
    "from intergrax.utils.time_provider import SystemTimeProvider\n",
    "from intergrax.websearch.providers.google_cse_provider import GoogleCSEProvider\n",
    "from intergrax.websearch.service.websearch_executor import WebSearchExecutor\n",
    "\n",
    "# =====================================================================\n",
    "# Global test identifiers / paths (no tests executed in this cell)\n",
    "# =====================================================================\n",
    "\n",
    "USER_ID = \"user_chatgpt_like_001\"\n",
    "\n",
    "SESSION_A = \"sess_chatgpt_like_A\"\n",
    "SESSION_B = \"sess_chatgpt_like_B\"\n",
    "SESSION_C = \"sess_chatgpt_like_C\"\n",
    "\n",
    "RUN_ID = SystemTimeProvider.utc_now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).resolve()\n",
    "ARTIFACTS_DIR = BASE_DIR / \"_artifacts\" / \"notebook_11_chatgpt_like\" / RUN_ID\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Separate vectorstore collections (1 instance = 1 collection_name)\n",
    "# - RAG_DOCS: document ingestion/retrieval\n",
    "# - USER_LTM: user long-term memory retrieval\n",
    "RAG_DIR = ARTIFACTS_DIR / \"vs_rag_docs\"\n",
    "LTM_DIR = ARTIFACTS_DIR / \"vs_user_ltm\"\n",
    "RAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LTM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LLM adapter (real adapter, no wrappers)\n",
    "# - assumes env is configured (OPENAI_API_KEY etc.)\n",
    "# ---------------------------------------------------------------------\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Embeddings + vectorstore (real managers)\n",
    "# Pick providers you actually use in your repo/env.\n",
    "# ---------------------------------------------------------------------\n",
    "embed_manager = EmbeddingManager(\n",
    "    provider=\"ollama\",\n",
    ")\n",
    "\n",
    "rag_vs = VectorstoreManager(\n",
    "    config=VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=f\"rag_docs_{RUN_ID}\",\n",
    "        chroma_persist_directory=str(RAG_DIR),\n",
    "    )\n",
    ")\n",
    "\n",
    "# User LTM vectorstore\n",
    "ltm_vs = VectorstoreManager(\n",
    "    VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=f\"user_ltm_{RUN_ID}\",\n",
    "        chroma_persist_directory=str(LTM_DIR),\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Stores\n",
    "# ---------------------------------------------------------------------\n",
    "session_store = InMemorySessionStorage()\n",
    "user_profile_store = InMemoryUserProfileStore()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Managers\n",
    "# ---------------------------------------------------------------------\n",
    "user_profile_manager = UserProfileManager(\n",
    "    store=user_profile_store,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=ltm_vs,\n",
    ")\n",
    "\n",
    "user_profile_instructions_service = UserProfileInstructionsService(\n",
    "    llm=llm_adapter,\n",
    "    manager=user_profile_manager,\n",
    ")\n",
    "\n",
    "session_memory_consolidation_service = SessionMemoryConsolidationService(\n",
    "    llm=llm_adapter,\n",
    "    profile_manager=user_profile_manager,\n",
    "    instructions_service=user_profile_instructions_service,\n",
    ")\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=session_store,\n",
    "    user_profile_manager=user_profile_manager,\n",
    "    session_memory_consolidation_service=session_memory_consolidation_service\n",
    ")\n",
    "\n",
    "websearch_executor = WebSearchExecutor(\n",
    "    providers=[\n",
    "        GoogleCSEProvider(),\n",
    "    ],\n",
    "    max_text_chars=None,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Base Runtime config (DO NOT mutate this object in build_runtime)\n",
    "# ---------------------------------------------------------------------\n",
    "base_config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=rag_vs,\n",
    "    websearch_executor=websearch_executor,\n",
    "    enable_user_profile_memory=True,\n",
    "    enable_org_profile_memory=False,\n",
    "    enable_user_longterm_memory=True,\n",
    "    enable_rag=True,\n",
    "    enable_websearch=True,\n",
    "    tools_mode=\"off\",\n",
    ")\n",
    "\n",
    "def _clone_runtime_config(cfg: RuntimeConfig) -> RuntimeConfig:\n",
    "    \"\"\"\n",
    "    Create a shallow clone of RuntimeConfig.\n",
    "    Supports either dataclass-like configs or plain attribute containers.\n",
    "    \"\"\"\n",
    "    if hasattr(cfg, \"copy\") and callable(getattr(cfg, \"copy\")):\n",
    "        try:\n",
    "            return cfg.copy()\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "    data = dict(cfg.__dict__)\n",
    "    return RuntimeConfig(**data)\n",
    "\n",
    "# =====================================================================\n",
    "# Runtime factory\n",
    "# =====================================================================\n",
    "def build_runtime(*, override_config: dict | None = None, **runtime_kwargs) -> RuntimeEngine:\n",
    "    \"\"\"\n",
    "    Build a RuntimeEngine instance.\n",
    "    - override_config: dict of RuntimeConfig fields to override (shallow).\n",
    "    - runtime_kwargs: runtime init kwargs (e.g., ingestion_service, context_builder, prompt builders).\n",
    "    \"\"\"\n",
    "    cfg = _clone_runtime_config(base_config)\n",
    "\n",
    "    if override_config:\n",
    "        for k, v in override_config.items():\n",
    "            setattr(cfg, k, v)\n",
    "\n",
    "    context = RuntimeContext(\n",
    "        config=cfg,\n",
    "        session_manager=session_manager,\n",
    "        ingestion_service=runtime_kwargs.get(\"ingestion_service\"),\n",
    "        context_builder=runtime_kwargs.get(\"context_builder\"),\n",
    "        rag_prompt_builder=runtime_kwargs.get(\"rag_prompt_builder\"),\n",
    "        websearch_prompt_builder=runtime_kwargs.get(\"websearch_prompt_builder\"),\n",
    "        history_prompt_builder=runtime_kwargs.get(\"history_prompt_builder\"),\n",
    "    )\n",
    "\n",
    "    return RuntimeEngine(context=context)\n",
    "\n",
    "print(\"BOOTSTRAP OK\")\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"ARTIFACTS_DIR:\", str(ARTIFACTS_DIR))\n",
    "print(\"USER_ID:\", USER_ID)\n",
    "print(\"SESSION_A:\", SESSION_A)\n",
    "print(\"SESSION_B:\", SESSION_B)\n",
    "print(\"SESSION_C:\", SESSION_C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be54a0",
   "metadata": {},
   "source": [
    "## Cell 2 — Session A: onboarding + LTM write\n",
    "\n",
    "Goal:\n",
    "- Simulate a real onboarding turn (user introduces themselves and preferences).\n",
    "- Run the full runtime pipeline via `runtime.run(...)`.\n",
    "- Close the session to trigger **session → LTM consolidation**.\n",
    "- Minimal asserts:\n",
    "  1) Assistant answer is non-empty\n",
    "  2) Session history is persisted\n",
    "  3) User LTM contains at least one entry after closing the session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c572699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 1\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 2\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 3\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 4\n",
      "SESSION A OK\n",
      "Answer length: 335\n",
      "History messages: 2\n",
      "LTM hits_count: None\n",
      "LTM debug: {'enabled': True, 'used': True, 'reason': 'hits', 'where': {'user_id': 'user_chatgpt_like_001', 'deleted': 0}, 'top_k': 5, 'threshold': 0.0, 'raw_ids': ['8ae1799e8a50424ebaa502498b1b51b8', '6662c8be36c84eb3bb60790ba15e4924', '5f18da4f7b8e4548bdaffe0ca684855a', '3a9d6f3525cc4819aa8cdfab88560317'], 'raw_scores': [0.23801147937774658, -0.1927858591079712, -0.2161877155303955, -0.26020002365112305], 'raw_metadatas': [{'source': 'session_consolidation', 'tags': 'user,project', 'deleted': 0, 'user_id': 'user_chatgpt_like_001', 'kind': 'user_fact', 'entry_id': '8ae1799e8a50424ebaa502498b1b51b8'}, {'kind': 'preference', 'entry_id': '6662c8be36c84eb3bb60790ba15e4924', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'tags': 'workflow,style', 'deleted': 0}, {'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'deleted': 0, 'tags': 'session_summary', 'entry_id': '5f18da4f7b8e4548bdaffe0ca684855a', 'kind': 'session_summary'}, {'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'entry_id': '3a9d6f3525cc4819aa8cdfab88560317', 'kind': 'preference', 'deleted': 0, 'tags': 'communication,tone'}], 'raw_documents_preview': ['Artur buduje Integrax i Mooff.', 'Nie używać emotikonów w kodzie ani w dokumentacji technicznej.', 'Artur zapoznał się z asystentem i wyjaśnił swoje preferencje dotyczące odpowiedzi. Zapytał o możliwość dyskusji lub rozwiązania konkretnego problemu.', 'Odpowiedź powinna być krótka, techniczna.'], 'returned_count': 1, 'hits_count': 1}\n",
      "LTM hits: 1\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.nexus.responses.response_schema import RuntimeRequest\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build runtime (no special overrides yet)\n",
    "# ---------------------------------------------------------------------\n",
    "runtime = build_runtime()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Session A — onboarding message (behaves like real ChatGPT)\n",
    "# ---------------------------------------------------------------------\n",
    "onboarding_message = (\n",
    "    \"Hi. I am Artur. I build Integrax and Mooff. \"\n",
    "    \"I prefer concise, technical answers. Never use emojis in code or technical docs. \"\n",
    "    \"Please remember this for future sessions.\"\n",
    ")\n",
    "\n",
    "request_a = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_A,\n",
    "    message=onboarding_message,\n",
    ")\n",
    "\n",
    "answer_a = await runtime.run(request_a)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal assert #1: non-empty assistant answer\n",
    "# ---------------------------------------------------------------------\n",
    "assert isinstance(answer_a.answer, str) and answer_a.answer.strip(), \"Empty assistant answer in Session A.\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal assert #2: session history persisted (via SessionManager storage)\n",
    "# ---------------------------------------------------------------------\n",
    "history_a = await session_manager.get_history(session_id=SESSION_A)\n",
    "assert len(history_a) >= 2, f\"Expected >=2 messages in history, got {len(history_a)}.\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Trigger consolidation to LTM by closing the session\n",
    "# (This is the behavior boundary: Session -> LTM)\n",
    "# ---------------------------------------------------------------------\n",
    "await session_manager.close_session(session_id=SESSION_A)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal assert #3: LTM entries were created (semantic recall evidence)\n",
    "# We do a vector search against the user's LTM store.\n",
    "# ---------------------------------------------------------------------\n",
    "ltm_search = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"Artur Integrax Mooff preferences concise technical no emojis\",\n",
    "        top_k=5,\n",
    "        score_threshold=0.0,\n",
    "    )\n",
    "\n",
    "assert ltm_search.get(\"used_longterm\") is True, \"Expected long-term memory retrieval to be enabled.\"\n",
    "assert (ltm_search.get(\"debug\") or {}).get(\"hits_count\", 0) > 0, \"Expected at least one LTM entry after closing Session A.\"\n",
    "\n",
    "print(\"SESSION A OK\")\n",
    "print(\"Answer length:\", len(answer_a.answer))\n",
    "print(\"History messages:\", len(history_a))\n",
    "print(\"LTM hits_count:\", ltm_search.get(\"hits_count\"))\n",
    "print(\"LTM debug:\", ltm_search.get(\"debug\"))\n",
    "print(\"LTM hits:\", len(ltm_search.get(\"hits\") or []))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f27e96",
   "metadata": {},
   "source": [
    "## Cell 3 — Session B: recall (ChatGPT behavior)\n",
    "\n",
    "Goal:\n",
    "- Start a new session_id (fresh history).\n",
    "- Ask the system to recall facts from Session A using User LTM.\n",
    "- Minimal asserts:\n",
    "  1) Non-empty assistant answer\n",
    "  2) trace shows User LTM was used\n",
    "  3) Answer contains recalled facts/preferences\n",
    "  4) Session B history is isolated from Session A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION B OK\n",
      "Answer length: 294\n",
      "LTM debug: {'enabled': True, 'used': True, 'reason': 'hits', 'where': {'user_id': 'user_chatgpt_like_001', 'deleted': 0}, 'top_k': 8, 'threshold': None, 'raw_ids': ['5f18da4f7b8e4548bdaffe0ca684855a', '3a9d6f3525cc4819aa8cdfab88560317', '8ae1799e8a50424ebaa502498b1b51b8', '6662c8be36c84eb3bb60790ba15e4924'], 'raw_scores': [-0.2620260715484619, -0.44957518577575684, -0.49666833877563477, -0.5566085577011108], 'raw_metadatas': [{'entry_id': '5f18da4f7b8e4548bdaffe0ca684855a', 'user_id': 'user_chatgpt_like_001', 'kind': 'session_summary', 'source': 'session_consolidation', 'deleted': 0, 'tags': 'session_summary'}, {'entry_id': '3a9d6f3525cc4819aa8cdfab88560317', 'deleted': 0, 'kind': 'preference', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'tags': 'communication,tone'}, {'deleted': 0, 'tags': 'user,project', 'kind': 'user_fact', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'entry_id': '8ae1799e8a50424ebaa502498b1b51b8'}, {'tags': 'workflow,style', 'kind': 'preference', 'entry_id': '6662c8be36c84eb3bb60790ba15e4924', 'source': 'session_consolidation', 'user_id': 'user_chatgpt_like_001', 'deleted': 0}], 'raw_documents_preview': ['Artur zapoznał się z asystentem i wyjaśnił swoje preferencje dotyczące odpowiedzi. Zapytał o możliwość dyskusji lub rozwiązania konkretnego problemu.', 'Odpowiedź powinna być krótka, techniczna.', 'Artur buduje Integrax i Mooff.', 'Nie używać emotikonów w kodzie ani w dokumentacji technicznej.'], 'returned_count': 4, 'hits_count': 4, 'context_blocks_count': 1, 'context_preview': 'USER LONG-TERM MEMORY (retrieved)\\nUse these as factual user memory only if relevant to the question.\\nIf not relevant, ignore them.\\n\\n- [id=5f18da4f7b8e4548bdaffe0ca684855a, session=sess_chatgpt_like_A, kind=session_summary, importance=low] Artur zapoznał się z asystentem i wyjaśnił swoje preferencje dotyczące odpowiedzi. Zapytał o możliwość dyskusji lub rozwiązania konkretnego problemu.\\n- [id=3a9d6f3525cc4819aa8cdfab88560317, session=sess_chatgpt_like_A, kind=preference, importance=high] Odpowiedź powinna być krótka, techniczna.\\n- [id=8ae1799e8a50424ebaa502498b1b51b8, session=sess_chatgpt_like_A, kind=user_fact, importance=high] Artur buduje Integrax i Mooff.\\n- [id=6662c8be36c84eb3bb60790ba15e4924, session=sess_chatgpt_like_A, kind=preference, importance=high] Nie używać emotikonów w kodzie ani w dokumentacji technicznej.', 'context_preview_chars': 832, 'hits_preview': [{'entry_id': '5f18da4f7b8e4548bdaffe0ca684855a', 'title': 'Podsumowanie sesji', 'kind': 'session_summary', 'deleted': False}, {'entry_id': '3a9d6f3525cc4819aa8cdfab88560317', 'title': 'preferencje odpowiedzi techniczne', 'kind': 'preference', 'deleted': False}, {'entry_id': '8ae1799e8a50424ebaa502498b1b51b8', 'title': 'Twórca Integrax i Mooff', 'kind': 'user_fact', 'deleted': False}, {'entry_id': '6662c8be36c84eb3bb60790ba15e4924', 'title': 'brak emotikonów w kodzie i dokumentacji', 'kind': 'preference', 'deleted': False}]}\n",
      "History messages: 2\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.nexus.responses.response_schema import RuntimeRequest\n",
    "from intergrax.runtime.nexus.tracing.memory.user_longterm_memory_summary import UserLongtermMemorySummaryDiagV1\n",
    "from intergrax.runtime.nexus.tracing.trace_query import TraceQuery\n",
    "\n",
    "runtime = build_runtime()\n",
    "\n",
    "recall_prompt = (\n",
    "    \"Before we continue: remind me who I am and what I build. \"\n",
    "    \"Also remind me what answer style I prefer.\"\n",
    ")\n",
    "\n",
    "request_b = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_B,\n",
    "    message=recall_prompt,\n",
    ")\n",
    "\n",
    "answer_b = await runtime.run(request_b)\n",
    "\n",
    "# 1) Non-empty answer\n",
    "assert isinstance(answer_b.answer, str) and answer_b.answer.strip(), \"Empty assistant answer in Session B.\"\n",
    "\n",
    "# 1b) LLM usage must be present and non-empty\n",
    "assert_llm_usage_basic(answer_b, require_adapters=[\"core_adapter\"])\n",
    "\n",
    "# 2) Debug evidence: User LTM used\n",
    "q = TraceQuery.from_iter(answer_b.trace_events)\n",
    "\n",
    "ltm = q.one_payload(UserLongtermMemorySummaryDiagV1)\n",
    "\n",
    "# NOTE:\n",
    "# This assertion is valid only if earlier cells have already persisted something to user LTM.\n",
    "# If you run this cell standalone, it may fail.\n",
    "assert ltm.enabled, f\"Expected LTM enabled in Session B. diag={ltm.to_dict()}\"\n",
    "assert ltm.used_user_longterm_memory, f\"Expected LTM to be used in Session B. diag={ltm.to_dict()}\"\n",
    "assert ltm.hits_count > 0, f\"Expected LTM hits_count > 0 in Session B. diag={ltm.to_dict()}\"\n",
    "\n",
    "# 3) Behavior evidence: answer contains recalled facts/preferences\n",
    "ans_norm = answer_b.answer.lower()\n",
    "expected_any = [\n",
    "    \"artur\",\n",
    "    \"intergrax\",\n",
    "    \"mooff\",\n",
    "    \"concise\",\n",
    "    \"technical\",\n",
    "    \"never use emojis\",\n",
    "    \"no emojis\",\n",
    "]\n",
    "assert any(k in ans_norm for k in expected_any), (\n",
    "    \"Expected the answer to include recalled facts/preferences from Session A. \"\n",
    "    f\"Answer was:\\n{answer_b.answer}\"\n",
    ")\n",
    "\n",
    "# 4) Session history isolation sanity check\n",
    "history_b = await session_manager.get_history(session_id=SESSION_B)\n",
    "assert len(history_b) >= 2, f\"Expected >=2 messages in Session B history, got {len(history_b)}.\"\n",
    "\n",
    "# First user message in Session B should be the recall prompt (not Session A onboarding)\n",
    "first_user_b = next((m for m in history_b if getattr(m, \"role\", None) == \"user\"), None)\n",
    "assert first_user_b is not None, \"Expected a user message in Session B history.\"\n",
    "assert recall_prompt.strip() in (first_user_b.content or \"\"), \"Session B history isolation issue.\"\n",
    "\n",
    "print(\"SESSION B OK\")\n",
    "print(\"Answer length:\", len(answer_b.answer))\n",
    "print(\"LTM debug:\", ltm_dbg)\n",
    "print(\"History messages:\", len(history_b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6d343",
   "metadata": {},
   "source": [
    "## Cell 4 — Ingestion + RAG (document Q&A)\n",
    "\n",
    "Goal:\n",
    "- Ingest a real document into the **RAG vectorstore** via the runtime attachment ingestion flow.\n",
    "- Ask a question that can be answered only from the document.\n",
    "- Ask a second question that requires **RAG + User LTM** at the same time.\n",
    "\n",
    "Minimal asserts:\n",
    "1) Ingestion step completed (debug evidence)\n",
    "2) RAG was used (debug evidence: rag.used or rag_chunks > 0)\n",
    "3) Answer is non-empty and includes document facts\n",
    "4) For the combined question: both RAG and LTM were used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 1\n",
      "INGEST OK\n",
      "Ingestion debug: [{'attachment_id': 'rag_doc_001', 'attachment_type': 'md', 'num_chunks': 1, 'vector_ids_count': 1, 'metadata': {'source_path': 'D:\\\\Projekty\\\\intergrax\\\\notebooks\\\\nexus\\\\_artifacts\\\\notebook_11_chatgpt_like\\\\20251226_091323_5de869a1\\\\rag_doc_001.md', 'session_id': 'sess_chatgpt_like_RAG', 'user_id': 'user_chatgpt_like_001', 'tenant_id': None, 'workspace_id': None}}]\n",
      "DOC QA OK\n",
      "Answer length: 281\n",
      "RAG debug: {'enabled': True, 'used': True, 'hits_count': 1, 'where_filter': {'session_id': 'sess_chatgpt_like_RAG', 'user_id': 'user_chatgpt_like_001'}, 'top_k': 8, 'score_threshold': None, 'hits': [{'id': 'rag_doc_001-0', 'score': 0.9068, 'metadata': {'user_id': 'user_chatgpt_like_001', 'parent_id': '69bb64d5a70e4b5b', 'ext': '.md', 'attachment_id': 'rag_doc_001', 'session_id': 'sess_chatgpt_like_RAG', 'source_path': 'D:\\\\Projekty\\\\intergrax\\\\notebooks\\\\nexus\\\\_artifacts\\\\notebook_11_chatgpt_like\\\\20251226_091323_5de869a1\\\\rag_doc_001.md', 'chunk_id': '69bb64d5a70e4b5b#ch0000-467cd427', 'source': 'D:\\\\Projekty\\\\intergrax\\\\notebooks\\\\nexus\\\\_artifacts\\\\notebook_11_chatgpt_like\\\\20251226_091323_5de869a1\\\\rag_doc_001.md', 'attachment_type': 'md', 'chunk_total': 1, 'source_name': 'rag_doc_001.md', 'chunk_index': 0, 'label': 'RAG Demo Document'}, 'preview': '# Integrax — RAG Demo Document\\n\\nThis document is used for an E2E test of nexus Runtime.\\n\\nKey modules:\\n- nexus Runtime\\n- User Long-Term Memory (LTM)\\n- Websearch\\n\\nImportant const'}]}\n",
      "RAG chunks: 1\n",
      "COMBINED RAG+LTM OK\n",
      "Answer length: 240\n",
      "RAG chunks: 1\n",
      "LTM debug: {'enabled': True, 'used': True, 'reason': 'hits', 'where': {'user_id': 'user_chatgpt_like_001', 'deleted': 0}, 'top_k': 8, 'threshold': None, 'raw_ids': ['3a9d6f3525cc4819aa8cdfab88560317', '6662c8be36c84eb3bb60790ba15e4924', '8ae1799e8a50424ebaa502498b1b51b8', '5f18da4f7b8e4548bdaffe0ca684855a'], 'raw_scores': [-0.33048534393310547, -0.3462045192718506, -0.34972238540649414, -0.4128221273422241], 'raw_metadatas': [{'entry_id': '3a9d6f3525cc4819aa8cdfab88560317', 'tags': 'communication,tone', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'deleted': 0, 'kind': 'preference'}, {'deleted': 0, 'kind': 'preference', 'tags': 'workflow,style', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'entry_id': '6662c8be36c84eb3bb60790ba15e4924'}, {'kind': 'user_fact', 'entry_id': '8ae1799e8a50424ebaa502498b1b51b8', 'tags': 'user,project', 'source': 'session_consolidation', 'deleted': 0, 'user_id': 'user_chatgpt_like_001'}, {'tags': 'session_summary', 'kind': 'session_summary', 'deleted': 0, 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'entry_id': '5f18da4f7b8e4548bdaffe0ca684855a'}], 'raw_documents_preview': ['Odpowiedź powinna być krótka, techniczna.', 'Nie używać emotikonów w kodzie ani w dokumentacji technicznej.', 'Artur buduje Integrax i Mooff.', 'Artur zapoznał się z asystentem i wyjaśnił swoje preferencje dotyczące odpowiedzi. Zapytał o możliwość dyskusji lub rozwiązania konkretnego problemu.'], 'returned_count': 4, 'hits_count': 4, 'context_blocks_count': 1, 'context_preview': 'USER LONG-TERM MEMORY (retrieved)\\nUse these as factual user memory only if relevant to the question.\\nIf not relevant, ignore them.\\n\\n- [id=3a9d6f3525cc4819aa8cdfab88560317, session=sess_chatgpt_like_A, kind=preference, importance=high] Odpowiedź powinna być krótka, techniczna.\\n- [id=6662c8be36c84eb3bb60790ba15e4924, session=sess_chatgpt_like_A, kind=preference, importance=high] Nie używać emotikonów w kodzie ani w dokumentacji technicznej.\\n- [id=8ae1799e8a50424ebaa502498b1b51b8, session=sess_chatgpt_like_A, kind=user_fact, importance=high] Artur buduje Integrax i Mooff.\\n- [id=5f18da4f7b8e4548bdaffe0ca684855a, session=sess_chatgpt_like_A, kind=session_summary, importance=low] Artur zapoznał się z asystentem i wyjaśnił swoje preferencje dotyczące odpowiedzi. Zapytał o możliwość dyskusji lub rozwiązania konkretnego problemu.', 'context_preview_chars': 832, 'hits_preview': [{'entry_id': '3a9d6f3525cc4819aa8cdfab88560317', 'title': 'preferencje odpowiedzi techniczne', 'kind': 'preference', 'deleted': False}, {'entry_id': '6662c8be36c84eb3bb60790ba15e4924', 'title': 'brak emotikonów w kodzie i dokumentacji', 'kind': 'preference', 'deleted': False}, {'entry_id': '8ae1799e8a50424ebaa502498b1b51b8', 'title': 'Twórca Integrax i Mooff', 'kind': 'user_fact', 'deleted': False}, {'entry_id': '5f18da4f7b8e4548bdaffe0ca684855a', 'title': 'Podsumowanie sesji', 'kind': 'session_summary', 'deleted': False}]}\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.nexus.responses.response_schema import RuntimeRequest\n",
    "from intergrax.llm.messages import AttachmentRef\n",
    "from intergrax.runtime.nexus.ingestion.attachments import FileSystemAttachmentResolver\n",
    "from intergrax.runtime.nexus.ingestion.ingestion_service import AttachmentIngestionService\n",
    "from intergrax.runtime.nexus.tracing.attachments.attachments_context_summary import AttachmentsContextSummaryDiagV1\n",
    "from intergrax.runtime.nexus.tracing.memory.user_longterm_memory_summary import UserLongtermMemorySummaryDiagV1\n",
    "from intergrax.runtime.nexus.tracing.rag.rag_summary import RagSummaryDiagV1\n",
    "from intergrax.runtime.nexus.tracing.trace_query import TraceQuery\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Create a real document file to ingest\n",
    "# ---------------------------------------------------------------------\n",
    "DOC_SESSION = \"sess_chatgpt_like_RAG\"\n",
    "\n",
    "doc_path = ARTIFACTS_DIR / \"rag_doc_001.md\"\n",
    "doc_text = \"\"\"# Integrax — RAG Demo Document\n",
    "\n",
    "This document is used for an E2E test of nexus Runtime.\n",
    "\n",
    "Key modules:\n",
    "- nexus Runtime\n",
    "- User Long-Term Memory (LTM)\n",
    "- Websearch\n",
    "\n",
    "Important constants:\n",
    "- The default max entries per LTM query is 8.\n",
    "- The project codename for the demo is \"NEBULA-11\".\n",
    "\n",
    "Behavior requirement:\n",
    "- Answers must be concise and technical.\n",
    "\"\"\"\n",
    "doc_path.write_text(doc_text, encoding=\"utf-8\")\n",
    "\n",
    "attachment = AttachmentRef(\n",
    "    id=\"rag_doc_001\",\n",
    "    type=\"md\",\n",
    "    uri=doc_path.as_uri(),          # raw path is supported by FileSystemAttachmentResolver\n",
    "    metadata={\"label\": \"RAG Demo Document\"}\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build ingestion service (indexes into rag_vs)\n",
    "# ---------------------------------------------------------------------\n",
    "resolver = FileSystemAttachmentResolver()\n",
    "\n",
    "ingestion_service = AttachmentIngestionService(\n",
    "    resolver=resolver,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=rag_vs,     # IMPORTANT: documents go to RAG vectorstore\n",
    ")\n",
    "\n",
    "runtime = build_runtime(ingestion_service=ingestion_service)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Turn 1: Upload + ingestion\n",
    "# ---------------------------------------------------------------------\n",
    "request_ingest = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=DOC_SESSION,\n",
    "    message=\"I uploaded a document. Please ingest it and confirm.\",\n",
    "    attachments=[attachment],\n",
    ")\n",
    "\n",
    "answer_ingest = await runtime.run(request_ingest)\n",
    "\n",
    "assert isinstance(answer_ingest.answer, str) and answer_ingest.answer.strip(), \"Empty assistant answer after ingestion.\"\n",
    "\n",
    "# LLM usage must be present and non-empty\n",
    "assert_llm_usage_basic(answer_ingest, require_adapters=[\"core_adapter\"])\n",
    "\n",
    "q = TraceQuery.from_iter(answer_ingest.trace_events)\n",
    "\n",
    "# per-attachment results\n",
    "results = q.all_payloads(AttachmentsContextSummaryDiagV1)\n",
    "assert len(results) > 0, \"Expected at least one AttachmentsContextSummaryDiagV1 payload.\"\n",
    "\n",
    "# summary (optional but usually present)\n",
    "summary = q.first_payload(AttachmentsContextSummaryDiagV1)\n",
    "assert summary is not None, \"Expected AttachmentsContextSummaryDiagV1 payload.\"\n",
    "\n",
    "print(\"INGEST OK\")\n",
    "print(\"Ingestion summary:\", summary.to_dict())\n",
    "print(\"Ingestion results:\")\n",
    "for r in results:\n",
    "    print(\"-\", r.to_dict())\n",
    "# ---------------------------------------------------------------------\n",
    "# Turn 2: Pure document Q&A (RAG must be used)\n",
    "# ---------------------------------------------------------------------\n",
    "q_doc = \"In the uploaded document: what is the demo codename and what are the three key modules?\"\n",
    "\n",
    "request_doc_qa = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=DOC_SESSION,\n",
    "    message=q_doc,\n",
    ")\n",
    "\n",
    "answer_doc = await runtime.run(request_doc_qa)\n",
    "\n",
    "assert isinstance(answer_doc.answer, str) and answer_doc.answer.strip(), \"Empty assistant answer in document Q&A.\"\n",
    "\n",
    "# LLM usage must be present and non-empty\n",
    "assert_llm_usage_basic(answer_doc, require_adapters=[\"core_adapter\"])\n",
    "\n",
    "q = TraceQuery.from_iter(answer_doc.trace_events)\n",
    "\n",
    "rag = q.one_payload(RagSummaryDiagV1)\n",
    "assert rag.rag_enabled, f\"Expected RAG enabled. diag={rag.to_dict()}\"\n",
    "assert rag.used_rag, f\"Expected RAG to be used. diag={rag.to_dict()}\"\n",
    "assert rag.chunks_count > 0, f\"Expected chunks_count > 0. diag={rag.to_dict()}\"\n",
    "\n",
    "ans_doc_norm = answer_doc.answer.lower()\n",
    "assert \"nebula-11\" in ans_doc_norm, \"Expected the answer to include the codename from the document.\"\n",
    "assert (\"drop-in\" in ans_doc_norm) or (\"long-term\" in ans_doc_norm) or (\"websearch\" in ans_doc_norm), (\n",
    "    \"Expected the answer to include at least one key module from the document.\"\n",
    ")\n",
    "\n",
    "print(\"DOC QA OK\")\n",
    "print(\"Answer length:\", len(answer_doc.answer))\n",
    "print(\"RAG chunks:\", rag.chunks_count)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Turn 3: Combined question (RAG + LTM)\n",
    "# - Must use RAG (document facts) and LTM (user preference / identity).\n",
    "# ---------------------------------------------------------------------\n",
    "q_combined = (\n",
    "    \"Using the uploaded document AND what you remember about me: \"\n",
    "    \"write a concise technical answer that (1) states who I am and what I build, \"\n",
    "    \"(2) lists the document's key modules, and (3) includes the codename.\"\n",
    ")\n",
    "\n",
    "request_combined = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=DOC_SESSION,\n",
    "    message=q_combined,\n",
    ")\n",
    "\n",
    "answer_combined = await runtime.run(request_combined)\n",
    "\n",
    "assert isinstance(answer_combined.answer, str) and answer_combined.answer.strip(), \"Empty assistant answer in combined RAG+LTM question.\"\n",
    "\n",
    "# LLM usage must be present and non-empty\n",
    "assert_llm_usage_basic(answer_combined, require_adapters=[\"core_adapter\"])\n",
    "\n",
    "q = TraceQuery.from_iter(answer_combined.trace_events)\n",
    "\n",
    "# --- RAG evidence (typed) ---\n",
    "rag = q.one_payload(RagSummaryDiagV1)\n",
    "assert rag.rag_enabled, f\"Expected RAG enabled. diag={rag.to_dict()}\"\n",
    "assert rag.used_rag, f\"Expected RAG to be used in combined question. diag={rag.to_dict()}\"\n",
    "assert rag.chunks_count > 0, f\"Expected chunks_count > 0. diag={rag.to_dict()}\"\n",
    "\n",
    "# --- LTM evidence (typed) ---\n",
    "ltm = q.one_payload(UserLongtermMemorySummaryDiagV1)\n",
    "assert ltm.enabled, f\"Expected LTM enabled. diag={ltm.to_dict()}\"\n",
    "assert ltm.used_user_longterm_memory, f\"Expected LTM to be used in combined question. diag={ltm.to_dict()}\"\n",
    "assert ltm.hits_count > 0, f\"Expected LTM hits_count > 0. diag={ltm.to_dict()}\"\n",
    "assert ltm.context_blocks_count > 0, f\"Expected LTM context_blocks_count > 0. diag={ltm.to_dict()}\"\n",
    "assert ltm.context_preview.strip(), f\"Expected non-empty LTM context_preview. diag={ltm.to_dict()}\"\n",
    "\n",
    "# --- Behavior evidence: contains memory facts + doc codename ---\n",
    "ans2 = answer_combined.answer.lower()\n",
    "assert \"artur\" in ans2, \"Expected the combined answer to include 'Artur' from LTM.\"\n",
    "assert (\"intergrax\" in ans2) or (\"mooff\" in ans2), \"Expected the combined answer to include Integrax/Mooff from LTM.\"\n",
    "assert \"nebula-11\" in ans2, \"Expected the combined answer to include the document codename.\"\n",
    "\n",
    "print(\"COMBINED RAG+LTM OK\")\n",
    "print(\"Answer length:\", len(answer_combined.answer))\n",
    "print(\"RAG chunks:\", rag.chunks_count)\n",
    "print(\"LTM hits:\", ltm.hits_count)\n",
    "print(\"LTM blocks:\", ltm.context_blocks_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28407554",
   "metadata": {},
   "source": [
    "## Cell 5 — Websearch (current knowledge, no documents)\n",
    "\n",
    "Goal:\n",
    "- Ask a question that requires up-to-date knowledge.\n",
    "- Ensure there are no documents in context (disable RAG for this cell).\n",
    "- Verify that:\n",
    "  1) Websearch was used (debug evidence)\n",
    "  2) Websearch produced context blocks / sources\n",
    "  3) The final answer references the retrieved web context (behavior evidence)\n",
    "\n",
    "Minimal asserts:\n",
    "- Non-empty assistant answer\n",
    "- websearch.used == True (or equivalent)\n",
    "- websearch.context_blocks_count > 0 (or sources_count > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed30c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEBSEARCH OK\n",
      "Answer length: 393\n",
      "used_websearch: True\n",
      "Websearch blocks: 1\n",
      "Websearch preview chars: 587\n",
      "Websearch no_evidence: False\n",
      "Websearch docs preview: [{'title': 'Strategie projektowania promptów \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'url': 'https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=pl'}, {'title': 'Java Jobs for December 2025 | Freelancer', 'url': 'https://www.freelancer.pl/jobs/java'}, {'title': 'Relation 2025 - Data&AI Warsaw Tech Summit | Data&AI Warsaw Tech Summit', 'url': 'https://dataiwarsaw.tech/relation-2025/'}, {'title': 'Promocja urodzinowa Ebookpoint 2025 – Informatyka | Świat Czytników', 'url': 'https://swiatczytnikow.pl/ebookpoint-informatyka/'}, {'title': 'Reddit - The heart of the internet', 'url': 'https://www.reddit.com/r/apple/comments/13ajz6z/boring_report_a_news_app_that_uses_ai_language/'}]\n",
      "Websearch raw preview top urls: ['https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=pl', 'https://www.freelancer.pl/jobs/java', 'https://dataiwarsaw.tech/relation-2025/', 'https://swiatczytnikow.pl/ebookpoint-informatyka/']\n",
      "LLM usage adapters: ['core_adapter', 'web_map_adapter', 'web_reduce_adapter']\n",
      "LLM usage total: {'calls': 10, 'input_tokens': 6947, 'output_tokens': 472, 'total_tokens': 7419, 'duration_ms': 15748, 'errors': 0}\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.nexus.responses.response_schema import RuntimeRequest\n",
    "from intergrax.runtime.nexus.tracing.adapters.llm_usage_snapshot import LLMUsageSnapshotDiag\n",
    "from intergrax.runtime.nexus.tracing.trace_query import TraceQuery\n",
    "from intergrax.runtime.nexus.tracing.websearch.websearch_summary import WebsearchSummaryDiagV1\n",
    "from intergrax.websearch.service.websearch_config import WebSearchConfig, WebSearchStrategyType\n",
    "\n",
    "WEB_SESSION = \"sess_chatgpt_like_WEB\"\n",
    "\n",
    "# Build a fully configured websearch config (no touching private runtime._config)\n",
    "ws_cfg = WebSearchConfig(strategy=WebSearchStrategyType.MAP_REDUCE)\n",
    "ws_cfg.llm.map_adapter = llm_adapter\n",
    "ws_cfg.llm.reduce_adapter = llm_adapter\n",
    "# optional:\n",
    "# ws_cfg.llm.rerank_adapter = llm_adapter\n",
    "\n",
    "runtime_web = build_runtime(\n",
    "    override_config={\n",
    "        \"enable_rag\": False,\n",
    "        \"enable_websearch\": True,\n",
    "        \"websearch_config\": ws_cfg,\n",
    "    }\n",
    ")\n",
    "\n",
    "web_q = (\n",
    "    \"What are the most recent major changes to the OpenAI API regarding the Responses API \"\n",
    "    \"and tool calling? Provide a concise technical summary with the date of the change.\"\n",
    ")\n",
    "\n",
    "request_web = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=WEB_SESSION,\n",
    "    message=web_q,\n",
    ")\n",
    "\n",
    "answer_web = await runtime_web.run(request_web)\n",
    "\n",
    "# 1) Non-empty answer (runtime answered)\n",
    "assert isinstance(answer_web.answer, str) and answer_web.answer.strip(), \"Empty assistant answer in Websearch session.\"\n",
    "\n",
    "# 1b) LLM usage must be present and non-empty\n",
    "usage = assert_llm_usage_basic(answer_web, require_adapters=[\"core_adapter\"])\n",
    "\n",
    "adapters = usage.get(\"adapters\") or {}\n",
    "assert \"web_map_adapter\" in adapters, f\"Missing web_map_adapter. present={list(adapters.keys())}\"\n",
    "assert \"web_reduce_adapter\" in adapters, f\"Missing web_reduce_adapter. present={list(adapters.keys())}\"\n",
    "assert int((adapters[\"web_map_adapter\"] or {}).get(\"calls\", 0) or 0) >= 1, f\"web_map_adapter.calls invalid: {adapters['web_map_adapter']}\"\n",
    "assert int((adapters[\"web_reduce_adapter\"] or {}).get(\"calls\", 0) or 0) >= 1, f\"web_reduce_adapter.calls invalid: {adapters['web_reduce_adapter']}\"\n",
    "\n",
    "# 2) Routing says websearch was used\n",
    "route = answer_web.route\n",
    "assert route is not None, \"Missing route in RuntimeAnswer.\"\n",
    "assert route.used_websearch is True, f\"Expected used_websearch=True. route={route}\"\n",
    "\n",
    "q = TraceQuery.from_iter(answer_web.trace_events)\n",
    "\n",
    "# --- Websearch summary (typed) ---\n",
    "ws = q.one_payload(WebsearchSummaryDiagV1)\n",
    "\n",
    "assert ws.enabled is True, f\"Expected websearch enabled. diag={ws.to_dict()}\"\n",
    "assert ws.configured is True, f\"Expected websearch configured. diag={ws.to_dict()}\"\n",
    "assert ws.used_websearch is True, f\"Expected used_websearch True. diag={ws.to_dict()}\"\n",
    "\n",
    "# 3) Websearch produced context blocks + preview\n",
    "ctx_blocks = int(ws.context_blocks_count)\n",
    "assert ctx_blocks > 0, f\"Expected websearch context blocks > 0. diag={ws.to_dict()}\"\n",
    "\n",
    "preview = (ws.context_preview or \"\").strip()\n",
    "assert preview, f\"Expected non-empty websearch context preview. diag={ws.to_dict()}\"\n",
    "\n",
    "# Raw results preview is no longer available as a typed field.\n",
    "# Replace it with a stable assertion on results_count.\n",
    "assert int(ws.results_count) > 0, f\"Expected websearch results_count > 0. diag={ws.to_dict()}\"\n",
    "\n",
    "# No evidence / errors should be false/empty in the happy path.\n",
    "assert not ws.no_evidence, f\"Expected no_evidence=False. diag={ws.to_dict()}\"\n",
    "assert (ws.error_type or \"\") == \"\", f\"Expected no error_type. diag={ws.to_dict()}\"\n",
    "assert (ws.error_message or \"\") == \"\", f\"Expected no error_message. diag={ws.to_dict()}\"\n",
    "\n",
    "# --- LLM usage (typed) ---\n",
    "usage = q.first_payload(LLMUsageSnapshotDiag)\n",
    "assert usage is not None, \"Expected LLMUsageSnapshotDiag in trace events.\"\n",
    "\n",
    "assert usage.calls >= 1, f\"Expected usage.calls >= 1. diag={usage.to_dict()}\"\n",
    "assert usage.total_tokens == usage.input_tokens + usage.output_tokens, (\n",
    "    f\"Expected total_tokens == input_tokens + output_tokens. diag={usage.to_dict()}\"\n",
    ")\n",
    "\n",
    "assert answer_web.answer is not None and answer_web.answer.strip() != \"\", \"Expected a non-empty final answer\"\n",
    "\n",
    "print(\"WEBSEARCH OK\")\n",
    "print(\"Answer length:\", len(answer_web.answer))\n",
    "print(\"used_websearch (route):\", route.used_websearch)\n",
    "print(\"Websearch results_count:\", ws.results_count)\n",
    "print(\"Websearch blocks:\", ctx_blocks)\n",
    "print(\"Websearch preview chars:\", ws.context_preview_chars)\n",
    "print(\"Websearch no_evidence:\", ws.no_evidence)\n",
    "print(\"LLM usage calls:\", usage.calls)\n",
    "print(\"LLM usage total_tokens:\", usage.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48cc3885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': 'sess_chatgpt_like_WEB',\n",
       " 'user_id': 'user_chatgpt_like_001',\n",
       " 'memory_layer': {'implemented': True,\n",
       "  'has_user_profile_instructions': True,\n",
       "  'has_org_profile_instructions': False,\n",
       "  'enable_user_profile_memory': True,\n",
       "  'enable_org_profile_memory': False},\n",
       " 'steps': [{'timestamp': '2025-12-26T09:14:12.772073+00:00',\n",
       "   'component': 'engine',\n",
       "   'step': 'memory_layer',\n",
       "   'message': 'Profile-based instructions loaded for session.',\n",
       "   'data': {'has_user_profile_instructions': True,\n",
       "    'has_org_profile_instructions': False,\n",
       "    'enable_user_profile_memory': True,\n",
       "    'enable_org_profile_memory': False}},\n",
       "  {'timestamp': '2025-12-26T09:14:12.772073+00:00',\n",
       "   'component': 'engine',\n",
       "   'step': 'history',\n",
       "   'message': 'Conversation history built for LLM.',\n",
       "   'data': {'history_length': 1,\n",
       "    'base_history_length': 1,\n",
       "    'history_includes_current_user': True}},\n",
       "  {'timestamp': '2025-12-26T09:14:12.772073+00:00',\n",
       "   'component': 'reasoning',\n",
       "   'step': 'apply_reasoning_to_instructions',\n",
       "   'message': 'Reasoning policy applied to system instructions.',\n",
       "   'data': {'mode': 'direct', 'applied': False}},\n",
       "  {'timestamp': '2025-12-26T09:14:13.093687+00:00',\n",
       "   'component': 'engine',\n",
       "   'step': 'user_longterm_memory',\n",
       "   'message': 'User long-term memory step executed.',\n",
       "   'data': {'ltm_enabled': True,\n",
       "    'used_user_longterm_memory': True,\n",
       "    'hits': 4}},\n",
       "  {'timestamp': '2025-12-26T09:14:42.608885+00:00',\n",
       "   'component': 'engine',\n",
       "   'step': 'websearch',\n",
       "   'message': 'Web search step executed.',\n",
       "   'data': {'websearch_enabled': True,\n",
       "    'used_websearch': True,\n",
       "    'has_error': False,\n",
       "    'no_evidence': False}},\n",
       "  {'timestamp': '2025-12-26T09:14:44.855422+00:00',\n",
       "   'component': 'engine',\n",
       "   'step': 'core_llm',\n",
       "   'message': 'Core LLM adapter returned answer.',\n",
       "   'data': {'used_tools_answer': False,\n",
       "    'adapter_return_type': 'str',\n",
       "    'answer_len': 393,\n",
       "    'answer_is_empty': False}},\n",
       "  {'timestamp': '2025-12-26T09:14:44.855422+00:00',\n",
       "   'component': 'engine',\n",
       "   'step': 'persist_and_build_answer',\n",
       "   'message': 'Assistant answer persisted and RuntimeAnswer built.',\n",
       "   'data': {'session_id': 'sess_chatgpt_like_WEB',\n",
       "    'strategy': 'llm_with_websearch',\n",
       "    'used_rag': False,\n",
       "    'used_websearch': True,\n",
       "    'used_tools': False}},\n",
       "  {'timestamp': '2025-12-26T09:14:44.855422+00:00',\n",
       "   'component': 'engine',\n",
       "   'step': 'run_end',\n",
       "   'message': 'DropInKnowledgeRuntime.run() finished.',\n",
       "   'data': {'strategy': 'llm_with_websearch',\n",
       "    'used_rag': False,\n",
       "    'used_websearch': True,\n",
       "    'used_tools': False,\n",
       "    'used_user_longterm_memory': True,\n",
       "    'run_id': 'run_bb0606d42d334edfa20247e482cd618d'}}],\n",
       " 'base_history_length': 1,\n",
       " 'history_tokens': {'raw_history_messages': 1,\n",
       "  'raw_history_tokens': 32,\n",
       "  'history_budget_tokens': 4096,\n",
       "  'strategy_requested': 'truncate_oldest',\n",
       "  'strategy_effective': 'truncate_oldest',\n",
       "  'truncated': False,\n",
       "  'summary_used': False,\n",
       "  'summary_tokens_budget': 0,\n",
       "  'tail_tokens_budget': 0},\n",
       " 'history_length': 1,\n",
       " 'instructions': {'has_instructions': True,\n",
       "  'sources': {'request': False,\n",
       "   'user_profile': True,\n",
       "   'organization_profile': False}},\n",
       " 'reasoning': {'mode': 'direct',\n",
       "  'applied': False,\n",
       "  'extra': {'note': 'DIRECT mode (no reasoning augmentation).'}},\n",
       " 'rag_chunks': 0,\n",
       " 'user_longterm_memory_hits': 4,\n",
       " 'user_longterm_memory': {'enabled': True,\n",
       "  'used': True,\n",
       "  'reason': 'hits',\n",
       "  'where': {'user_id': 'user_chatgpt_like_001', 'deleted': 0},\n",
       "  'top_k': 8,\n",
       "  'threshold': None,\n",
       "  'raw_ids': ['8ae1799e8a50424ebaa502498b1b51b8',\n",
       "   '6662c8be36c84eb3bb60790ba15e4924',\n",
       "   '5f18da4f7b8e4548bdaffe0ca684855a',\n",
       "   '3a9d6f3525cc4819aa8cdfab88560317'],\n",
       "  'raw_scores': [-0.4362671375274658,\n",
       "   -0.486522912979126,\n",
       "   -0.49448537826538086,\n",
       "   -0.5329033136367798],\n",
       "  'raw_metadatas': [{'tags': 'user,project',\n",
       "    'entry_id': '8ae1799e8a50424ebaa502498b1b51b8',\n",
       "    'deleted': 0,\n",
       "    'kind': 'user_fact',\n",
       "    'user_id': 'user_chatgpt_like_001',\n",
       "    'source': 'session_consolidation'},\n",
       "   {'user_id': 'user_chatgpt_like_001',\n",
       "    'source': 'session_consolidation',\n",
       "    'tags': 'workflow,style',\n",
       "    'kind': 'preference',\n",
       "    'entry_id': '6662c8be36c84eb3bb60790ba15e4924',\n",
       "    'deleted': 0},\n",
       "   {'user_id': 'user_chatgpt_like_001',\n",
       "    'tags': 'session_summary',\n",
       "    'kind': 'session_summary',\n",
       "    'source': 'session_consolidation',\n",
       "    'deleted': 0,\n",
       "    'entry_id': '5f18da4f7b8e4548bdaffe0ca684855a'},\n",
       "   {'tags': 'communication,tone',\n",
       "    'deleted': 0,\n",
       "    'entry_id': '3a9d6f3525cc4819aa8cdfab88560317',\n",
       "    'user_id': 'user_chatgpt_like_001',\n",
       "    'kind': 'preference',\n",
       "    'source': 'session_consolidation'}],\n",
       "  'raw_documents_preview': ['Artur buduje Integrax i Mooff.',\n",
       "   'Nie używać emotikonów w kodzie ani w dokumentacji technicznej.',\n",
       "   'Artur zapoznał się z asystentem i wyjaśnił swoje preferencje dotyczące odpowiedzi. Zapytał o możliwość dyskusji lub rozwiązania konkretnego problemu.',\n",
       "   'Odpowiedź powinna być krótka, techniczna.'],\n",
       "  'returned_count': 4,\n",
       "  'hits_count': 4,\n",
       "  'context_blocks_count': 1,\n",
       "  'context_preview': 'USER LONG-TERM MEMORY (retrieved)\\nUse these as factual user memory only if relevant to the question.\\nIf not relevant, ignore them.\\n\\n- [id=8ae1799e8a50424ebaa502498b1b51b8, session=sess_chatgpt_like_A, kind=user_fact, importance=high] Artur buduje Integrax i Mooff.\\n- [id=6662c8be36c84eb3bb60790ba15e4924, session=sess_chatgpt_like_A, kind=preference, importance=high] Nie używać emotikonów w kodzie ani w dokumentacji technicznej.\\n- [id=5f18da4f7b8e4548bdaffe0ca684855a, session=sess_chatgpt_like_A, kind=session_summary, importance=low] Artur zapoznał się z asystentem i wyjaśnił swoje preferencje dotyczące odpowiedzi. Zapytał o możliwość dyskusji lub rozwiązania konkretnego problemu.\\n- [id=3a9d6f3525cc4819aa8cdfab88560317, session=sess_chatgpt_like_A, kind=preference, importance=high] Odpowiedź powinna być krótka, techniczna.',\n",
       "  'context_preview_chars': 832,\n",
       "  'hits_preview': [{'entry_id': '8ae1799e8a50424ebaa502498b1b51b8',\n",
       "    'title': 'Twórca Integrax i Mooff',\n",
       "    'kind': 'user_fact',\n",
       "    'deleted': False},\n",
       "   {'entry_id': '6662c8be36c84eb3bb60790ba15e4924',\n",
       "    'title': 'brak emotikonów w kodzie i dokumentacji',\n",
       "    'kind': 'preference',\n",
       "    'deleted': False},\n",
       "   {'entry_id': '5f18da4f7b8e4548bdaffe0ca684855a',\n",
       "    'title': 'Podsumowanie sesji',\n",
       "    'kind': 'session_summary',\n",
       "    'deleted': False},\n",
       "   {'entry_id': '3a9d6f3525cc4819aa8cdfab88560317',\n",
       "    'title': 'preferencje odpowiedzi techniczne',\n",
       "    'kind': 'preference',\n",
       "    'deleted': False}]},\n",
       " 'websearch': {'raw_results_preview': [{'type': 'WebSearchResult',\n",
       "    'title': 'Strategie projektowania promptów \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers',\n",
       "    'url': 'https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=pl',\n",
       "    'snippet_len': 118,\n",
       "    'text_len': 36238},\n",
       "   {'type': 'WebSearchResult',\n",
       "    'title': 'Java Jobs for December 2025 | Freelancer',\n",
       "    'url': 'https://www.freelancer.pl/jobs/java',\n",
       "    'snippet_len': 132,\n",
       "    'text_len': 39093},\n",
       "   {'type': 'WebSearchResult',\n",
       "    'title': 'Relation 2025 - Data&AI Warsaw Tech Summit | Data&AI Warsaw Tech Summit',\n",
       "    'url': 'https://dataiwarsaw.tech/relation-2025/',\n",
       "    'snippet_len': 154,\n",
       "    'text_len': 58637},\n",
       "   {'type': 'WebSearchResult',\n",
       "    'title': 'Promocja urodzinowa Ebookpoint 2025 – Informatyka | Świat Czytników',\n",
       "    'url': 'https://swiatczytnikow.pl/ebookpoint-informatyka/',\n",
       "    'snippet_len': 153,\n",
       "    'text_len': 2231337},\n",
       "   {'type': 'WebSearchResult',\n",
       "    'title': 'Reddit - The heart of the internet',\n",
       "    'url': 'https://www.reddit.com/r/apple/comments/13ajz6z/boring_report_a_news_app_that_uses_ai_language/',\n",
       "    'snippet_len': 156,\n",
       "    'text_len': 964}],\n",
       "  'context_blocks_count': 1,\n",
       "  'context_preview': \"WEB SOURCES (MAP_REDUCE GROUNDED)\\n- The following context was synthesized from fetched pages.\\n- Cite sources using [index] markers and URLs.\\n\\nGROUNDED_CONTEXT:\\n* There is no evidence of recent major changes to the OpenAI API ( [3] )\\n* The Boring Report app uses OpenAI's Responses API to transform news articles into a more factual format ( [5] )\\n* No specific changes are mentioned regarding tool calling in the provided excerpt ( [5] )\\n\\nSOURCES:\\n• https://dataiwarsaw.tech/relation-2025/\\n• https://www.reddit.com/r/apple/comments/13ajz6z/boring_report_a_news_app_that_uses_ai_language/\",\n",
       "  'context_preview_chars': 587,\n",
       "  'no_evidence': False,\n",
       "  'docs_preview': [{'title': 'Strategie projektowania promptów \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers',\n",
       "    'url': 'https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=pl'},\n",
       "   {'title': 'Java Jobs for December 2025 | Freelancer',\n",
       "    'url': 'https://www.freelancer.pl/jobs/java'},\n",
       "   {'title': 'Relation 2025 - Data&AI Warsaw Tech Summit | Data&AI Warsaw Tech Summit',\n",
       "    'url': 'https://dataiwarsaw.tech/relation-2025/'},\n",
       "   {'title': 'Promocja urodzinowa Ebookpoint 2025 – Informatyka | Świat Czytników',\n",
       "    'url': 'https://swiatczytnikow.pl/ebookpoint-informatyka/'},\n",
       "   {'title': 'Reddit - The heart of the internet',\n",
       "    'url': 'https://www.reddit.com/r/apple/comments/13ajz6z/boring_report_a_news_app_that_uses_ai_language/'}]},\n",
       " 'llm_usage': {'run_id': 'run_bb0606d42d334edfa20247e482cd618d',\n",
       "  'total': {'calls': 10,\n",
       "   'input_tokens': 6947,\n",
       "   'output_tokens': 472,\n",
       "   'total_tokens': 7419,\n",
       "   'duration_ms': 15748,\n",
       "   'errors': 0},\n",
       "  'adapters': {'core_adapter': {'calls': 10,\n",
       "    'input_tokens': 6947,\n",
       "    'output_tokens': 472,\n",
       "    'total_tokens': 7419,\n",
       "    'duration_ms': 15748,\n",
       "    'errors': 0},\n",
       "   'web_map_adapter': {'calls': 10,\n",
       "    'input_tokens': 6947,\n",
       "    'output_tokens': 472,\n",
       "    'total_tokens': 7419,\n",
       "    'duration_ms': 15748,\n",
       "    'errors': 0},\n",
       "   'web_reduce_adapter': {'calls': 10,\n",
       "    'input_tokens': 6947,\n",
       "    'output_tokens': 472,\n",
       "    'total_tokens': 7419,\n",
       "    'duration_ms': 15748,\n",
       "    'errors': 0}},\n",
       "  'adapter_instance_ids': {'core_adapter': 2366542548000,\n",
       "   'web_map_adapter': 2366542548000,\n",
       "   'web_reduce_adapter': 2366542548000}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
