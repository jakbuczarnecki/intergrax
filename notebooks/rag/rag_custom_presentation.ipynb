{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27157c81",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce05732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc4865",
   "metadata": {},
   "source": [
    "### Load files with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intergrax.rag.documents_loader import IntergraxDocumentsLoader\n",
    "\n",
    "import intergrax.logging\n",
    "\n",
    "doc_loader = IntergraxDocumentsLoader(verbose=True, docx_mode=\"paragraphs\")\n",
    "docs = doc_loader.load_documents(\"../documents/mooff-strategy\",)\n",
    "print(f\"Loaded docs: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2211fd",
   "metadata": {},
   "source": [
    "### Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.documents_splitter import IntergraxDocumentsSplitter\n",
    "from intergrax.rag.documents_loader import IntergraxDocumentsLoader\n",
    "from langchain_core.documents import Document\n",
    "import intergrax.logging\n",
    "\n",
    "doc_loader = IntergraxDocumentsLoader(verbose=True, docx_mode=\"paragraphs\")\n",
    "docs = doc_loader.load_documents(\"../documents/mooff-strategy\",)\n",
    "\n",
    "splitter = IntergraxDocumentsSplitter(verbose=True)\n",
    "split_docs = splitter.split_documents(documents=docs)\n",
    "\n",
    "print(f\"Loaded docs: {len(docs)} - splitter into {len(split_docs)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a79eea",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.documents_splitter import IntergraxDocumentsSplitter\n",
    "from intergrax.rag.documents_loader import IntergraxDocumentsLoader\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "import intergrax.logging\n",
    "\n",
    "doc_loader = IntergraxDocumentsLoader(verbose=True)\n",
    "docs = doc_loader.load_documents(\"../documents/mooff-strategy/doc\",)\n",
    "\n",
    "splitter = IntergraxDocumentsSplitter(verbose=True)\n",
    "split_docs = splitter.split_documents(documents=docs)\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\", \n",
    "    assume_ollama_dim=1536)\n",
    "\n",
    "embeddings, documents = embed_manager.embed_documents(docs=docs)\n",
    "print(f\"Embedding length: {len(embeddings)} for documents: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad672d",
   "metadata": {},
   "source": [
    "# Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dcf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "from intergrax.rag.documents_splitter import IntergraxDocumentsSplitter\n",
    "from intergrax.rag.documents_loader import IntergraxDocumentsLoader\n",
    "from langchain_core.documents import Document\n",
    "from collections import Counter\n",
    "\n",
    "TENANT = \"intergrax\"\n",
    "CORPUS = \"intergrax-strategy\"\n",
    "VERSION = \"v1\"\n",
    "\n",
    "# 0) Vectorstore na starcie (bez ładowania/splita/embedu)\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=True)\n",
    "\n",
    "# 1) Tylko 'probe' do sprawdzenia obecności korpusu (lekko, bez embedowania chunków)\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "def corpus_present(store: IntergraxVectorstoreManager, embed_mgr: IntergraxEmbeddingManager) -> bool:\n",
    "    if store.count() == 0:\n",
    "        return False\n",
    "    qvec = embed_mgr.embed_one(\"probe\")\n",
    "    \n",
    "    where = {\n",
    "        \"$and\": [\n",
    "            {\"tenant\": {\"$eq\": TENANT}},\n",
    "            {\"corpus\": {\"$eq\": CORPUS}},\n",
    "            {\"version\": {\"$eq\": VERSION}},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    res = store.query(query_embeddings=qvec, top_k=1, where=where)\n",
    "    return bool(res[\"ids\"] and res[\"ids\"][0])\n",
    "\n",
    "NEED_INGEST = not corpus_present(store, embed_manager)\n",
    "print(f\"Need ingest: {NEED_INGEST}\")\n",
    "\n",
    "if not NEED_INGEST:\n",
    "    # Nic nie ładujemy: unikamy zbędnego splitu i embedów\n",
    "    print(\"[INGEST] Skipping — corpus already present.\")\n",
    "else:\n",
    "    # 2) Load\n",
    "    doc_loader = IntergraxDocumentsLoader(verbose=False)\n",
    "    docs = doc_loader.load_documents(\"../documents/mooff-strategy/doc\")\n",
    "\n",
    "    # 3) Split + metadata\n",
    "    def add_meta(chunk_doc: Document, idx: int, total: int):\n",
    "        return {\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION}\n",
    "\n",
    "    splitter = IntergraxDocumentsSplitter(verbose=False)\n",
    "    split_docs = splitter.split_documents(documents=docs, call_custom_metadata=add_meta)\n",
    "\n",
    "    # 4) Embed tylko chunków\n",
    "    embeddings, documents = embed_manager.embed_documents(docs=split_docs)\n",
    "\n",
    "    # 5) Stabilne ID — bierz prosto z metadanych splittera (chunk_id)\n",
    "    ids = []\n",
    "    for d in documents:\n",
    "        cid = d.metadata.get(\"chunk_id\")\n",
    "        if not cid:\n",
    "            # awaryjnie: parent + index (hash już nadaje splitter; tu fallback)\n",
    "            parent = d.metadata.get(\"parent_id\") or d.metadata.get(\"source_path\") or d.metadata.get(\"source_name\", \"doc\")\n",
    "            idx = int(d.metadata.get(\"chunk_index\", 0))\n",
    "            cid = f\"{parent}#ch{idx:04d}\"\n",
    "        ids.append(cid)\n",
    "\n",
    "    # 6) Deduplikacja ID w paczce (Chroma wymaga unikalnych ID per upsert)\n",
    "    def dedup_batch(ids, docs, embs):\n",
    "        seen = set()\n",
    "        new_ids, new_docs, new_embs = [], [], []\n",
    "        for i, _id in enumerate(ids):\n",
    "            if _id in seen:\n",
    "                continue\n",
    "            seen.add(_id)\n",
    "            new_ids.append(_id)\n",
    "            new_docs.append(docs[i])\n",
    "            new_embs.append(embs[i])\n",
    "        return new_ids, new_docs, new_embs\n",
    "\n",
    "    ids, documents, embeddings = dedup_batch(ids, documents, embeddings)\n",
    "\n",
    "    # (opcjonalnie) ostrzeż, jeśli w batchu były duplikaty\n",
    "    c = Counter(ids)\n",
    "    dups = [k for k, v in c.items() if v > 1]\n",
    "    if dups:\n",
    "        print(f\"[WARN] Duplicate IDs after dedup? {len(dups)}\")\n",
    "\n",
    "    # 7) Ingest\n",
    "    base_metadata = {\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION}\n",
    "    store.add_documents(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        batch_size=128,\n",
    "        base_metadata=base_metadata\n",
    "    )\n",
    "    print(\"[INGEST] Vectorstore updated. Count:\", store.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c5759",
   "metadata": {},
   "source": [
    "### RAG Retriever Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af50dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.rag_retriever import IntergraxRagRetriever\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "\n",
    "retriever = IntergraxRagRetriever(store, embed_manager, verbose=True)\n",
    "\n",
    "question = \"Czym są wirtualne targi mooff ?\"\n",
    "\n",
    "# MMR\n",
    "hits = retriever.retrieve(\n",
    "    question=question,\n",
    "    top_k=8,\n",
    "    score_threshold=0.15,\n",
    "    where={\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION},\n",
    "    max_per_parent=2,\n",
    "    use_mmr=True,\n",
    "    include_embeddings=True,\n",
    "    prefetch_factor=5\n",
    ")\n",
    "for h in hits:\n",
    "    print(h[\"rank\"], f\"{h['similarity_score']:.3f}\", h[\"metadata\"])\n",
    "    print(h['content'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e51002",
   "metadata": {},
   "source": [
    "# ReRanker Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454afb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.rag_retriever import IntergraxRagRetriever\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import IntergraxReRanker, ReRankerConfig\n",
    "\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "reranker = IntergraxReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "retriever = IntergraxRagRetriever(store, embed_manager, verbose=True)\n",
    "\n",
    "question = \"Czym są wirtualne targi mooff ?\"\n",
    "\n",
    "hits = retriever.retrieve(\n",
    "    question=question,\n",
    "    top_k=8,\n",
    "    score_threshold=0.15,\n",
    "    where={\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION},\n",
    "    max_per_parent=2,\n",
    "    reranker=reranker\n",
    ")\n",
    "for h in hits:\n",
    "    print(h[\"rank\"], f\"{h['similarity_score']:.3f}\", h[\"metadata\"])\n",
    "    print(h['content'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a946d",
   "metadata": {},
   "source": [
    "# RAG with LLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ade7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.rag_retriever import IntergraxRagRetriever\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import IntergraxReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import IntergraxAnswererConfig, IntergraxRagAnswerer\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "reranker = IntergraxReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "retriever = IntergraxRagRetriever(store, embed_manager, verbose=False)\n",
    "\n",
    "\n",
    "# create answerer configuration\n",
    "cfg = IntergraxAnswererConfig(\n",
    "    top_k=10,\n",
    "    min_score=0.15,\n",
    "    re_rank_k=5,\n",
    "    max_context_chars=12000,\n",
    ")\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "cfg.system_context_template = \"Użyj następującego kontekstu aby odpowiedzieć na pytanie użytkownika: {context}\"\n",
    "\n",
    "\n",
    "llm = LLMAdapterRegistry.create(name=\"ollama\", chat= ChatOllama(model=\"llama3.1:latest\"))\n",
    "\n",
    "\n",
    "# create rag answerer (retriever + LLM)\n",
    "answerer = IntergraxRagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,  \n",
    "    config=cfg,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"Jakie unikalne przewagi na tle konkurencji posiada Mooff? Odpowiedź bardzo dokładnie używając nie mniej niż 1000 słów.\",\n",
    "    \"Czym są wirtualne targi na platformie mooff ?\",    \n",
    "]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    # create answer\n",
    "    print(\"QUESTION: \", question)\n",
    "    res = answerer.run(\n",
    "        question=question,\n",
    "        stream=False,\n",
    "        summarize=True,\n",
    "    )\n",
    "\n",
    "    print(\"ANSWER: \", res['answer'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d8672",
   "metadata": {},
   "source": [
    "# min wiring : Dual-Index (TOC + Chunks) + map-reduce + citates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import IntergraxReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import IntergraxAnswererConfig, IntergraxfRagAnswerer\n",
    "from intergrax.rag.rag_answerer import IntergraxAnswererConfig, IntergraxRagAnswerer\n",
    "from intergrax.llm.conversational_memory import IntergraxConversationalMemory\n",
    "from intergrax.rag.dual_index_builder import build_dual_index\n",
    "from intergrax.rag.dual_retriever import IntergraxDualRetriever\n",
    "from intergrax.rag.windowed_answerer import IntergraxWindowedAnswerer\n",
    "\n",
    "from intergrax.rag.documents_loader import IntergraxDocumentsLoader\n",
    "from intergrax.rag.documents_splitter import IntergraxDocumentsSplitter\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1) Vectorstore: CHUNKS + TOC (ta sama ścieżka persist)\n",
    "# -----------------------------------------------------------\n",
    "vs_chunks = IntergraxVectorstoreManager(VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_chunks\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    "))\n",
    "vs_toc = IntergraxVectorstoreManager(VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_toc\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    "))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2) Embedding manager\n",
    "# -----------------------------------------------------------\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3) Ingest dokumentów i budowa dual indexu\n",
    "#    Automatycznie uruchamiany tylko, gdy kolekcje są puste\n",
    "# -----------------------------------------------------------\n",
    "def safe_count(vs_manager):\n",
    "    try:\n",
    "        return int(vs_manager.count() or 0)\n",
    "    except Exception:\n",
    "        return 0  # jeśli nie da się policzyć → traktuj jako 0\n",
    "\n",
    "chunks_count = safe_count(vs_chunks)\n",
    "toc_count    = safe_count(vs_toc)\n",
    "\n",
    "if chunks_count == 0:\n",
    "    print(f\"[INFO] Vectorstore CHUNKS empty (TOC={toc_count}) — performing initial ingest...\")\n",
    "\n",
    "    loader = IntergraxDocumentsLoader(\n",
    "        verbose=True,\n",
    "        docx_mode=\"paragraphs\",\n",
    "        pdf_enable_ocr=True,\n",
    "        image_enable_ocr=True\n",
    "    )\n",
    "    raw_docs = loader.load_documents(\"../documents/mooff-strategy/doc\")\n",
    "\n",
    "    splitter = IntergraxDocumentsSplitter(verbose=True)\n",
    "    docs = splitter.split_documents(raw_docs)\n",
    "    print(f\"Loaded {len(docs)} documents after splitting\")\n",
    "\n",
    "    build_dual_index(\n",
    "        docs=docs,\n",
    "        embed_manager=embed_manager,\n",
    "        vs_chunks=vs_chunks,\n",
    "        vs_toc=vs_toc,\n",
    "        batch_size=512,\n",
    "        verbose=True,\n",
    "    )\n",
    "elif toc_count == 0:\n",
    "    print(\"[WARN] CHUNKS exist but TOC is empty — skipping full ingest to avoid duplicates.\")\n",
    "    print(\"       (Jeśli to niezamierzone, dobuduj TOC osobno lub upewnij się, że loader/splitter oznaczają nagłówki: doc_type='docx', is_heading=True).\")\n",
    "else:\n",
    "    print(f\"[INFO] Vectorstore already populated — CHUNKS={chunks_count}, TOC={toc_count}. Skipping ingest.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) Dual retriever + Reranker\n",
    "# -----------------------------------------------------------\n",
    "retriever = IntergraxDualRetriever(\n",
    "    vs_chunks=vs_chunks,\n",
    "    vs_toc=vs_toc,\n",
    "    embed_manager=embed_manager,\n",
    "    k_chunks=40,\n",
    "    k_toc=10,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "reranker = IntergraxReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) LLM + Answerer (z pamięcią)\n",
    "# -----------------------------------------------------------\n",
    "cfg = IntergraxAnswererConfig(\n",
    "    top_k=20,\n",
    "    min_score=None,\n",
    "    re_rank_k=8,\n",
    "    max_context_chars=12000,\n",
    "    temperature=0.0,\n",
    ")\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "cfg.system_context_template = \"Użyj następującego kontekstu aby odpowiedzieć na pytanie użytkownika: {context}\"\n",
    "\n",
    "llm = LLMAdapterRegistry.create(name=\"ollama\", chat=ChatOllama(model=\"llama3.1:latest\"))\n",
    "\n",
    "memory = IntergraxConversationalMemory()\n",
    "\n",
    "answerer = IntergraxRagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,\n",
    "    config=cfg,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6) Warstwa okienkowa (map→reduce) dla długich odpowiedzi\n",
    "# -----------------------------------------------------------\n",
    "windowed = IntergraxWindowedAnswerer(answerer=answerer, retriever=retriever, verbose=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7) Zapytania\n",
    "#    - długie pytanie → tryb windowed (lepsza spójność przy dużym kontekście)\n",
    "#    - krótsze pytanie → standardowy answerer.ask\n",
    "# -----------------------------------------------------------\n",
    "questions = [\n",
    "    (\"standard\", \"Czym są wirtualne targi na platformie Mooff?\"),\n",
    "    (\"windowed\", \"Jakie unikalne przewagi na tle konkurencji posiada Mooff? Odpowiedź bardzo dokładnie używając nie mniej niż 1000 słów.\"),    \n",
    "]\n",
    "\n",
    "for mode, question in questions:\n",
    "    print(\"\\n==========================\")\n",
    "    print(\"QUESTION (\", mode, \"): \", question)\n",
    "    if mode == \"windowed\":\n",
    "        res = windowed.ask_windowed(\n",
    "            question=question,\n",
    "            top_k_total=60,\n",
    "            window_size=12,\n",
    "            summarize_each=True,\n",
    "        )\n",
    "    else:\n",
    "        res = answerer.run(\n",
    "            question=question,\n",
    "            stream=False,\n",
    "            summarize=True,\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== ANSWER ===\\n\", res[\"answer\"])\n",
    "    if res.get(\"summary\"):\n",
    "        print(\"\\n=== SUMMARY ===\\n\", res[\"summary\"])\n",
    "\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for s in res.get(\"sources\", []):\n",
    "        # s: AnswerSource(source, page, score, preview)\n",
    "        pg = f\"|{s.page}\" if s.page is not None else \"\"\n",
    "        print(f\"- {s.source}{pg}  (score={s.score})\")\n",
    "    print()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8) Test pamięci — pytanie o historię rozmowy\n",
    "# -----------------------------------------------------------\n",
    "res = answerer.run(\n",
    "    question=\"Na podstawie naszej historii rozmowy napisz, czym jest zainteresowany użytkownik w kontekście Mooff.\",\n",
    "    summarize=False,\n",
    "    stream=False\n",
    ")\n",
    "print(\"\\n=== MEMORY TEST ===\\n\", res[\"answer\"])\n",
    "\n",
    "if memory is not None:\n",
    "    print(\"\\n=== MEMORY DUMP ===\")\n",
    "    for m in memory.get_all():\n",
    "        print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a47b70",
   "metadata": {},
   "source": [
    "# LLM + RAG + Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.rag_retriever import IntergraxRagRetriever\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import IntergraxReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import IntergraxAnswererConfig, IntergraxRagAnswerer\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from intergrax.llm.conversational_memory import IntergraxConversationalMemory\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "reranker = IntergraxReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "retriever = IntergraxRagRetriever(store, embed_manager, verbose=False)\n",
    "\n",
    "\n",
    "# create answerer configuration\n",
    "cfg = IntergraxAnswererConfig(\n",
    "    top_k=10,\n",
    "    min_score=0.15,\n",
    "    re_rank_k=5,\n",
    "    max_context_chars=12000,\n",
    ")\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "cfg.system_context_template = \"Użyj następującego kontekstu aby odpowiedzieć na pytanie użytkownika: {context}\"\n",
    "\n",
    "\n",
    "llm = LLMAdapterRegistry.create(name=\"ollama\", chat= ChatOllama(model=\"llama3.1:latest\"))\n",
    "\n",
    "memory = IntergraxConversationalMemory()\n",
    "\n",
    "# create rag answerer (retriever + LLM)\n",
    "answerer = IntergraxRagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,  \n",
    "    config=cfg,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"Jakie unikalne przewagi na tle konkurencji posiada Mooff? Odpowiedź bardzo dokładnie używając nie mniej niż 1000 słów.\",\n",
    "    \"Czym są wirtualne targi na platformie mooff ?\",    \n",
    "]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    # create answer\n",
    "    print(\"QUESTION: \", question)\n",
    "    res = answerer.run(\n",
    "        question=question,\n",
    "        stream=False,\n",
    "        summarize=True,\n",
    "    )\n",
    "\n",
    "    print(\"ANSWER: \", res['answer'])\n",
    "    print()\n",
    "    \n",
    "\n",
    "res = answerer.run(\n",
    "    question=\"Na podstawie historii konwersacji napisz czym zainteresowany jest użytkownik jeżeli chodzi o mooffa ?\",\n",
    "    summarize=False,\n",
    "    stream=False\n",
    ")\n",
    "print(\"MEMORY TEST: \", res['answer'])\n",
    "print()\n",
    "\n",
    "if memory is not None:\n",
    "    print(\"MEMORY:\")\n",
    "    # print(memory.get_all())\n",
    "    for m in memory.get_all():\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8ecc1",
   "metadata": {},
   "source": [
    "# Tool agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab332ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.llm.conversational_memory import IntergraxConversationalMemory, ChatMessage\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from intergrax.tools.tools_agent import IntergraxToolsAgent, ToolsAgentConfig\n",
    "from intergrax.tools.tools_base import ToolRegistry, ToolBase\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "import math\n",
    "\n",
    "# ---------- WEATHER TOOL ----------\n",
    "class WeatherArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"City name, e.g. 'Warsaw'\")\n",
    "\n",
    "class WeatherTool(ToolBase):\n",
    "    name = \"get_weather\"\n",
    "    description = \"Returns current weather for a city. Use only for queries about weather/temperature/conditions in a city.\"\n",
    "    schema_model = WeatherArgs\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        city = kwargs[\"city\"]\n",
    "        # demo output\n",
    "        return {\"city\": city, \"tempC\": 12.3, \"summary\": \"partly cloudy\"}\n",
    "\n",
    "# ---------- CALCULATOR TOOL ----------\n",
    "class CalcArgs(BaseModel):\n",
    "    expression: str = Field(\n",
    "        ...,\n",
    "        description=\"A safe arithmetic expression to evaluate, e.g. '235*17', '2*(3+4)'. No variables.\"\n",
    "    )\n",
    "\n",
    "class CalcTool(ToolBase):\n",
    "    name = \"calc_expression\"\n",
    "    description = \"Safely evaluates a basic arithmetic expression (integers/floats, + - * / parentheses). Use for math/calculation questions.\"\n",
    "    schema_model = CalcArgs\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        expr = kwargs[\"expression\"]\n",
    "        # Minimalny, bezpieczny eval – tylko do znaków matematycznych\n",
    "        allowed = \"0123456789.+-*/() \"\n",
    "        if not all(ch in allowed for ch in expr):\n",
    "            raise ValueError(\"Unsupported characters in expression.\")\n",
    "        # Oblicz\n",
    "        try:\n",
    "            result = eval(expr, {\"__builtins__\": {}}, {})\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Invalid expression: {e}\")\n",
    "        return {\"expression\": expr, \"result\": result}\n",
    "\n",
    "# ---------- AGENT SETUP ----------\n",
    "memory = IntergraxConversationalMemory()\n",
    "tools = ToolRegistry()\n",
    "tools.register(WeatherTool())\n",
    "tools.register(CalcTool())\n",
    "\n",
    "# OLLAMA (planner JSON) – lub odkomentuj OpenAI dla native tools\n",
    "llm = LLMAdapterRegistry.create(name=\"ollama\", chat=ChatOllama(model=\"llama3.1:latest\"))\n",
    "\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# llm = LLMAdapterRegistry.create(name=\"openai\", client=client, model=\"gpt-4o-mini\")\n",
    "\n",
    "agent = IntergraxToolsAgent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    config=ToolsAgentConfig(),\n",
    "    verbose=True,    \n",
    ")\n",
    "\n",
    "\n",
    "# ---------- TEST 1: pogoda -> powinien wybrać get_weather ----------\n",
    "res1 = agent.run(\"Jaka jest pogoda i temperatura w Warszawie?\", context=None)\n",
    "print(\"[ANS 1]\", res1[\"answer\"])\n",
    "print(\"[TOOLS 1]\", res1[\"tool_traces\"])\n",
    "\n",
    "# ---------- TEST 2: obliczenia -> powinien wybrać calc_expression ----------\n",
    "res2 = agent.run(\"Policz 235*17 i podaj sam wynik.\", context=None)\n",
    "print(\"[ANS 2]\", res2[\"answer\"])\n",
    "print(\"[TOOLS 2]\", res2[\"tool_traces\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103ba1f",
   "metadata": {},
   "source": [
    "# Output structure + Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c874f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.llm.conversational_memory import IntergraxConversationalMemory\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from intergrax.tools.tools_agent import IntergraxToolsAgent, ToolsAgentConfig\n",
    "from intergrax.tools.tools_base import ToolRegistry, ToolBase\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ---------- SCHEMA ----------\n",
    "class WeatherAnswer(BaseModel):\n",
    "    \"\"\"Structured representation of a weather query result.\"\"\"\n",
    "    city: str = Field(\n",
    "        ...,\n",
    "        description=\"City name for which the weather information applies. Example: 'Warsaw'.\"\n",
    "    )\n",
    "    tempC: float = Field(\n",
    "        ...,\n",
    "        description=\"Current temperature in degrees Celsius for the specified city.\"\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=\"Short text summary of current weather conditions, e.g. 'partly cloudy', 'sunny', or 'light rain'.\"\n",
    "    )\n",
    "\n",
    "# ---------- TOOL ----------\n",
    "class WeatherArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"City name, e.g. 'Warsaw'\")\n",
    "\n",
    "class WeatherTool(ToolBase):\n",
    "    name = \"get_weather\"\n",
    "    description = \"Returns current weather for a city.\"\n",
    "    schema_model = WeatherArgs\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        city = kwargs[\"city\"]\n",
    "        return {\"city\": city, \"tempC\": 12.3, \"summary\": \"partly cloudy\"}\n",
    "\n",
    "# ---------- SETUP ----------\n",
    "memory = IntergraxConversationalMemory()\n",
    "tools = ToolRegistry()\n",
    "tools.register(WeatherTool())\n",
    "\n",
    "llm = LLMAdapterRegistry.create(name=\"ollama\", chat=ChatOllama(model=\"llama3.1:latest\"))\n",
    "\n",
    "agent = IntergraxToolsAgent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    config=ToolsAgentConfig(),\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# ---------- UŻYCIE ----------\n",
    "res = agent.run(\"Jaka jest pogoda w Warszawie?\", context=None, output_model=WeatherAnswer)\n",
    "\n",
    "print(\"[ANS]\", res[\"answer\"])\n",
    "print(\"[OUTPUT_STRUCTURE]\", res[\"output_structure\"])      # ← obiekt WeatherAnswer\n",
    "print(\"[AS DICT]\", res[\"output_structure\"].model_dump())  # np. {'city': 'Warsaw', 'tempC': 12.3, 'summary': 'partly cloudy'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acfacfc",
   "metadata": {},
   "source": [
    "# Output structure + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.rag_retriever import IntergraxRagRetriever\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import IntergraxReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import IntergraxAnswererConfig, IntergraxRagAnswerer\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from intergrax.llm.conversational_memory import IntergraxConversationalMemory\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- (opcjonalnie) Pydantic dla structured output ---\n",
    "try:\n",
    "    from pydantic import BaseModel, Field\n",
    "except Exception:\n",
    "    class BaseModel: ...\n",
    "    def Field(*a, **k): return None  # fallback\n",
    "\n",
    "# -------------------------\n",
    "# SCHEMA do structured output\n",
    "# -------------------------\n",
    "class ExecSummary(BaseModel):\n",
    "    \"\"\"Zwięzła, ustrukturyzowana odpowiedź RAG (podsumowanie dla kadry).\"\"\"\n",
    "    answer: str = Field(..., description=\"Krótka, klarowna odpowiedź bazująca wyłącznie na kontekście.\")\n",
    "    bullets: list[str] = Field(..., description=\"Lista 3-6 kluczowych punktów streszczenia.\")\n",
    "    citations: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Identyfikatory źródeł użytych w odpowiedzi (np. nazwa pliku lub tytuł).\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# VectorStore & embedding\n",
    "# -------------------------\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "reranker = IntergraxReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "retriever = IntergraxRagRetriever(store, embed_manager, verbose=False)\n",
    "\n",
    "# -------------------------\n",
    "# Answerer config\n",
    "# -------------------------\n",
    "cfg = IntergraxAnswererConfig(\n",
    "    top_k=10,\n",
    "    min_score=0.15,\n",
    "    re_rank_k=5,\n",
    "    max_context_chars=12000,\n",
    ")\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "cfg.system_context_template = \"Użyj następującego kontekstu aby odpowiedzieć na pytanie użytkownika: {context}\"\n",
    "\n",
    "# -------------------------\n",
    "# LLM: OLLAMA\n",
    "# (Adapter powinien wspierać generate_structured → np. przez tryb JSON + walidację)\n",
    "# -------------------------\n",
    "llm = LLMAdapterRegistry.create(name=\"ollama\", chat=ChatOllama(model=\"llama3.1:latest\"))\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# llm = LLMAdapterRegistry.create(name=\"openai\", client=client, model=\"gpt-4o-mini\")\n",
    "\n",
    "memory = IntergraxConversationalMemory()\n",
    "\n",
    "answerer = IntergraxRagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,\n",
    "    config=cfg,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# PYTANIE 1: ustrukturyzowana odpowiedź (output_model=ExecSummary)\n",
    "# -------------------------\n",
    "q1 = \"Jakie unikalne przewagi na tle konkurencji posiada Mooff? Opisz jak najdokładniej. Użyj minimum 1000 słów. Użyj przekazanego modelu output_model do wygenerowania odpowiedzi.\"\n",
    "print(\"QUESTION (structured):\", q1)\n",
    "res1 = answerer.run(\n",
    "    question=q1,\n",
    "    stream=False,\n",
    "    summarize=False,           # summary niepotrzebne – mamy już strukturę\n",
    "    output_model=ExecSummary,  # <<< wymuszenie schematu (zwrócone w 'output_structure')\n",
    ")\n",
    "\n",
    "# answer = tekst, output_structure = instancja ExecSummary lub None\n",
    "print(\"ANSWER (text):\", res1[\"answer\"])\n",
    "print(\"OUTPUT_STRUCTURE:\", res1[\"output_structure\"])\n",
    "print(\"SOURCES:\", [f\"{s.source}|{s.page}\" if s.page else s.source for s in res1[\"sources\"]])\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739bc0ea",
   "metadata": {},
   "source": [
    "# Chat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a816ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.rag_retriever import IntergraxRagRetriever\n",
    "from intergrax.rag.rag_answerer import IntergraxRagAnswerer, IntergraxAnswererConfig\n",
    "from intergrax.llm.conversational_memory import IntergraxConversationalMemory\n",
    "from intergrax.tools.tools_base import ToolRegistry, ToolBase\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, IntergraxVectorstoreManager\n",
    "from intergrax.rag.re_ranker import ReRankerConfig, IntergraxReRanker\n",
    "from intergrax.chat_agent import ChatAgentConfig, RagComponent, IntergraxChatAgent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "\n",
    "llm = LLMAdapterRegistry.create(name=\"ollama\", chat=ChatOllama(model=\"llama3.1:latest\"))\n",
    "memory = IntergraxConversationalMemory()\n",
    "\n",
    "\n",
    "# ---------- WEATHER TOOL ----------\n",
    "class WeatherArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"City name, e.g. 'Warsaw'\")\n",
    "\n",
    "class WeatherTool(ToolBase):\n",
    "    name = \"get_weather\"\n",
    "    description = \"Returns current weather for a city. Use only for queries about weather/temperature/conditions in a city.\"\n",
    "    schema_model = WeatherArgs\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        city = kwargs[\"city\"]\n",
    "        # demo output\n",
    "        return {\"city\": city, \"tempC\": 12.3, \"summary\": \"partly cloudy\"}\n",
    "\n",
    "\n",
    "\n",
    "# Tools (opcjonalnie)\n",
    "tools = ToolRegistry()\n",
    "tools.register(WeatherTool())\n",
    "\n",
    "\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "reranker = IntergraxReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "retriever = IntergraxRagRetriever(store, embed_manager, verbose=False)\n",
    "\n",
    "\n",
    "# create answerer configuration\n",
    "cfg = IntergraxAnswererConfig(\n",
    "    top_k=10,\n",
    "    min_score=0.15,\n",
    "    re_rank_k=5,\n",
    "    max_context_chars=12000,\n",
    ")\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "cfg.system_context_template = \"Użyj następującego kontekstu aby odpowiedzieć na pytanie użytkownika: {context}\"\n",
    "\n",
    "\n",
    "# create rag answerer (retriever + LLM)\n",
    "answerer_docs = IntergraxRagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,  \n",
    "    config=cfg,\n",
    "    memory=None,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "rag_docs = RagComponent(\n",
    "    name=\"intergrax_docs\",\n",
    "    answerer=answerer_docs,\n",
    "    description=(\n",
    "        \"Ten komponent obsługuje wszystkie dane i informacje związane z regulaminami, \"\n",
    "        \"politykami prywatności, zasadami bezpieczeństwa oraz dokumentacją wewnętrzną Mooff. \"\n",
    "        \"Użyj go, gdy trzeba sprawdzić zgodność działania z politykami firmy, znaleźć postanowienia regulaminu \"\n",
    "        \"lub odpowiedzieć na pytania bazujące na oficjalnych dokumentach i politykach.\"\n",
    "    ),\n",
    "    priority=10,\n",
    ")\n",
    "\n",
    "agent = IntergraxChatAgent(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    tools=tools,\n",
    "    rag_components=[rag_docs],\n",
    "    config=ChatAgentConfig(verbose=True),\n",
    ")\n",
    "\n",
    "\n",
    "for question in [\"jaka jest pogoda w Warszawie ?\", \"czym są wirtualne targi mooff - podaj szczegółowe wyjaśnienie.\", \"policz 22 *3 i podaj wynik\"]:    \n",
    "    print(question)\n",
    "    res1 = agent.run(question=question)\n",
    "    print(\"Route: \",res1[\"route\"])\n",
    "    print(\"Rag: \",res1['rag_component'])\n",
    "    print(\"Tool: \",res1['tool_traces'])\n",
    "    print(\"Answer: \",res1['answer'])\n",
    "    print(\"=============================================\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
