{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27157c81",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce05732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc4865",
   "metadata": {},
   "source": [
    "### Load files with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intergrax.rag.documents_loader import DocumentsLoader\n",
    "\n",
    "import intergrax.logging\n",
    "\n",
    "doc_loader = DocumentsLoader(verbose=True, docx_mode=\"paragraphs\")\n",
    "docs = doc_loader.load_documents(\"../documents/mooff-strategy\",)\n",
    "print(f\"Loaded docs: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2211fd",
   "metadata": {},
   "source": [
    "### Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Extend Python's import path to include the parent directory.\n",
    "# This enables direct import of the Intergrax framework modules\n",
    "# without requiring a separate installation.\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.documents_splitter import DocumentsSplitter\n",
    "from intergrax.rag.documents_loader import DocumentsLoader\n",
    "from langchain_core.documents import Document\n",
    "import intergrax.logging\n",
    "\n",
    "# Initialize the document loader.\n",
    "# The 'docx_mode=\"paragraphs\"' option indicates that DOCX files will be parsed\n",
    "# into separated paragraph-level units rather than full-document chunks.\n",
    "doc_loader = DocumentsLoader(verbose=True, docx_mode=\"paragraphs\")\n",
    "\n",
    "# Load all documents from the given directory.\n",
    "# Supported formats depend on the loader configuration\n",
    "# (e.g., PDF, DOCX, TXT, Markdown).\n",
    "docs = doc_loader.load_documents(\"../documents/mooff-strategy\")\n",
    "\n",
    "# Initialize the document splitter.\n",
    "# This component breaks larger documents into smaller segments (chunks),\n",
    "# which improves embedding granularity and retrieval efficiency in RAG pipelines.\n",
    "splitter = DocumentsSplitter(verbose=True)\n",
    "\n",
    "# Perform chunking on the loaded document set.\n",
    "# Splitting boundaries depend on the configured chunk size and rules in the splitter.\n",
    "split_docs = splitter.split_documents(documents=docs)\n",
    "\n",
    "# Print a summary of the transformation process for validation and debugging.\n",
    "print(f\"Loaded docs: {len(docs)} - splitter produced {len(split_docs)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a79eea",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Ensure the parent directory is included in the Python path.\n",
    "# This allows importing the Intergrax framework modules without requiring installation.\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.documents_splitter import DocumentsSplitter\n",
    "from intergrax.rag.documents_loader import DocumentsLoader\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "import intergrax.logging\n",
    "\n",
    "# Initialize the document loader.\n",
    "# The loader is responsible for scanning the directory and normalizing files\n",
    "# into a unified structured format used by the RAG pipeline.\n",
    "doc_loader = DocumentsLoader(verbose=True)\n",
    "\n",
    "# Load raw documents from the specified directory.\n",
    "# Supported formats depend on the loader configuration (PDF, DOCX, TXT, etc.).\n",
    "docs = doc_loader.load_documents(\"../documents/mooff-strategy/doc\")\n",
    "\n",
    "# Initialize the document splitter.\n",
    "# This component divides documents into smaller chunks to ensure optimal\n",
    "# embedding granularity for efficient vector search and context retrieval.\n",
    "splitter = DocumentsSplitter(verbose=True)\n",
    "\n",
    "# Perform the split operation on the loaded documents.\n",
    "# Splitting typically occurs by paragraphs, tokens, or semantic boundaries.\n",
    "split_docs = splitter.split_documents(documents=docs)\n",
    "\n",
    "# Initialize the embedding manager.\n",
    "# This component creates vector embeddings for each document chunk.\n",
    "# Here we specify Ollama as the provider and use a model optimized for embeddings.\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536  # Required if model metadata is not auto-detected.\n",
    ")\n",
    "\n",
    "# Generate embeddings for the document set.\n",
    "# The resulting output consists of embedding vectors and document references.\n",
    "embeddings, documents = embed_manager.embed_documents(docs=docs)\n",
    "\n",
    "# Print summary information for verification and debugging.\n",
    "print(f\"Embedding length: {len(embeddings)} for documents: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad672d",
   "metadata": {},
   "source": [
    "# Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dcf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "from intergrax.rag.documents_splitter import DocumentsSplitter\n",
    "from intergrax.rag.documents_loader import DocumentsLoader\n",
    "from langchain_core.documents import Document\n",
    "from collections import Counter\n",
    "\n",
    "TENANT = \"intergrax\"\n",
    "CORPUS = \"intergrax-strategy\"\n",
    "VERSION = \"v1\"\n",
    "\n",
    "# 0) Initialize the vector store at startup (without loading/splitting/embedding yet).\n",
    "#    This gives us a handle to the underlying database and allows for a light-weight\n",
    "#    presence check before performing any expensive ingestion work.\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "store = VectorstoreManager(config=cfg, verbose=True)\n",
    "\n",
    "# 1) Create an embedding manager used only for a lightweight \"probe\" query\n",
    "#    to check whether the target corpus is already present in the vector store.\n",
    "#    This avoids embedding all chunks if they are already ingested.\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "def corpus_present(store: VectorstoreManager, embed_mgr: EmbeddingManager) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the vector store appears to already contain the target corpus.\n",
    "\n",
    "    Logic:\n",
    "      1. If the global count is zero, we can safely assume the store is empty.\n",
    "      2. Otherwise, embed a simple probe query and perform a filtered search\n",
    "         constrained by tenant, corpus, and version.\n",
    "      3. If at least one matching ID is returned, we treat the corpus as present.\n",
    "    \"\"\"\n",
    "    if store.count() == 0:\n",
    "        return False\n",
    "\n",
    "    # Create a single probe embedding vector.\n",
    "    qvec = embed_mgr.embed_one(\"probe\")\n",
    "    \n",
    "    # Structured filter to ensure we only match documents belonging to the\n",
    "    # current logical corpus (tenant + corpus + version).\n",
    "    where = {\n",
    "        \"$and\": [\n",
    "            {\"tenant\": {\"$eq\": TENANT}},\n",
    "            {\"corpus\": {\"$eq\": CORPUS}},\n",
    "            {\"version\": {\"$eq\": VERSION}},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    res = store.query(query_embeddings=qvec, top_k=1, where=where)\n",
    "    return bool(res[\"ids\"] and res[\"ids\"][0])\n",
    "\n",
    "# Determine whether ingestion is needed based on the above probe.\n",
    "NEED_INGEST = not corpus_present(store, embed_manager)\n",
    "print(f\"Need ingest: {NEED_INGEST}\")\n",
    "\n",
    "if not NEED_INGEST:\n",
    "    # Corpus already present: skip loading, splitting and embedding.\n",
    "    # This prevents unnecessary and expensive re-ingestion work.\n",
    "    print(\"[INGEST] Skipping — corpus already present.\")\n",
    "else:\n",
    "    # 2) Load raw documents from the specified directory.\n",
    "    #    At this stage we only bring documents into memory; no embeddings yet.\n",
    "    doc_loader = DocumentsLoader(verbose=False)\n",
    "    docs = doc_loader.load_documents(\"../documents/mooff-strategy/doc\")\n",
    "\n",
    "    # 3) Split documents into chunks and inject metadata (tenant/corpus/version).\n",
    "    #    The metadata function is called for each chunk and merged into its metadata.\n",
    "    def add_meta(chunk_doc: Document, idx: int, total: int):\n",
    "        # Additional fields can be added here if needed (e.g. language, domain).\n",
    "        return {\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION}\n",
    "\n",
    "    splitter = DocumentsSplitter(verbose=False)\n",
    "    split_docs = splitter.split_documents(documents=docs, call_custom_metadata=add_meta)\n",
    "\n",
    "    # 4) Embed only the chunked documents.\n",
    "    #    This keeps the embedding granularity aligned with retrieval resolution.\n",
    "    embeddings, documents = embed_manager.embed_documents(docs=split_docs)\n",
    "\n",
    "    # 5) Build stable, deterministic IDs for each chunk.\n",
    "    #    Prefer the chunk_id generated by the splitter; if missing, fall back\n",
    "    #    to a deterministic pattern: parent identifier + chunk index.\n",
    "    ids = []\n",
    "    for d in documents:\n",
    "        cid = d.metadata.get(\"chunk_id\")\n",
    "        if not cid:\n",
    "            # Fallback: derive ID from parent document and chunk index.\n",
    "            parent = (\n",
    "                d.metadata.get(\"parent_id\")\n",
    "                or d.metadata.get(\"source_path\")\n",
    "                or d.metadata.get(\"source_name\", \"doc\")\n",
    "            )\n",
    "            idx = int(d.metadata.get(\"chunk_index\", 0))\n",
    "            cid = f\"{parent}#ch{idx:04d}\"\n",
    "        ids.append(cid)\n",
    "\n",
    "    # 6) Ensure all IDs in this batch are unique.\n",
    "    #    Some vector stores (like Chroma) require unique IDs per upsert call.\n",
    "    #    We remove duplicates in-memory before ingestion.\n",
    "    def dedup_batch(ids, docs, embs):\n",
    "        \"\"\"\n",
    "        Remove duplicate IDs within the current batch while preserving order.\n",
    "\n",
    "        Returns:\n",
    "          new_ids, new_docs, new_embs\n",
    "        \"\"\"\n",
    "        seen = set()\n",
    "        new_ids, new_docs, new_embs = [], [], []\n",
    "        for i, _id in enumerate(ids):\n",
    "            if _id in seen:\n",
    "                continue\n",
    "            seen.add(_id)\n",
    "            new_ids.append(_id)\n",
    "            new_docs.append(docs[i])\n",
    "            new_embs.append(embs[i])\n",
    "        return new_ids, new_docs, new_embs\n",
    "\n",
    "    ids, documents, embeddings = dedup_batch(ids, documents, embeddings)\n",
    "\n",
    "    # Optionally, warn if duplicates remain (should normally not happen\n",
    "    # after a correct deduplication step).\n",
    "    c = Counter(ids)\n",
    "    dups = [k for k, v in c.items() if v > 1]\n",
    "    if dups:\n",
    "        print(f\"[WARN] Duplicate IDs after dedup? {len(dups)}\")\n",
    "\n",
    "    # 7) Ingest the chunked documents into the vector store.\n",
    "    #    base_metadata is applied to each record and can be used later for filtering.\n",
    "    base_metadata = {\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION}\n",
    "    store.add_documents(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        batch_size=128,\n",
    "        base_metadata=base_metadata\n",
    "    )\n",
    "    print(\"[INGEST] Vectorstore updated. Count:\", store.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c5759",
   "metadata": {},
   "source": [
    "### RAG Retriever Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af50dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Extend Python path with the parent directory so Intergrax modules\n",
    "# can be imported without installing the package globally.\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.rag_retriever import RagRetriever\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "\n",
    "# Configure access to the underlying vector store (Chroma in this case).\n",
    "# The collection name and persist directory must match the ingestion script.\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "\n",
    "# High-level wrapper around the selected vector store implementation.\n",
    "store = VectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "# Initialize the embedding manager used for turning user questions\n",
    "# into embedding vectors compatible with the stored document embeddings.\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "# Create the RAG retriever that combines:\n",
    "# - the vector store (for similarity search),\n",
    "# - the embedding manager (for encoding queries),\n",
    "# - and retrieval logic (MMR, parent limiting, filters, etc.).\n",
    "retriever = RagRetriever(store, embed_manager, verbose=True)\n",
    "\n",
    "# Example user question.\n",
    "# Note: the question is in Polish; this is fine as long as the embedding model\n",
    "# can handle multilingual input and the corpus is in a compatible language.\n",
    "question = \"Czym są wirtualne targi mooff ?\"\n",
    "\n",
    "# Perform retrieval using Maximal Marginal Relevance (MMR).\n",
    "# Parameters:\n",
    "#   - top_k: maximum number of chunks to return.\n",
    "#   - score_threshold: minimum similarity score to keep a hit.\n",
    "#   - where: metadata filter (tenant/corpus/version must be defined in scope).\n",
    "#   - max_per_parent: limit number of chunks per parent document.\n",
    "#   - use_mmr: enable MMR to reduce redundancy among retrieved chunks.\n",
    "#   - include_embeddings: include stored embeddings in the result if needed downstream.\n",
    "#   - prefetch_factor: how many candidates to consider before MMR filtering.\n",
    "hits = retriever.retrieve(\n",
    "    question=question,\n",
    "    top_k=8,\n",
    "    score_threshold=0.15,\n",
    "    where={\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION},\n",
    "    max_per_parent=2,\n",
    "    use_mmr=True,\n",
    "    include_embeddings=True,\n",
    "    prefetch_factor=5\n",
    ")\n",
    "\n",
    "# Print diagnostic output for each retrieved chunk:\n",
    "# rank, similarity score, metadata and the actual content.\n",
    "for h in hits:\n",
    "    print(h[\"rank\"], f\"{h['similarity_score']:.3f}\", h[\"metadata\"])\n",
    "    print(h[\"content\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e51002",
   "metadata": {},
   "source": [
    "# ReRanker Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454afb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Extend Python path with the parent directory so that Intergrax modules\n",
    "# can be imported directly without installing the package globally.\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.rag_retriever import RagRetriever\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import ReRanker, ReRankerConfig\n",
    "\n",
    "# Configure connection to the vector store (Chroma in this example).\n",
    "# The collection name and persist directory must match what was used during ingestion.\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "\n",
    "# High-level manager for the underlying vector store implementation.\n",
    "store = VectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "# Initialize the embedding manager used to:\n",
    "#  - encode user questions into vectors,\n",
    "#  - support similarity and re-ranking computations.\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "# Configure and create the re-ranker.\n",
    "# This component takes initial retrieval results and refines their ordering\n",
    "# based on combined similarity signals (score fusion).\n",
    "reranker = ReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,  # Combine original similarity with re-ranker scores.\n",
    "        fusion_alpha=0.4,       # Weight for blending scores (0–1; tune per use case).\n",
    "        normalize=\"minmax\",     # Normalization strategy for scores before fusion.\n",
    "        doc_batch_size=256      # Batch size for processing documents during re-ranking.\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create the main RAG retriever, which:\n",
    "#  - queries the vector store,\n",
    "#  - applies metadata filters,\n",
    "#  - optionally delegates to a re-ranker for improved ranking.\n",
    "retriever = RagRetriever(store, embed_manager, verbose=True)\n",
    "\n",
    "# Example user query.\n",
    "# The language of the question should be compatible with the language\n",
    "# of the corpus and supported by the embedding model.\n",
    "question = \"What is the mooff virtual fair?\"\n",
    "\n",
    "# Retrieve documents using the retriever with re-ranking enabled.\n",
    "# Parameters:\n",
    "#   - top_k: maximum number of chunks to return.\n",
    "#   - score_threshold: minimum similarity score required to keep a hit.\n",
    "#   - where: metadata filter; TENANT/CORPUS/VERSION must be defined in scope.\n",
    "#   - max_per_parent: limit the number of chunks per source document.\n",
    "#   - reranker: IntergraxReRanker instance used to refine result ordering.\n",
    "hits = retriever.retrieve(\n",
    "    question=question,\n",
    "    top_k=8,\n",
    "    score_threshold=0.15,\n",
    "    where={\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION},\n",
    "    max_per_parent=2,\n",
    "    reranker=reranker\n",
    ")\n",
    "\n",
    "# Print diagnostic information for each retrieved and re-ranked hit:\n",
    "# rank, final similarity score, metadata and the chunk content.\n",
    "for h in hits:\n",
    "    print(h[\"rank\"], f\"{h['similarity_score']:.3f}\", h[\"metadata\"])\n",
    "    print(h[\"content\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a946d",
   "metadata": {},
   "source": [
    "# RAG with LLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ade7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Extend Python path with the parent directory so that Intergrax modules\n",
    "# can be imported directly without requiring a global package installation.\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.rag_retriever import RagRetriever\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import ReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import AnswererConfig, RagAnswerer\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "\n",
    "# Configure access to the vector store (Chroma in this case).\n",
    "# The collection name and persist directory must match the ingestion step.\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "\n",
    "# High-level manager for the underlying vector store implementation.\n",
    "store = VectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "# Initialize the embedding manager responsible for:\n",
    "#  - encoding user questions,\n",
    "#  - encoding documents during ingestion,\n",
    "#  - supporting re-ranking and retrieval.\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536\n",
    ")\n",
    "\n",
    "# Configure and initialize the re-ranker.\n",
    "# This component refines the initial vector store results by combining\n",
    "# similarity scores and additional re-ranking scores (score fusion).\n",
    "reranker = ReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,   # Blend base similarity with re-ranker scores.\n",
    "        fusion_alpha=0.4,        # Balance between original similarity and re-ranker score.\n",
    "        normalize=\"minmax\",      # Normalization strategy applied to scores before fusion.\n",
    "        doc_batch_size=256       # Batch size for processing documents during re-ranking.\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create the RAG retriever that:\n",
    "#  - performs similarity search in the vector store,\n",
    "#  - applies metadata filters, limits per parent document, etc.\n",
    "retriever = RagRetriever(store, embed_manager, verbose=False)\n",
    "\n",
    "# Create answerer configuration:\n",
    "#  - top_k: maximum number of chunks retrieved from the vector store.\n",
    "#  - min_score: minimum similarity score required to accept a hit.\n",
    "#  - re_rank_k: number of top documents to pass through the re-ranker stage.\n",
    "#  - max_context_chars: maximum size of concatenated context passed to the LLM.\n",
    "cfg = AnswererConfig(\n",
    "    top_k=10,\n",
    "    min_score=0.15,\n",
    "    re_rank_k=5,\n",
    "    max_context_chars=12000,\n",
    ")\n",
    "\n",
    "# System-level instructions used to steer the LLM's behavior in RAG mode.\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "\n",
    "# Template for injecting retrieved context into the final system message.\n",
    "# The placeholder {context} will be replaced with concatenated retrieved chunks.\n",
    "cfg.system_context_template = \"Use the following context to answer the user's question: {context}\"\n",
    "\n",
    "# Create an LLM instance via the Intergrax adapter registry.\n",
    "# Here we use an Ollama-backed ChatOllama model and wrap it in a unified interface.\n",
    "llm = LLMAdapterRegistry.create(\n",
    "    name=\"ollama\",\n",
    "    chat=ChatOllama(model=\"llama3.1:latest\")\n",
    ")\n",
    "\n",
    "# Create the high-level RAG answerer abstraction that:\n",
    "#  - retrieves relevant context (retriever),\n",
    "#  - optionally re-ranks it (reranker),\n",
    "#  - builds prompts and calls the LLM to produce the final answer.\n",
    "answerer = RagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,   # Optional re-ranker injection for higher-quality context.\n",
    "    config=cfg,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Sample user questions to be answered using the configured RAG pipeline.\n",
    "questions = [\n",
    "    \"What unique advantages does Mooff have over its competitors? Answer very carefully, using no less than 1,000 words.\",\n",
    "    \"What are virtual fairs on the mooff platform?\",\n",
    "]\n",
    "\n",
    "# Execute the RAG pipeline for each question and print the answers.\n",
    "for question in questions:\n",
    "    print(\"QUESTION: \", question)\n",
    "\n",
    "    # Run the answerer:\n",
    "    #  - stream=False: collect the full response before printing,\n",
    "    #  - summarize=True: if supported, may produce a concise answer plus reasoning.\n",
    "    res = answerer.run(\n",
    "        question=question,\n",
    "        stream=False,\n",
    "        summarize=True,\n",
    "    )\n",
    "\n",
    "    # Print final answer returned by the pipeline.\n",
    "    print(\"ANSWER: \", res[\"answer\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d8672",
   "metadata": {},
   "source": [
    "# min wiring : Dual-Index (TOC + Chunks) + map-reduce + citates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Extend Python path with the parent directory so that Intergrax modules\n",
    "# can be imported directly without installing the package globally.\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import ReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import AnswererConfig, RagAnswerer\n",
    "from intergrax.memory.conversational_memory import ConversationalMemory\n",
    "from intergrax.rag.dual_index_builder import build_dual_index\n",
    "from intergrax.rag.dual_retriever import DualRetriever\n",
    "from intergrax.rag.windowed_answerer import WindowedAnswerer\n",
    "\n",
    "from intergrax.rag.documents_loader import DocumentsLoader\n",
    "from intergrax.rag.documents_splitter import DocumentsSplitter\n",
    "\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1) Vector stores: CHUNKS + TOC (shared persistence path)\n",
    "#    - CHUNKS: fine-grained content chunks\n",
    "#    - TOC: higher-level structural entries (headings, sections)\n",
    "# -----------------------------------------------------------\n",
    "vs_chunks = VectorstoreManager(VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_chunks\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    "))\n",
    "vs_toc = VectorstoreManager(VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_toc\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    "))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2) Embedding manager\n",
    "#    Shared component for:\n",
    "#      - encoding documents for ingestion,\n",
    "#      - encoding queries for retrieval,\n",
    "#      - supporting re-ranking and dual retrieval logic.\n",
    "# -----------------------------------------------------------\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3) Document ingest and dual index build\n",
    "#    This is executed only when the collections are empty.\n",
    "#    - Loads raw docs\n",
    "#    - Splits them into chunks\n",
    "#    - Builds both CHUNKS and TOC indices in one pass\n",
    "# -----------------------------------------------------------\n",
    "def safe_count(vs_manager: VectorstoreManager) -> int:\n",
    "    \"\"\"\n",
    "    Safely return the number of stored items.\n",
    "\n",
    "    If the underlying store does not support counting or raises an error,\n",
    "    treat it as empty (0) to avoid breaking initialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(vs_manager.count() or 0)\n",
    "    except Exception:\n",
    "        return 0  # if counting fails → treat as empty\n",
    "\n",
    "chunks_count = safe_count(vs_chunks)\n",
    "toc_count = safe_count(vs_toc)\n",
    "\n",
    "if chunks_count == 0:\n",
    "    # No chunks present: perform full initial ingest and build dual index.\n",
    "    print(f\"[INFO] Vectorstore CHUNKS empty (TOC={toc_count}) — performing initial ingest...\")\n",
    "\n",
    "    # Loader configuration:\n",
    "    #  - docx_mode=\"paragraphs\": DOCX files are split into logical paragraph units\n",
    "    #  - pdf_enable_ocr / image_enable_ocr: OCR is used where needed\n",
    "    loader = DocumentsLoader(\n",
    "        verbose=True,\n",
    "        docx_mode=\"paragraphs\",\n",
    "        pdf_enable_ocr=True,\n",
    "        image_enable_ocr=True,\n",
    "    )\n",
    "    raw_docs = loader.load_documents(\"../documents/mooff-strategy/doc\")\n",
    "\n",
    "    # Split raw documents into semantic chunks for RAG.\n",
    "    splitter = DocumentsSplitter(verbose=True)\n",
    "    docs = splitter.split_documents(raw_docs)\n",
    "    print(f\"Loaded {len(docs)} documents after splitting\")\n",
    "\n",
    "    # Build the dual index:\n",
    "    #  - vs_chunks: stores fine-grained chunks\n",
    "    #  - vs_toc: stores higher-level TOC entries for structural guidance\n",
    "    build_dual_index(\n",
    "        docs=docs,\n",
    "        embed_manager=embed_manager,\n",
    "        vs_chunks=vs_chunks,\n",
    "        vs_toc=vs_toc,\n",
    "        batch_size=512,\n",
    "        verbose=True,\n",
    "    )\n",
    "elif toc_count == 0:\n",
    "    # Chunks exist but TOC is missing. We do not rebuild everything automatically\n",
    "    # to avoid duplicated embeddings; this requires a targeted TOC rebuild later.\n",
    "    print(\"[WARN] CHUNKS exist but TOC is empty — skipping full ingest to avoid duplicates.\")\n",
    "    print(\"       (Jeśli to niezamierzone, dobuduj TOC osobno lub upewnij się, że loader/splitter oznaczają nagłówki: doc_type='docx', is_heading=True).\")\n",
    "else:\n",
    "    # Both indices contain data — skip ingest completely.\n",
    "    print(f\"[INFO] Vectorstore already populated — CHUNKS={chunks_count}, TOC={toc_count}. Skipping ingest.\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) Dual retriever + Re-ranker\n",
    "#    Dual retriever:\n",
    "#      - queries both CHUNKS and TOC indices\n",
    "#      - blends structural and content-level evidence\n",
    "# -----------------------------------------------------------\n",
    "retriever = DualRetriever(\n",
    "    vs_chunks=vs_chunks,\n",
    "    vs_toc=vs_toc,\n",
    "    embed_manager=embed_manager,\n",
    "    k_chunks=40,  # how many chunk-level hits to consider\n",
    "    k_toc=10,     # how many TOC-level hits to consider\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Re-ranker configuration:\n",
    "#  - use_score_fusion: combine base similarity and re-ranker scores\n",
    "#  - fusion_alpha: weighting between the two score types\n",
    "#  - normalize: score normalization strategy\n",
    "reranker = ReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256,\n",
    "    ),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) LLM + Answerer (with conversational memory)\n",
    "#    - RAG answerer: retrieval + ranking + prompt construction + LLM call\n",
    "#    - memory: stores dialogue history for follow-up questions\n",
    "# -----------------------------------------------------------\n",
    "cfg = AnswererConfig(\n",
    "    top_k=20,           # max number of chunks fed into the answer\n",
    "    min_score=None,     # no explicit global minimum score\n",
    "    re_rank_k=8,        # number of items going through re-ranking\n",
    "    max_context_chars=12000,  # context budget for the LLM\n",
    "    temperature=0.0,    # deterministic behavior preferred for documentation Q&A\n",
    ")\n",
    "\n",
    "# System instructions controlling RAG behavior (STRICT RAG prompt, etc.).\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "\n",
    "# Template used to inject retrieved context into the system message\n",
    "# before sending the prompt to the LLM.\n",
    "cfg.system_context_template = \"Use the following context to answer the user's question: {context}\"\n",
    "\n",
    "# Create LLM instance via the Intergrax adapter registry (Ollama backend).\n",
    "llm = LLMAdapterRegistry.create(\n",
    "    name=\"ollama\",\n",
    "    chat=ChatOllama(model=\"llama3.1:latest\"),\n",
    ")\n",
    "\n",
    "# Conversational memory to keep track of previous Q&A pairs.\n",
    "memory = ConversationalMemory()\n",
    "\n",
    "# High-level RAG answerer that:\n",
    "#  - runs the dual retriever,\n",
    "#  - applies re-ranking,\n",
    "#  - manages prompts,\n",
    "#  - uses conversational memory,\n",
    "#  - calls the LLM.\n",
    "answerer = RagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,\n",
    "    config=cfg,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6) Windowed layer (map→reduce) for long-form answers\n",
    "#    IntergraxWindowedAnswerer:\n",
    "#      - runs answerer over multiple windows of context\n",
    "#      - summarizes intermediate results\n",
    "#      - produces a coherent global answer for large corpora/questions\n",
    "# -----------------------------------------------------------\n",
    "windowed = WindowedAnswerer(\n",
    "    answerer=answerer,\n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7) Queries:\n",
    "#    - \"standard\": classic single-shot RAG answer\n",
    "#    - \"windowed\": multi-window map→reduce for very detailed answers\n",
    "# -----------------------------------------------------------\n",
    "questions = [\n",
    "    (\"standard\", \"What are virtual fairs on the Mooff platform?\"),\n",
    "    (\"windowed\", \"What unique advantages does Mooff have over its competitors? Answer very carefully, using no less than 1,000 words.\"),\n",
    "]\n",
    "\n",
    "for mode, question in questions:\n",
    "    print(\"\\n==========================\")\n",
    "    print(\"QUESTION (\", mode, \"): \", question)\n",
    "\n",
    "    if mode == \"windowed\":\n",
    "        # Windowed mode:\n",
    "        #   - top_k_total: total number of docs collected across all windows\n",
    "        #   - window_size: number of docs per window\n",
    "        #   - summarize_each: whether to summarize each window separately\n",
    "        res = windowed.ask_windowed(\n",
    "            question=question,\n",
    "            top_k_total=60,\n",
    "            window_size=12,\n",
    "            summarize_each=True,\n",
    "        )\n",
    "    else:\n",
    "        # Standard mode: single-shot answer using the answerer.\n",
    "        res = answerer.run(\n",
    "            question=question,\n",
    "            stream=False,\n",
    "            summarize=True,\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== ANSWER ===\\n\", res[\"answer\"])\n",
    "    if res.get(\"summary\"):\n",
    "        print(\"\\n=== SUMMARY ===\\n\", res[\"summary\"])\n",
    "\n",
    "    # Print structured list of sources used in the answer (if provided).\n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for s in res.get(\"sources\", []):\n",
    "        # s is expected to be an AnswerSource-like object with:\n",
    "        #   source, page, score, preview\n",
    "        pg = f\"|{s.page}\" if s.page is not None else \"\"\n",
    "        print(f\"- {s.source}{pg}  (score={s.score})\")\n",
    "    print()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8) Memory test — ask a question about the conversation history\n",
    "#    This checks whether IntergraxConversationalMemory is correctly\n",
    "#    accumulating interaction data.\n",
    "# -----------------------------------------------------------\n",
    "res = answerer.run(\n",
    "    question=\"Based on our conversation history, write what the user is interested in in the context of Mooff.\",\n",
    "    summarize=False,\n",
    "    stream=False,\n",
    ")\n",
    "print(\"\\n=== MEMORY TEST ===\\n\", res[\"answer\"])\n",
    "\n",
    "# Optional: inspect raw memory entries for debugging.\n",
    "if memory is not None:\n",
    "    print(\"\\n=== MEMORY DUMP ===\")\n",
    "    for m in memory.get_all():\n",
    "        print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a47b70",
   "metadata": {},
   "source": [
    "# LLM + RAG + Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Extend Python path with the parent directory so that Intergrax modules\n",
    "# can be imported directly without installing the package globally.\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.rag_retriever import RagRetriever\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import ReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import AnswererConfig, RagAnswerer\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from intergrax.memory.conversational_memory import ConversationalMemory\n",
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1) Vector store configuration\n",
    "#    - Uses Chroma as the backend\n",
    "#    - 'intergrax_docs' must match the collection name used during ingestion\n",
    "# -----------------------------------------------------------\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "\n",
    "# High-level manager for the underlying vector store implementation.\n",
    "store = VectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2) Embedding manager\n",
    "#    - Shared component for encoding both documents and user questions\n",
    "# -----------------------------------------------------------\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3) Re-ranker\n",
    "#    - Refines the initial similarity-based ranking from the vector store\n",
    "#      using score fusion and optional normalization\n",
    "# -----------------------------------------------------------\n",
    "reranker = ReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,  # Combine base similarity and re-ranker scores.\n",
    "        fusion_alpha=0.4,       # Weight for blending original and re-ranker scores.\n",
    "        normalize=\"minmax\",     # Normalize scores into a comparable range.\n",
    "        doc_batch_size=256,     # Batch size for re-ranking operations.\n",
    "    ),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) Retriever\n",
    "#    - Responsible for:\n",
    "#      * similarity search in the vector store,\n",
    "#      * applying metadata filters (if provided),\n",
    "#      * returning ranked context chunks\n",
    "# -----------------------------------------------------------\n",
    "retriever = RagRetriever(store, embed_manager, verbose=False)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) Answerer configuration\n",
    "#    - Controls how many documents are retrieved and re-ranked\n",
    "#    - Limits the maximum context size passed to the LLM\n",
    "# -----------------------------------------------------------\n",
    "cfg = AnswererConfig(\n",
    "    top_k=10,             # Maximum number of chunks retrieved from the store.\n",
    "    min_score=0.15,       # Minimum similarity threshold for accepting a hit.\n",
    "    re_rank_k=5,          # Number of top candidates to pass through the re-ranker.\n",
    "    max_context_chars=12000,  # Context budget (characters) fed into the LLM.\n",
    ")\n",
    "\n",
    "# System-level instructions for STRICT RAG behavior.\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "\n",
    "# Template for injecting the retrieved context into the system message.\n",
    "cfg.system_context_template = (\n",
    "    \"Use the following context to answer the user's question: {context}\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6) LLM + Conversational Memory\n",
    "#    - LLM is created via the Intergrax adapter registry (Ollama backend)\n",
    "#    - Memory stores conversation history for follow-up questions\n",
    "# -----------------------------------------------------------\n",
    "llm = LLMAdapterRegistry.create(\n",
    "    name=\"ollama\",\n",
    "    chat=ChatOllama(model=\"llama3.1:latest\"),\n",
    ")\n",
    "\n",
    "memory = ConversationalMemory()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7) High-level RAG answerer\n",
    "#    - Orchestrates retrieval, re-ranking, prompt construction and LLM calls\n",
    "#    - Uses memory to keep context across multiple questions\n",
    "# -----------------------------------------------------------\n",
    "answerer = RagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,\n",
    "    config=cfg,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8) Example questions\n",
    "#    - First: long, detailed strategic question about Mooff\n",
    "#    - Second: focused feature question about virtual fairs\n",
    "# -----------------------------------------------------------\n",
    "questions = [\n",
    "    \"What unique advantages does Mooff have over its competitors? Answer very carefully, using no less than 1,000 words.\",\n",
    "    \"What are virtual fairs on the mooff platform?\",\n",
    "]\n",
    "\n",
    "# Execute the RAG pipeline for each question.\n",
    "for question in questions:\n",
    "    print(\"QUESTION: \", question)\n",
    "\n",
    "    # Run the answerer:\n",
    "    #   - stream=False: gather full response before printing\n",
    "    #   - summarize=True: may trigger a summarized form if supported by the pipeline\n",
    "    res = answerer.run(\n",
    "        question=question,\n",
    "        stream=False,\n",
    "        summarize=True,\n",
    "    )\n",
    "\n",
    "    print(\"ANSWER: \", res[\"answer\"])\n",
    "    print()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 9) Memory test\n",
    "#    - Ask a meta-question that should be answered based on conversation history\n",
    "#    - Validates that IntergraxConversationalMemory is working as intended\n",
    "# -----------------------------------------------------------\n",
    "res = answerer.run(\n",
    "    question=\"Based on the conversation history, what is the user interested in when it comes to mooff?\",\n",
    "    summarize=False,\n",
    "    stream=False,\n",
    ")\n",
    "print(\"MEMORY TEST: \", res[\"answer\"])\n",
    "print()\n",
    "\n",
    "# Optional: inspect raw memory content for debugging/validation.\n",
    "if memory is not None:\n",
    "    print(\"MEMORY:\")\n",
    "    for m in memory.get_all():\n",
    "        print(m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
