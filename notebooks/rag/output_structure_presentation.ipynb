{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8af43e8",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e0a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANS] The current weather in Warsaw is partly cloudy with a temperature of 12.3 degrees Celsius.\n",
      "[OUTPUT_STRUCTURE] city='Warsaw' tempC=12.3 summary='partly cloudy'\n",
      "[AS DICT] {'city': 'Warsaw', 'tempC': 12.3, 'summary': 'partly cloudy'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "from intergrax.globals.settings import GLOBAL_SETTINGS\n",
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.llm_adapters.llm_usage_track import LLMUsageTracker\n",
    "from intergrax.memory.conversational_memory import ConversationalMemory\n",
    "from intergrax.tools.tools_agent import ToolsAgent, ToolsAgentConfig\n",
    "from intergrax.tools.tools_base import ToolRegistry, ToolBase\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ---------- SCHEMA ----------\n",
    "class WeatherAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured representation of a weather query result.\n",
    "\n",
    "    This model is used as `output_model` for the tools agent, allowing the LLM\n",
    "    to return a validated, typed object instead of a free-form string.\n",
    "    \"\"\"\n",
    "    city: str = Field(\n",
    "        ...,\n",
    "        description=\"City name for which the weather information applies. Example: 'Warsaw'.\",\n",
    "    )\n",
    "    tempC: float = Field(\n",
    "        ...,\n",
    "        description=\"Current temperature in degrees Celsius for the specified city.\",\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Short text summary of current weather conditions, \"\n",
    "            \"e.g. 'partly cloudy', 'sunny', or 'light rain'.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# ---------- TOOL ----------\n",
    "class WeatherArgs(BaseModel):\n",
    "    \"\"\"\n",
    "    Input schema for the WeatherTool.\n",
    "\n",
    "    Fields:\n",
    "      - city: name of the city to query the weather for.\n",
    "    \"\"\"\n",
    "    city: str = Field(..., description=\"City name, e.g. 'Warsaw'\")\n",
    "\n",
    "\n",
    "class WeatherTool(ToolBase):\n",
    "    \"\"\"\n",
    "    Simple weather tool returning demo data.\n",
    "\n",
    "    Responsibilities:\n",
    "      - Accept a city name via validated arguments (WeatherArgs).\n",
    "      - Return a dictionary compatible with the WeatherAnswer schema.\n",
    "\n",
    "    NOTE:\n",
    "      This is a stub implementation. In production, integrate with a real\n",
    "      weather API (e.g., OpenWeatherMap) and map API responses to the schema.\n",
    "    \"\"\"\n",
    "    name = \"get_weather\"\n",
    "    description = \"Returns current weather for a city.\"\n",
    "    schema_model = WeatherArgs\n",
    "\n",
    "    def run(self, \n",
    "            run_id:Optional[str] = None,\n",
    "            llm_usage_tracker: Optional[LLMUsageTracker] = None,\n",
    "            **kwargs) -> Any:\n",
    "        city = kwargs[\"city\"]\n",
    "        # Demo/static result, for testing the tools agent + output_model wiring.\n",
    "        return {\"city\": city, \"tempC\": 12.3, \"summary\": \"partly cloudy\"}\n",
    "\n",
    "\n",
    "# ---------- SETUP ----------\n",
    "# Shared conversational memory for multi-turn interactions.\n",
    "memory = ConversationalMemory()\n",
    "\n",
    "# Tool registry that contains all callable tools available to the agent.\n",
    "tools = ToolRegistry()\n",
    "tools.register(WeatherTool())\n",
    "\n",
    "# LLM used as planner/controller for tools and as a natural language generator.\n",
    "# Here we use an Ollama-backed model exposed via LangChain's ChatOllama.\n",
    "llm = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "# Tools agent orchestrating:\n",
    "#   - LLM reasoning,\n",
    "#   - automatic tool selection and invocation,\n",
    "#   - mapping tool outputs into structured responses,\n",
    "#   - optional use of conversational memory.\n",
    "agent = ToolsAgent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    config=ToolsAgentConfig(),    \n",
    ")\n",
    "\n",
    "# ---------- USAGE ----------\n",
    "# Ask a natural-language question and request a structured result\n",
    "# using the WeatherAnswer Pydantic model as `output_model`.\n",
    "res = agent.run(\n",
    "    \"What's the weather like in Warsaw?\",\n",
    "    context=None,\n",
    "    output_model=WeatherAnswer,\n",
    ")\n",
    "\n",
    "# Human-readable answer produced by the LLM (string).\n",
    "print(\"[ANS]\", res[\"answer\"])\n",
    "\n",
    "# Parsed, validated Pydantic object of type WeatherAnswer.\n",
    "print(\"[OUTPUT_STRUCTURE]\", res[\"output_structure\"])\n",
    "\n",
    "# The same object converted to a plain dictionary.\n",
    "# Example: {'city': 'Warsaw', 'tempC': 12.3, 'summary': 'partly cloudy'}\n",
    "print(\"[AS DICT]\", res[\"output_structure\"].model_dump())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67008ae",
   "metadata": {},
   "source": [
    "# Output structure + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa618973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "QUESTION (structured): What unique advantages does Mooff have over its competitors? Describe in as much detail as possible. Use a minimum of 1,000 words. Use the passed output_model to generate the response.\n",
      "ANSWER (text): No sufficiently relevant context fragments were found to answer the question.\n",
      "OUTPUT_STRUCTURE: None\n",
      "SOURCES: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.rag_prompts import default_rag_system_instruction\n",
    "from intergrax.rag.rag_retriever import RagRetriever\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "from intergrax.rag.re_ranker import ReRanker, ReRankerConfig\n",
    "from intergrax.rag.rag_answerer import AnswererConfig, RagAnswerer\n",
    "from intergrax.memory.conversational_memory import ConversationalMemory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (useful when the LLM adapter or vector store\n",
    "# expects configuration via environment, e.g., API keys, model names).\n",
    "load_dotenv()\n",
    "\n",
    "# --- (optional) Pydantic for structured output ---\n",
    "# If Pydantic is not available, provide a minimal fallback so the script\n",
    "# can still run (though structured output will be degraded).\n",
    "try:\n",
    "    from pydantic import BaseModel, Field\n",
    "except Exception:\n",
    "    class BaseModel:\n",
    "        \"\"\"Fallback BaseModel stub when Pydantic is not installed.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def Field(*a, **k):\n",
    "        \"\"\"Fallback Field stub doing nothing when Pydantic is not installed.\"\"\"\n",
    "        return None  # no-op placeholder\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SCHEMA for structured output\n",
    "# -------------------------\n",
    "class ExecSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    Compact, structured RAG response tailored for executives.\n",
    "\n",
    "    Fields:\n",
    "      - answer: short, clear summary based strictly on retrieved context.\n",
    "      - bullets: 3-6 key bullet points highlighting the most important aspects.\n",
    "      - citations: free-form identifiers of the sources used (e.g., filenames).\n",
    "    \"\"\"\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A short, clear answer based solely on context.\"\n",
    "    )\n",
    "    bullets: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"List 3-6 key points of the summary.\"\n",
    "    )\n",
    "    citations: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Identifiers of sources used in the response (e.g. file name or title).\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# VectorStore & embedding\n",
    "# -------------------------\n",
    "# Configure the vector store backend (Chroma) and collection to use.\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "\n",
    "# High-level vector store manager wrapping the underlying implementation.\n",
    "store = VectorstoreManager(config=cfg)\n",
    "\n",
    "# Embedding manager shared across:\n",
    "#   - document ingestion,\n",
    "#   - query encoding,\n",
    "#   - re-ranking logic.\n",
    "embed_manager = EmbeddingManager(    \n",
    "    provider=\"ollama\",\n",
    "    model_name=GLOBAL_SETTINGS.default_ollama_embed_model,\n",
    "    assume_ollama_dim=1536,\n",
    ")\n",
    "\n",
    "# Re-ranker that refines initial similarity scores from the vector store.\n",
    "# Score fusion combines base similarity with re-ranker scores for better ranking.\n",
    "reranker = ReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256,\n",
    "    ),    \n",
    ")\n",
    "\n",
    "# Retriever responsible for:\n",
    "#   - querying the vector store,\n",
    "#   - returning an ordered list of relevant chunks.\n",
    "retriever = RagRetriever(store, embed_manager)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Answerer config\n",
    "# -------------------------\n",
    "# Configuration for the RAG answerer:\n",
    "#   - top_k: maximum number of chunks retrieved.\n",
    "#   - min_score: minimum similarity threshold for keeping a chunk.\n",
    "#   - re_rank_k: how many chunks pass through the re-ranker.\n",
    "#   - max_context_chars: maximum concatenated context passed to the LLM.\n",
    "cfg = AnswererConfig(\n",
    "    top_k=10,\n",
    "    min_score=0.15,\n",
    "    re_rank_k=5,\n",
    "    max_context_chars=12000,\n",
    ")\n",
    "\n",
    "# System-level instructions controlling STRICT RAG behavior.\n",
    "cfg.system_instructions = default_rag_system_instruction()\n",
    "\n",
    "# Template used to inject retrieved context into the system message for the LLM.\n",
    "cfg.system_context_template = (\n",
    "    \"Use the following context to answer the user's question: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LLM: OLLAMA\n",
    "# (Adapter should support generate_structured, e.g., via JSON mode + validation)\n",
    "# -------------------------\n",
    "# Create the LLM via the Intergrax adapter registry; here using Ollama backend.\n",
    "llm = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "\n",
    "# Conversational memory for multi-turn interactions and follow-up questions.\n",
    "memory = ConversationalMemory()\n",
    "\n",
    "# High-level RAG answerer:\n",
    "#   - runs retrieval + re-ranking,\n",
    "#   - builds prompts,\n",
    "#   - optionally performs structured generation when output_model is provided.\n",
    "answerer = RagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,\n",
    "    config=cfg,\n",
    "    memory=memory,    \n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# QUESTION 1: structured response (output_model=ExecSummary)\n",
    "# -------------------------\n",
    "q1 = (\n",
    "    \"What unique advantages does Mooff have over its competitors? \"\n",
    "    \"Describe in as much detail as possible. Use a minimum of 1,000 words. \"\n",
    "    \"Use the passed output_model to generate the response.\"\n",
    ")\n",
    "\n",
    "print(\"QUESTION (structured):\", q1)\n",
    "\n",
    "# Run the RAG pipeline with a structured output model.\n",
    "#   - summarize=False: we rely on ExecSummary instead of a generic summary.\n",
    "#   - output_model=ExecSummary: forces the answerer to construct a typed object.\n",
    "res1 = answerer.run(\n",
    "    question=q1,\n",
    "    stream=False,\n",
    "    summarize=False,           # no generic summary; we have structured schema\n",
    "    output_model=ExecSummary,  # returned under 'output_structure'\n",
    ")\n",
    "\n",
    "# res1[\"answer\"]: free-form textual answer (if the pipeline still provides it).\n",
    "print(\"ANSWER (text):\", res1[\"answer\"])\n",
    "\n",
    "# res1[\"output_structure\"]: instance of ExecSummary or None if parsing failed.\n",
    "print(\"OUTPUT_STRUCTURE:\", res1[\"output_structure\"])\n",
    "\n",
    "# Print compact list of sources in \"source|page\" or \"source\" form.\n",
    "print(\n",
    "    \"SOURCES:\",\n",
    "    [\n",
    "        f\"{s.source}|{s.page}\" if s.page else s.source\n",
    "        for s in res1[\"sources\"]\n",
    "    ],\n",
    ")\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
