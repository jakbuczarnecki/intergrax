{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed35e26",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2180ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import intergrax.system_prompts as prompts\n",
    "from intergrax.llm_adapters import LLMAdapterRegistry\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.rag_retriever import IntergraxRagRetriever\n",
    "from intergrax.rag.rag_answerer import IntergraxRagAnswerer, IntergraxAnswererConfig\n",
    "from intergrax.llm.conversational_memory import IntergraxConversationalMemory\n",
    "from intergrax.tools.tools_base import ToolRegistry, ToolBase\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, IntergraxVectorstoreManager\n",
    "from intergrax.rag.re_ranker import ReRankerConfig, IntergraxReRanker\n",
    "from intergrax.chat_agent import ChatAgentConfig, RagComponent, IntergraxChatAgent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1) LLM + conversational memory\n",
    "# -----------------------------------------------------------\n",
    "llm = LLMAdapterRegistry.create(\n",
    "    name=\"ollama\",\n",
    "    chat=ChatOllama(model=\"llama3.1:latest\"),\n",
    ")\n",
    "memory = IntergraxConversationalMemory()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2) Example Tool: Weather\n",
    "# -----------------------------------------------------------\n",
    "class WeatherArgs(BaseModel):\n",
    "    \"\"\"Arguments required to call the weather tool.\"\"\"\n",
    "    city: str = Field(..., description=\"City name. Example: 'Warsaw'.\")\n",
    "\n",
    "\n",
    "class WeatherTool(ToolBase):\n",
    "    \"\"\"\n",
    "    Demo weather tool that simulates a weather API response.\n",
    "\n",
    "    Used by the agent when the user requests weather information.\n",
    "    \"\"\"\n",
    "    name = \"get_weather\"\n",
    "    description = (\n",
    "        \"Returns the current weather for a given city. Use only for queries \"\n",
    "        \"related to weather, temperature or atmospheric conditions.\"\n",
    "    )\n",
    "    schema_model = WeatherArgs\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        city = kwargs[\"city\"]\n",
    "        # Static demo response.\n",
    "        return {\"city\": city, \"tempC\": 12.3, \"summary\": \"Partly cloudy\"}\n",
    "\n",
    "\n",
    "# Register available tools\n",
    "tools = ToolRegistry()\n",
    "tools.register(WeatherTool())\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3) Vector store & RAG configuration\n",
    "# -----------------------------------------------------------\n",
    "cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs\",\n",
    "    chroma_persist_directory=\"chroma_db/intergrax_docs_v1\",\n",
    ")\n",
    "\n",
    "store = IntergraxVectorstoreManager(config=cfg, verbose=False)\n",
    "\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=False,\n",
    "    provider=\"ollama\",\n",
    "    model_name=\"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\",\n",
    "    assume_ollama_dim=1536,\n",
    ")\n",
    "\n",
    "reranker = IntergraxReRanker(\n",
    "    embedding_manager=embed_manager,\n",
    "    config=ReRankerConfig(\n",
    "        use_score_fusion=True,\n",
    "        fusion_alpha=0.4,\n",
    "        normalize=\"minmax\",\n",
    "        doc_batch_size=256,\n",
    "    ),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "retriever = IntergraxRagRetriever(store, embed_manager, verbose=False)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) RAG Answerer configuration\n",
    "# -----------------------------------------------------------\n",
    "cfg = IntergraxAnswererConfig(\n",
    "    top_k=10,\n",
    "    min_score=0.15,\n",
    "    re_rank_k=5,\n",
    "    max_context_chars=12000,\n",
    ")\n",
    "\n",
    "cfg.system_instructions = prompts.default_rag_system_instruction()\n",
    "cfg.system_context_template = (\n",
    "    \"Use the following retrieved context to answer the user question: {context}\"\n",
    ")\n",
    "\n",
    "answerer_docs = IntergraxRagAnswerer(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    reranker=reranker,\n",
    "    config=cfg,\n",
    "    memory=None,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# RAG routing logic description\n",
    "rag_docs = RagComponent(\n",
    "    name=\"intergrax_docs\",\n",
    "    answerer=answerer_docs,\n",
    "    description=(\n",
    "        \"This component responds to questions related to Mooff regulations, \"\n",
    "        \"privacy policies, internal documentation, and compliance rules. \"\n",
    "        \"Use this component for legally-bound questions or factual document-based queries.\"\n",
    "    ),\n",
    "    priority=10,\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) High-level hybrid agent (RAG + tools + LLM chat)\n",
    "# -----------------------------------------------------------\n",
    "agent = IntergraxChatAgent(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    tools=tools,\n",
    "    rag_components=[rag_docs],\n",
    "    config=ChatAgentConfig(verbose=True),\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6) Test Questions\n",
    "# -----------------------------------------------------------\n",
    "for question in [\n",
    "    \"What is the weather in Warsaw?\",\n",
    "    \"Explain in detail what Mooff virtual fairs are.\",\n",
    "    \"Calculate 22 * 3 and provide just the result.\",\n",
    "]:\n",
    "    print(question)\n",
    "    \n",
    "    response = agent.run(question=question)\n",
    "    \n",
    "    print(\"Routing Decision: \", response[\"route\"])\n",
    "    print(\"RAG Component: \", response[\"rag_component\"])\n",
    "    print(\"Tool Triggered: \", response[\"tool_traces\"])\n",
    "    print(\"Final Answer: \", response[\"answer\"])\n",
    "    print(\"=============================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
