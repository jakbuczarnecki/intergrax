{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee866560",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ce59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87f93d",
   "metadata": {},
   "source": [
    "# Hybrid Multi-Source RAG with Intergrax + LangGraph\n",
    "\n",
    "This notebook demonstrates a **practical, end-to-end RAG workflow** that combines multiple knowledge sources into a single in-memory vector index and exposes it through a LangGraph-based agent.\n",
    "\n",
    "We will use **Intergrax** components together with **LangGraph** to:\n",
    "\n",
    "1. **Ingest content from multiple sources:**\n",
    "   - Local PDF files (from a given directory),\n",
    "   - Local DOCX/Word files (from a given directory),\n",
    "   - Live web results using the Intergrax `WebSearchExecutor`.\n",
    "\n",
    "2. **Build a unified RAG corpus:**\n",
    "   - Normalize all documents into a common internal format,\n",
    "   - (Optionally) attach basic metadata about origin (pdf / docx / web),\n",
    "   - Split documents into chunks suitable for embedding.\n",
    "\n",
    "3. **Create an in-memory vector index:**\n",
    "   - Use an Intergrax embedding manager (e.g. OpenAI / Ollama),\n",
    "   - Store embeddings in an **in-memory Chroma** collection via Intergrax vectorstore manager,\n",
    "   - Keep everything ephemeral (no persistence, perfect for “ad-hoc research” scenarios).\n",
    "\n",
    "4. **Answer user questions with a RAG agent:**\n",
    "   - The user provides a natural language question,\n",
    "   - LangGraph orchestrates the flow: load → merge → index → retrieve → answer,\n",
    "   - An Intergrax `RagAnswerer` (or `WindowedAnswerer`) generates a **single, structured report**:\n",
    "     - short summary of the relevant information,\n",
    "     - key insights and conclusions,\n",
    "     - optionally: recommendations / action items.\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook showcases\n",
    "\n",
    "- How to combine **local files + web search** in a single RAG pipeline.\n",
    "- How to plug Intergrax components (loaders, splitter, embeddings, vectorstore, RAG answerer, websearch) into a **LangGraph `StateGraph`**.\n",
    "- How to build a **temporary, in-memory knowledge graph** for one-off research tasks (no database setup required).\n",
    "- A clean, production-oriented pattern that can be reused in:\n",
    "  - internal knowledge explorers,\n",
    "  - “research bot” agents,\n",
    "  - prototype assistants for teams or clients.\n",
    "\n",
    "All code and comments in this notebook are in **English** to keep it ready for public documentation, GitHub examples, and international collaborators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59eae8",
   "metadata": {},
   "source": [
    "## 1. Environment and configuration\n",
    "\n",
    "In this section we prepare the environment for the hybrid multi-source RAG agent.\n",
    "\n",
    "The goals of this step:\n",
    "\n",
    "- Load all required configuration values (API keys, base paths, model names) from environment variables or a `.env` file.\n",
    "- Import the core building blocks from:\n",
    "  - the Intergrax framework (LLM adapter, embedding manager, vectorstore manager, document loaders, RAG answerer, websearch executor),\n",
    "  - LangGraph (for defining and running the `StateGraph`),\n",
    "  - Standard Python modules (`os`, `pathlib`, `typing`, etc.).\n",
    "- Define the base directories for local documents:\n",
    "  - one directory for PDF files (e.g. `./data/pdf`),\n",
    "  - one directory for DOCX files (e.g. `./data/docx`).\n",
    "- Decide on the core RAG parameters:\n",
    "  - chunk size and overlap for splitting documents,\n",
    "  - embedding model name,\n",
    "  - LLM model name used by the answerer,\n",
    "  - number of retrieved documents (`top_k`) during similarity search.\n",
    "- Initialize the core Intergrax components that will be reused across the notebook:\n",
    "  - an embedding manager (e.g. OpenAI-based or Ollama-based),\n",
    "  - a vectorstore manager backed by an **in-memory** Chroma collection (no persistence),\n",
    "  - an LLM adapter used by the RAG answerer,\n",
    "  - a simple text splitter for turning documents into chunks.\n",
    "\n",
    "At the end of this section we want to have a small configuration block that:\n",
    "\n",
    "1. Reads configuration (API keys, paths, model names),\n",
    "2. Instantiates the main Intergrax services (embeddings, vectorstore, LLM adapter, text splitter),\n",
    "3. Is easy to adjust for different environments (OpenAI vs local models, different directories, different Chroma settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740e235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 16:28:52,641 [INFO] [intergraxEmbeddingManager] Loading model 'rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest' (provider=ollama)\n",
      "2025-11-18 16:28:53,175 [INFO] HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-18 16:28:53,177 [INFO] [intergraxEmbeddingManager] Loaded. Embedding dim = 1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=hybrid_multi_source_rag\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "Environment initialized.\n",
      "TENANT=intergrax, CORPUS=hybrid-multi-source, VERSION=v1\n",
      "PDF_DIR=..\\documents\\hybrid-corpus\\pdf\n",
      "DOCX_DIR=..\\documents\\hybrid-corpus\\docx\n",
      "Vectorstore collection=hybrid_multi_source_rag, persist=None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from intergrax.rag.documents_loader import IntergraxDocumentsLoader\n",
    "from intergrax.rag.documents_splitter import IntergraxDocumentsSplitter\n",
    "from intergrax.rag.embedding_manager import IntergraxEmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import IntergraxVectorstoreManager, VSConfig\n",
    "\n",
    "import intergrax.logging  # initializes logging format/levels for the framework\n",
    "\n",
    "# ---- Tenant / corpus configuration (for metadata + filtering) ----\n",
    "TENANT = \"intergrax\"\n",
    "CORPUS = \"hybrid-multi-source\"\n",
    "VERSION = \"v1\"\n",
    "\n",
    "# ---- Base directories for local documents ----\n",
    "# You can adjust these to your actual layout.\n",
    "# For the hybrid demo we assume:\n",
    "#   ../documents/hybrid-corpus/pdf\n",
    "#   ../documents/hybrid-corpus/docx\n",
    "BASE_DOCS_DIR = Path(\"../documents/hybrid-corpus\")\n",
    "PDF_DIR = BASE_DOCS_DIR / \"pdf\"\n",
    "DOCX_DIR = BASE_DOCS_DIR / \"docx\"\n",
    "\n",
    "# ---- Core RAG parameters ----\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 150\n",
    "TOP_K = 8\n",
    "\n",
    "# Embedding model configuration (using your existing Ollama-based setup)\n",
    "EMBED_PROVIDER = \"ollama\"\n",
    "EMBED_MODEL_NAME = \"rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest\"\n",
    "EMBED_DIM = 1536  # assumed dimension for this model\n",
    "\n",
    "# Vectorstore configuration\n",
    "# For *ephemeral* usage you can set `chroma_persist_directory=None`\n",
    "# or point it to a throwaway path. For now we keep a dedicated collection.\n",
    "VS_COLLECTION_NAME = \"hybrid_multi_source_rag\"\n",
    "VS_PERSIST_DIR = None  # set to e.g. \"chroma_db/hybrid_multi_source_rag_v1\" if you want persistence\n",
    "\n",
    "# ---- Instantiate core components ----\n",
    "\n",
    "# Loader and splitter (used later to build the hybrid corpus)\n",
    "doc_loader = IntergraxDocumentsLoader(\n",
    "    verbose=True,\n",
    "    # docx_mode=\"paragraphs\" lets you load Word files in finer-grained segments\n",
    "    docx_mode=\"paragraphs\",\n",
    ")\n",
    "\n",
    "splitter = IntergraxDocumentsSplitter(\n",
    "    verbose=True,\n",
    "    # Note: the splitter currently takes its chunking config from inside the class;\n",
    "    # if you expose chunk_size/overlap in the future, you can wire CHUNK_SIZE here.\n",
    ")\n",
    "\n",
    "# Embedding manager (Ollama-based embeddings)\n",
    "embed_manager = IntergraxEmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=EMBED_PROVIDER,\n",
    "    model_name=EMBED_MODEL_NAME,\n",
    "    assume_ollama_dim=EMBED_DIM,\n",
    ")\n",
    "\n",
    "# Vectorstore manager (Chroma backend)\n",
    "vs_config = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=VS_COLLECTION_NAME,\n",
    "    chroma_persist_directory=VS_PERSIST_DIR,\n",
    ")\n",
    "\n",
    "vectorstore = IntergraxVectorstoreManager(\n",
    "    config=vs_config,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(DOCX_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"Environment initialized.\")\n",
    "print(f\"TENANT={TENANT}, CORPUS={CORPUS}, VERSION={VERSION}\")\n",
    "print(f\"PDF_DIR={PDF_DIR}\")\n",
    "print(f\"DOCX_DIR={DOCX_DIR}\")\n",
    "print(f\"Vectorstore collection={VS_COLLECTION_NAME}, persist={VS_PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479fb7b",
   "metadata": {},
   "source": [
    "## 2. Web search setup (Intergrax WebSearchExecutor)\n",
    "\n",
    "In this section we configure the **web search layer** that will provide live web documents as one of the sources for the hybrid RAG corpus.\n",
    "\n",
    "We:\n",
    "\n",
    "- Load API keys from the environment,\n",
    "- Initialize:\n",
    "  - `OpenAIChatResponsesAdapter` (LLM used by web search),\n",
    "  - `WebSearchExecutor` with `GoogleCSEProvider`,\n",
    "  - `WebSearchContextBuilder` for building condensed context from web docs,\n",
    "  - (optionally) `WebSearchAnswerer` for standalone web-only QA,\n",
    "- Provide a small async helper function that returns **serialized web documents** (plain dicts) ready to be merged with local PDF/DOCX documents later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf14b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search layer initialized (Google CSE + OpenAI).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import Client\n",
    "\n",
    "from intergrax.llm_adapters import OpenAIChatResponsesAdapter\n",
    "from intergrax.websearch.service.websearch_executor import WebSearchExecutor\n",
    "from intergrax.websearch.context.websearch_context_builder import WebSearchContextBuilder\n",
    "from intergrax.websearch.service.websearch_answerer import WebSearchAnswerer\n",
    "from intergrax.websearch.providers.google_cse_provider import GoogleCSEProvider\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Environment variables for OpenAI + Google CSE ---\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "os.environ[\"GOOGLE_CSE_API_KEY\"] = os.getenv(\"GOOGLE_CSE_API_KEY\")\n",
    "os.environ[\"GOOGLE_CSE_CX\"] = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "\n",
    "# --- LLM adapter used by the websearch layer (and later we can reuse it) ---\n",
    "\n",
    "openai_client = Client()\n",
    "\n",
    "llm_adapter = OpenAIChatResponsesAdapter(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-5-mini\",  # adjust to your preferred model\n",
    ")\n",
    "\n",
    "# --- WebSearchExecutor with Google CSE provider ---\n",
    "\n",
    "websearch_executor = WebSearchExecutor(\n",
    "    providers=[GoogleCSEProvider()],\n",
    "    default_top_k=6,\n",
    "    default_locale=\"en-US\",\n",
    "    default_region=\"en-US\",\n",
    "    default_language=\"en\",\n",
    "    default_safe_search=True,\n",
    "    max_text_chars=2000,\n",
    ")\n",
    "\n",
    "# --- Context builder and (optional) web-only answerer ---\n",
    "\n",
    "context_builder = WebSearchContextBuilder(\n",
    "    max_docs=4,\n",
    "    max_chars_per_doc=1500,\n",
    "    include_snippet=True,\n",
    "    include_url=True,\n",
    "    source_label_prefix=\"Source\",\n",
    ")\n",
    "\n",
    "websearch_answerer = WebSearchAnswerer(\n",
    "    adapter=llm_adapter,\n",
    "    executor=websearch_executor,\n",
    "    context_builder=context_builder,\n",
    "    answer_language=\"en\",\n",
    "    # system_prompt=system_prompts.strict_web_rag,  # optional, if you have it\n",
    ")\n",
    "\n",
    "print(\"Web search layer initialized (Google CSE + OpenAI).\")\n",
    "\n",
    "# --- Async helper for raw web documents (serialized) ---\n",
    "\n",
    "async def websearch_fetch_serialized(\n",
    "    question: str,\n",
    "    top_k: int = 8,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run web search for a given question and return a list of serialized documents.\n",
    "\n",
    "    Each element in the returned list is a plain dict (serialized WebDocument),\n",
    "    ready to be:\n",
    "      - converted into RAG chunks,\n",
    "      - or used by WebSearchContextBuilder to build a condensed text context.\n",
    "    \"\"\"\n",
    "    web_docs: List[Dict[str, Any]] = await websearch_executor.search_async(\n",
    "        query=question,\n",
    "        top_k=top_k,\n",
    "        serialize=True,\n",
    "    )\n",
    "    return web_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5977337",
   "metadata": {},
   "source": [
    "## 3. Hybrid RAG state definition\n",
    "\n",
    "To orchestrate the hybrid RAG flow with LangGraph, we will use a single shared state\n",
    "object that flows through all nodes.\n",
    "\n",
    "The state should contain:\n",
    "\n",
    "- `question`: the original user question,\n",
    "- `pdf_docs`: documents loaded from local PDF files (before splitting),\n",
    "- `docx_docs`: documents loaded from local DOCX files (before splitting),\n",
    "- `web_docs_serialized`: web search results in serialized form (plain dicts coming from `WebSearchExecutor.search_async(..., serialize=True)`),\n",
    "- `split_docs`: the final list of **chunked** documents (LangChain `Document` objects) ready for embedding,\n",
    "- `vectorstore_ready`: a simple flag indicating that the vectorstore has been built/updated,\n",
    "- `answer`: the final answer text produced by the RAG pipeline,\n",
    "- `debug_info`: diagnostic information useful for inspecting what happened\n",
    "  (counts of docs/chunks, vectorstore collection name, etc.).\n",
    "\n",
    "We will represent this state as a `TypedDict` so that LangGraph can use it as the\n",
    "graph state type. This also makes the code easier to reason about and refactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191baccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "class HybridRagState(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    Shared state for the hybrid multi-source RAG pipeline.\n",
    "\n",
    "    This object flows through all LangGraph nodes and is gradually enriched with:\n",
    "      - local PDF/DOCX documents,\n",
    "      - web documents (serialized),\n",
    "      - split/chunked documents,\n",
    "      - vectorstore metadata,\n",
    "      - final RAG answer and debug info.\n",
    "    \"\"\"\n",
    "\n",
    "    # User input\n",
    "    question: str\n",
    "\n",
    "    # Local documents before splitting (LangChain Document objects)\n",
    "    pdf_docs: List[Document]\n",
    "    docx_docs: List[Document]\n",
    "\n",
    "    # Web documents as serialized dicts (from WebSearchExecutor.search_async(..., serialize=True))\n",
    "    web_docs_serialized: List[Dict[str, Any]]\n",
    "\n",
    "    # Final chunked documents ready for embedding / indexing\n",
    "    split_docs: List[Document]\n",
    "\n",
    "    # Vectorstore status / metadata\n",
    "    vectorstore_ready: bool\n",
    "    vectorstore_collection: Optional[str]\n",
    "\n",
    "    # Final answer\n",
    "    answer: str\n",
    "\n",
    "    # Misc debug information (counts, timings, etc.)\n",
    "    debug_info: Dict[str, Any]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb33ee",
   "metadata": {},
   "source": [
    "## 4. Local documents loading (PDF + DOCX)\n",
    "\n",
    "In this step we load **local documents** that will form the first part of the hybrid RAG corpus.\n",
    "\n",
    "We use the existing `IntergraxDocumentsLoader` to:\n",
    "\n",
    "- Load PDF files from a dedicated directory (e.g. `PDF_DIR`),\n",
    "- Load DOCX files from a dedicated directory (e.g. `DOCX_DIR`),\n",
    "- Return them as LangChain `Document` objects.\n",
    "\n",
    "Each document already carries metadata (such as `source_path`, `source_name`, etc.) that we will later\n",
    "retain when splitting into chunks and inserting into the vectorstore.\n",
    "\n",
    "The goals of this step:\n",
    "\n",
    "- Provide small helper functions:\n",
    "  - `load_pdf_docs(pdf_dir: Path) -> list[Document]`\n",
    "  - `load_docx_docs(docx_dir: Path) -> list[Document]`\n",
    "- Provide a convenience function that loads **both** PDF and DOCX documents and returns:\n",
    "  - the documents,\n",
    "  - basic debug information (counts per type),\n",
    "- Prepare the structure we will later wrap into a LangGraph node that updates `HybridRagState`\n",
    "  (`pdf_docs`, `docx_docs`, `debug_info`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d753006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "def load_pdf_docs(pdf_dir: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all PDF documents from the given directory using IntergraxDocumentsLoader.\n",
    "\n",
    "    The loader is configured globally (doc_loader) and will:\n",
    "      - scan the directory,\n",
    "      - load supported files (PDF),\n",
    "      - attach basic metadata (e.g. source_path, source_name).\n",
    "    \"\"\"\n",
    "    if not pdf_dir.exists():\n",
    "        print(f\"[WARN] PDF directory does not exist: {pdf_dir}\")\n",
    "        return []\n",
    "\n",
    "    # Reuse the global loader; it will handle PDF files as well.\n",
    "    pdf_docs: List[Document] = doc_loader.load_documents(str(pdf_dir))\n",
    "    print(f\"[LOCAL LOAD] PDF docs loaded: {len(pdf_docs)} from {pdf_dir}\")\n",
    "    return pdf_docs\n",
    "\n",
    "\n",
    "def load_docx_docs(docx_dir: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all DOCX documents from the given directory using IntergraxDocumentsLoader.\n",
    "\n",
    "    Because `doc_loader` was initialized with `docx_mode='paragraphs'`,\n",
    "    DOCX files will be loaded with finer-grained paragraph segmentation\n",
    "    (before we apply RAG splitting).\n",
    "    \"\"\"\n",
    "    if not docx_dir.exists():\n",
    "        print(f\"[WARN] DOCX directory does not exist: {docx_dir}\")\n",
    "        return []\n",
    "\n",
    "    docx_docs: List[Document] = doc_loader.load_documents(str(docx_dir))\n",
    "    print(f\"[LOCAL LOAD] DOCX docs loaded: {len(docx_docs)} from {docx_dir}\")\n",
    "    return docx_docs\n",
    "\n",
    "\n",
    "def load_all_local_docs() -> Tuple[List[Document], List[Document], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convenience helper for the notebook:\n",
    "\n",
    "    - Loads PDF docs from PDF_DIR,\n",
    "    - Loads DOCX docs from DOCX_DIR,\n",
    "    - Returns both lists plus a simple debug_info dict.\n",
    "    \"\"\"\n",
    "    pdf_docs = load_pdf_docs(PDF_DIR)\n",
    "    docx_docs = load_docx_docs(DOCX_DIR)\n",
    "\n",
    "    debug_info: Dict[str, Any] = {\n",
    "        \"pdf_docs_count\": len(pdf_docs),\n",
    "        \"docx_docs_count\": len(docx_docs),\n",
    "        \"pdf_dir\": str(PDF_DIR),\n",
    "        \"docx_dir\": str(DOCX_DIR),\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"[LOCAL LOAD] Total local docs -> \"\n",
    "        f\"PDF: {len(pdf_docs)}, DOCX: {len(docx_docs)}\"\n",
    "    )\n",
    "\n",
    "    return pdf_docs, docx_docs, debug_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5383c1",
   "metadata": {},
   "source": [
    "## 5. Source loading nodes (local PDF/DOCX + web)\n",
    "\n",
    "Now that we have:\n",
    "\n",
    "- a shared `HybridRagState`,\n",
    "- helpers for loading local documents (`load_all_local_docs()`),\n",
    "- a helper for fetching web documents as serialized dicts (`websearch_fetch_serialized()`),\n",
    "\n",
    "we can expose them as **LangGraph nodes**.\n",
    "\n",
    "We will create two nodes:\n",
    "\n",
    "1. `load_local_docs_node(state: HybridRagState) -> HybridRagState`  \n",
    "   - Loads PDF and DOCX documents from the configured directories,\n",
    "   - Stores them in `state[\"pdf_docs\"]` and `state[\"docx_docs\"]`,\n",
    "   - Updates `state[\"debug_info\"]` with basic counts and directory paths.\n",
    "\n",
    "2. `load_web_docs_node(state: HybridRagState) -> HybridRagState` (async)  \n",
    "   - Uses the user `question` from the state,\n",
    "   - Calls `websearch_fetch_serialized(question, top_k=...)`,\n",
    "   - Stores serialized web documents in `state[\"web_docs_serialized\"]`,\n",
    "   - Updates `state[\"debug_info\"]` with the number of web documents.\n",
    "\n",
    "These nodes will be the **first steps** of the hybrid RAG pipeline in the LangGraph graph.\n",
    "Later nodes will split documents, build the vectorstore, and generate the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4796d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def load_local_docs_node(state: HybridRagState) -> HybridRagState:\n",
    "    \"\"\"\n",
    "    LangGraph node:\n",
    "      - loads local PDF + DOCX documents,\n",
    "      - stores them in the state,\n",
    "      - updates debug_info with basic stats.\n",
    "    \"\"\"\n",
    "    pdf_docs, docx_docs, debug_local = load_all_local_docs()\n",
    "\n",
    "    # Merge with any existing debug info\n",
    "    debug = dict(state.get(\"debug_info\", {}))\n",
    "    debug.update(debug_local)\n",
    "\n",
    "    new_state: HybridRagState = {\n",
    "        **state,\n",
    "        \"pdf_docs\": pdf_docs,\n",
    "        \"docx_docs\": docx_docs,\n",
    "        \"debug_info\": debug,\n",
    "    }\n",
    "\n",
    "    return new_state\n",
    "\n",
    "\n",
    "async def load_web_docs_node(state: HybridRagState) -> HybridRagState:\n",
    "    \"\"\"\n",
    "    LangGraph node (async):\n",
    "      - uses the question from the state,\n",
    "      - runs web search via Intergrax WebSearchExecutor,\n",
    "      - stores serialized web docs in the state,\n",
    "      - updates debug_info with web_docs_count.\n",
    "    \"\"\"\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    if not question:\n",
    "        print(\"[WEB LOAD] Empty question in state; skipping web search.\")\n",
    "        web_docs_serialized: list[Dict[str, Any]] = []\n",
    "    else:\n",
    "        web_docs_serialized = await websearch_fetch_serialized(\n",
    "            question=question,\n",
    "            top_k=8,\n",
    "        )\n",
    "\n",
    "    debug = dict(state.get(\"debug_info\", {}))\n",
    "    debug.update(\n",
    "        {\n",
    "            \"web_docs_count\": len(web_docs_serialized),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    new_state: HybridRagState = {\n",
    "        **state,\n",
    "        \"web_docs_serialized\": web_docs_serialized,\n",
    "        \"debug_info\": debug,\n",
    "    }\n",
    "\n",
    "    return new_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
