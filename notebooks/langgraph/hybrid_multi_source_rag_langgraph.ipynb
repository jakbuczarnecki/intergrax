{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee866560",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ce59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87f93d",
   "metadata": {},
   "source": [
    "# Hybrid Multi-Source RAG with Intergrax + LangGraph\n",
    "\n",
    "This notebook demonstrates a **practical, end-to-end RAG workflow** that combines multiple knowledge sources into a single in-memory vector index and exposes it through a LangGraph-based agent.\n",
    "\n",
    "We will use **Intergrax** components together with **LangGraph** to:\n",
    "\n",
    "1. **Ingest content from multiple sources:**\n",
    "   - Local PDF files (from a given directory),\n",
    "   - Local DOCX/Word files (from a given directory),\n",
    "   - Live web results using the Intergrax `WebSearchExecutor`.\n",
    "\n",
    "2. **Build a unified RAG corpus:**\n",
    "   - Normalize all documents into a common internal format,\n",
    "   - (Optionally) attach basic metadata about origin (pdf / docx / web),\n",
    "   - Split documents into chunks suitable for embedding.\n",
    "\n",
    "3. **Create an in-memory vector index:**\n",
    "   - Use an Intergrax embedding manager (e.g. OpenAI / Ollama),\n",
    "   - Store embeddings in an **in-memory Chroma** collection via Intergrax vectorstore manager,\n",
    "   - Keep everything ephemeral (no persistence, perfect for “ad-hoc research” scenarios).\n",
    "\n",
    "4. **Answer user questions with a RAG agent:**\n",
    "   - The user provides a natural language question,\n",
    "   - LangGraph orchestrates the flow: load → merge → index → retrieve → answer,\n",
    "   - An Intergrax `RagAnswerer` (or `WindowedAnswerer`) generates a **single, structured report**:\n",
    "     - short summary of the relevant information,\n",
    "     - key insights and conclusions,\n",
    "     - optionally: recommendations / action items.\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook showcases\n",
    "\n",
    "- How to combine **local files + web search** in a single RAG pipeline.\n",
    "- How to plug Intergrax components (loaders, splitter, embeddings, vectorstore, RAG answerer, websearch) into a **LangGraph `StateGraph`**.\n",
    "- How to build a **temporary, in-memory knowledge graph** for one-off research tasks (no database setup required).\n",
    "- A clean, production-oriented pattern that can be reused in:\n",
    "  - internal knowledge explorers,\n",
    "  - “research bot” agents,\n",
    "  - prototype assistants for teams or clients.\n",
    "\n",
    "All code and comments in this notebook are in **English** to keep it ready for public documentation, GitHub examples, and international collaborators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9690fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
