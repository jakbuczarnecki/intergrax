{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee866560",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ce59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87f93d",
   "metadata": {},
   "source": [
    "# Hybrid Multi-Source RAG with Intergrax + LangGraph\n",
    "\n",
    "This notebook demonstrates a **practical, end-to-end RAG workflow** that combines multiple knowledge sources into a single in-memory vector index and exposes it through a LangGraph-based agent.\n",
    "\n",
    "We will use **Intergrax** components together with **LangGraph** to:\n",
    "\n",
    "1. **Ingest content from multiple sources:**\n",
    "   - Local PDF files (from a given directory),\n",
    "   - Local DOCX/Word files (from a given directory),\n",
    "   - Live web results using the Intergrax `WebSearchExecutor`.\n",
    "\n",
    "2. **Build a unified RAG corpus:**\n",
    "   - Normalize all documents into a common internal format,\n",
    "   - (Optionally) attach basic metadata about origin (pdf / docx / web),\n",
    "   - Split documents into chunks suitable for embedding.\n",
    "\n",
    "3. **Create an in-memory vector index:**\n",
    "   - Use an Intergrax embedding manager (e.g. OpenAI / Ollama),\n",
    "   - Store embeddings in an **in-memory Chroma** collection via Intergrax vectorstore manager,\n",
    "   - Keep everything ephemeral (no persistence, perfect for “ad-hoc research” scenarios).\n",
    "\n",
    "4. **Answer user questions with a RAG agent:**\n",
    "   - The user provides a natural language question,\n",
    "   - LangGraph orchestrates the flow: load → merge → index → retrieve → answer,\n",
    "   - An Intergrax `RagAnswerer` (or `WindowedAnswerer`) generates a **single, structured report**:\n",
    "     - short summary of the relevant information,\n",
    "     - key insights and conclusions,\n",
    "     - optionally: recommendations / action items.\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook showcases\n",
    "\n",
    "- How to combine **local files + web search** in a single RAG pipeline.\n",
    "- How to plug Intergrax components (loaders, splitter, embeddings, vectorstore, RAG answerer, websearch) into a **LangGraph `StateGraph`**.\n",
    "- How to build a **temporary, in-memory knowledge graph** for one-off research tasks (no database setup required).\n",
    "- A clean, production-oriented pattern that can be reused in:\n",
    "  - internal knowledge explorers,\n",
    "  - “research bot” agents,\n",
    "  - prototype assistants for teams or clients.\n",
    "\n",
    "All code and comments in this notebook are in **English** to keep it ready for public documentation, GitHub examples, and international collaborators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59eae8",
   "metadata": {},
   "source": [
    "## 1. Environment and configuration\n",
    "\n",
    "In this section we prepare the environment for the hybrid multi-source RAG agent.\n",
    "\n",
    "The goals of this step:\n",
    "\n",
    "- Load all required configuration values (API keys, base paths, model names) from environment variables or a `.env` file.\n",
    "- Import the core building blocks from:\n",
    "  - the Intergrax framework (LLM adapter, embedding manager, vectorstore manager, document loaders, RAG answerer, websearch executor),\n",
    "  - LangGraph (for defining and running the `StateGraph`),\n",
    "  - Standard Python modules (`os`, `pathlib`, `typing`, etc.).\n",
    "- Define the base directories for local documents:\n",
    "  - one directory for PDF files (e.g. `./data/pdf`),\n",
    "  - one directory for DOCX files (e.g. `./data/docx`).\n",
    "- Decide on the core RAG parameters:\n",
    "  - chunk size and overlap for splitting documents,\n",
    "  - embedding model name,\n",
    "  - LLM model name used by the answerer,\n",
    "  - number of retrieved documents (`top_k`) during similarity search.\n",
    "- Initialize the core Intergrax components that will be reused across the notebook:\n",
    "  - an embedding manager (e.g. OpenAI-based or Ollama-based),\n",
    "  - a vectorstore manager backed by an **in-memory** Chroma collection (no persistence),\n",
    "  - an LLM adapter used by the RAG answerer,\n",
    "  - a simple text splitter for turning documents into chunks.\n",
    "\n",
    "At the end of this section we want to have a small configuration block that:\n",
    "\n",
    "1. Reads configuration (API keys, paths, model names),\n",
    "2. Instantiates the main Intergrax services (embeddings, vectorstore, LLM adapter, text splitter),\n",
    "3. Is easy to adjust for different environments (OpenAI vs local models, different directories, different Chroma settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 16:28:52,641 [INFO] [intergraxEmbeddingManager] Loading model 'rjmalagon/gte-qwen2-1.5b-instruct-embed-f16:latest' (provider=ollama)\n",
      "2025-11-18 16:28:53,175 [INFO] HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-11-18 16:28:53,177 [INFO] [intergraxEmbeddingManager] Loaded. Embedding dim = 1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=hybrid_multi_source_rag\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "Environment initialized.\n",
      "TENANT=intergrax, CORPUS=hybrid-multi-source, VERSION=v1\n",
      "PDF_DIR=..\\documents\\hybrid-corpus\\pdf\n",
      "DOCX_DIR=..\\documents\\hybrid-corpus\\docx\n",
      "Vectorstore collection=hybrid_multi_source_rag, persist=None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from intergrax.globals.settings import GLOBAL_SETTINGS\n",
    "from intergrax.rag.documents_loader import DocumentsLoader\n",
    "from intergrax.rag.documents_splitter import DocumentsSplitter\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "\n",
    "import intergrax.logging  # initializes logging format/levels for the framework\n",
    "\n",
    "# ---- Tenant / corpus configuration (for metadata + filtering) ----\n",
    "TENANT = \"intergrax\"\n",
    "CORPUS = \"hybrid-multi-source\"\n",
    "VERSION = \"v1\"\n",
    "\n",
    "# ---- Base directories for local documents ----\n",
    "# You can adjust these to your actual layout.\n",
    "# For the hybrid demo we assume:\n",
    "#   ../documents/hybrid-corpus/pdf\n",
    "#   ../documents/hybrid-corpus/docx\n",
    "BASE_DOCS_DIR = Path(\"../documents/hybrid-corpus\")\n",
    "PDF_DIR = BASE_DOCS_DIR / \"pdf\"\n",
    "DOCX_DIR = BASE_DOCS_DIR / \"docx\"\n",
    "\n",
    "# ---- Core RAG parameters ----\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 150\n",
    "TOP_K = 8\n",
    "\n",
    "# Embedding model configuration (using your existing Ollama-based setup)\n",
    "EMBED_PROVIDER = \"ollama\"\n",
    "EMBED_MODEL_NAME = GLOBAL_SETTINGS.default_ollama_embed_model\n",
    "EMBED_DIM = 1536  # assumed dimension for this model\n",
    "\n",
    "# Vectorstore configuration\n",
    "# For *ephemeral* usage you can set `chroma_persist_directory=None`\n",
    "# or point it to a throwaway path. For now we keep a dedicated collection.\n",
    "VS_COLLECTION_NAME = \"hybrid_multi_source_rag\"\n",
    "VS_PERSIST_DIR = None  # set to e.g. \"chroma_db/hybrid_multi_source_rag_v1\" if you want persistence\n",
    "\n",
    "# ---- Instantiate core components ----\n",
    "\n",
    "# Loader and splitter (used later to build the hybrid corpus)\n",
    "doc_loader = DocumentsLoader(\n",
    "    verbose=True,\n",
    "    # docx_mode=\"paragraphs\" lets you load Word files in finer-grained segments\n",
    "    docx_mode=\"paragraphs\",\n",
    ")\n",
    "\n",
    "splitter = DocumentsSplitter(\n",
    "    verbose=True,\n",
    "    # Note: the splitter currently takes its chunking config from inside the class;\n",
    "    # if you expose chunk_size/overlap in the future, you can wire CHUNK_SIZE here.\n",
    ")\n",
    "\n",
    "# Embedding manager (Ollama-based embeddings)\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=EMBED_PROVIDER,\n",
    "    model_name=EMBED_MODEL_NAME,\n",
    "    assume_ollama_dim=EMBED_DIM,\n",
    ")\n",
    "\n",
    "# Vectorstore manager (Chroma backend)\n",
    "vs_config = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=VS_COLLECTION_NAME,\n",
    "    chroma_persist_directory=VS_PERSIST_DIR,\n",
    ")\n",
    "\n",
    "vectorstore = VectorstoreManager(\n",
    "    config=vs_config,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(DOCX_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"Environment initialized.\")\n",
    "print(f\"TENANT={TENANT}, CORPUS={CORPUS}, VERSION={VERSION}\")\n",
    "print(f\"PDF_DIR={PDF_DIR}\")\n",
    "print(f\"DOCX_DIR={DOCX_DIR}\")\n",
    "print(f\"Vectorstore collection={VS_COLLECTION_NAME}, persist={VS_PERSIST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479fb7b",
   "metadata": {},
   "source": [
    "## 2. Web search setup (Intergrax WebSearchExecutor)\n",
    "\n",
    "In this section we configure the **web search layer** that will provide live web documents as one of the sources for the hybrid RAG corpus.\n",
    "\n",
    "We:\n",
    "\n",
    "- Load API keys from the environment,\n",
    "- Initialize:\n",
    "  - `OpenAIChatResponsesAdapter` (LLM used by web search),\n",
    "  - `WebSearchExecutor` with `GoogleCSEProvider`,\n",
    "  - `WebSearchContextBuilder` for building condensed context from web docs,\n",
    "  - (optionally) `WebSearchAnswerer` for standalone web-only QA,\n",
    "- Provide a small async helper function that returns **serialized web documents** (plain dicts) ready to be merged with local PDF/DOCX documents later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf14b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search layer initialized (Google CSE + OpenAI).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import Client\n",
    "\n",
    "from intergrax.llm_adapters import OpenAIChatResponsesAdapter\n",
    "from intergrax.websearch.service.websearch_executor import WebSearchExecutor\n",
    "from intergrax.websearch.context.websearch_context_builder import WebSearchContextBuilder\n",
    "from intergrax.websearch.service.websearch_answerer import WebSearchAnswerer\n",
    "from intergrax.websearch.providers.google_cse_provider import GoogleCSEProvider\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Environment variables for OpenAI + Google CSE ---\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "os.environ[\"GOOGLE_CSE_API_KEY\"] = os.getenv(\"GOOGLE_CSE_API_KEY\")\n",
    "os.environ[\"GOOGLE_CSE_CX\"] = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "\n",
    "# --- LLM adapter used by the websearch layer (and later we can reuse it) ---\n",
    "\n",
    "openai_client = Client()\n",
    "\n",
    "llm_adapter = OpenAIChatResponsesAdapter(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-5-mini\",  # adjust to your preferred model\n",
    ")\n",
    "\n",
    "# --- WebSearchExecutor with Google CSE provider ---\n",
    "\n",
    "websearch_executor = WebSearchExecutor(\n",
    "    providers=[GoogleCSEProvider()],\n",
    "    default_top_k=6,\n",
    "    default_locale=\"en-US\",\n",
    "    default_region=\"en-US\",\n",
    "    default_language=\"en\",\n",
    "    default_safe_search=True,\n",
    "    max_text_chars=2000,\n",
    ")\n",
    "\n",
    "# --- Context builder and (optional) web-only answerer ---\n",
    "\n",
    "context_builder = WebSearchContextBuilder(\n",
    "    max_docs=4,\n",
    "    max_chars_per_doc=1500,\n",
    "    include_snippet=True,\n",
    "    include_url=True,\n",
    "    source_label_prefix=\"Source\",\n",
    ")\n",
    "\n",
    "websearch_answerer = WebSearchAnswerer(\n",
    "    adapter=llm_adapter,\n",
    "    executor=websearch_executor,\n",
    "    context_builder=context_builder,\n",
    "    answer_language=\"en\",\n",
    "    # system_prompt=system_prompts.strict_web_rag,  # optional, if you have it\n",
    ")\n",
    "\n",
    "print(\"Web search layer initialized (Google CSE + OpenAI).\")\n",
    "\n",
    "# --- Async helper for raw web documents (serialized) ---\n",
    "\n",
    "async def websearch_fetch_serialized(\n",
    "    question: str,\n",
    "    top_k: int = 8,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run web search for a given question and return a list of serialized documents.\n",
    "\n",
    "    Each element in the returned list is a plain dict (serialized WebDocument),\n",
    "    ready to be:\n",
    "      - converted into RAG chunks,\n",
    "      - or used by WebSearchContextBuilder to build a condensed text context.\n",
    "    \"\"\"\n",
    "    web_docs: List[Dict[str, Any]] = await websearch_executor.search_async(\n",
    "        query=question,\n",
    "        top_k=top_k,\n",
    "        serialize=True,\n",
    "    )\n",
    "    return web_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5977337",
   "metadata": {},
   "source": [
    "## 3. Hybrid RAG state definition\n",
    "\n",
    "To orchestrate the hybrid RAG flow with LangGraph, we will use a single shared state\n",
    "object that flows through all nodes.\n",
    "\n",
    "The state should contain:\n",
    "\n",
    "- `question`: the original user question,\n",
    "- `pdf_docs`: documents loaded from local PDF files (before splitting),\n",
    "- `docx_docs`: documents loaded from local DOCX files (before splitting),\n",
    "- `web_docs_serialized`: web search results in serialized form (plain dicts coming from `WebSearchExecutor.search_async(..., serialize=True)`),\n",
    "- `split_docs`: the final list of **chunked** documents (LangChain `Document` objects) ready for embedding,\n",
    "- `vectorstore_ready`: a simple flag indicating that the vectorstore has been built/updated,\n",
    "- `answer`: the final answer text produced by the RAG pipeline,\n",
    "- `debug_info`: diagnostic information useful for inspecting what happened\n",
    "  (counts of docs/chunks, vectorstore collection name, etc.).\n",
    "\n",
    "We will represent this state as a `TypedDict` so that LangGraph can use it as the\n",
    "graph state type. This also makes the code easier to reason about and refactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191baccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "class HybridRagState(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    Shared state for the hybrid multi-source RAG pipeline.\n",
    "\n",
    "    This object flows through all LangGraph nodes and is gradually enriched with:\n",
    "      - local PDF/DOCX documents,\n",
    "      - web documents (serialized),\n",
    "      - split/chunked documents,\n",
    "      - vectorstore metadata,\n",
    "      - final RAG answer and debug info.\n",
    "    \"\"\"\n",
    "\n",
    "    # User input\n",
    "    question: str\n",
    "\n",
    "    # Local documents before splitting (LangChain Document objects)\n",
    "    pdf_docs: List[Document]\n",
    "    docx_docs: List[Document]\n",
    "\n",
    "    # Web documents as serialized dicts (from WebSearchExecutor.search_async(..., serialize=True))\n",
    "    web_docs_serialized: List[Dict[str, Any]]\n",
    "\n",
    "    # Final chunked documents ready for embedding / indexing\n",
    "    split_docs: List[Document]\n",
    "\n",
    "    # Vectorstore status / metadata\n",
    "    vectorstore_ready: bool\n",
    "    vectorstore_collection: Optional[str]\n",
    "\n",
    "    # Final answer\n",
    "    answer: str\n",
    "\n",
    "    # Misc debug information (counts, timings, etc.)\n",
    "    debug_info: Dict[str, Any]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb33ee",
   "metadata": {},
   "source": [
    "## 4. Local documents loading (PDF + DOCX)\n",
    "\n",
    "In this step we load **local documents** that will form the first part of the hybrid RAG corpus.\n",
    "\n",
    "We use the existing `IntergraxDocumentsLoader` to:\n",
    "\n",
    "- Load PDF files from a dedicated directory (e.g. `PDF_DIR`),\n",
    "- Load DOCX files from a dedicated directory (e.g. `DOCX_DIR`),\n",
    "- Return them as LangChain `Document` objects.\n",
    "\n",
    "Each document already carries metadata (such as `source_path`, `source_name`, etc.) that we will later\n",
    "retain when splitting into chunks and inserting into the vectorstore.\n",
    "\n",
    "The goals of this step:\n",
    "\n",
    "- Provide small helper functions:\n",
    "  - `load_pdf_docs(pdf_dir: Path) -> list[Document]`\n",
    "  - `load_docx_docs(docx_dir: Path) -> list[Document]`\n",
    "- Provide a convenience function that loads **both** PDF and DOCX documents and returns:\n",
    "  - the documents,\n",
    "  - basic debug information (counts per type),\n",
    "- Prepare the structure we will later wrap into a LangGraph node that updates `HybridRagState`\n",
    "  (`pdf_docs`, `docx_docs`, `debug_info`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d753006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "def load_pdf_docs(pdf_dir: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all PDF documents from the given directory using IntergraxDocumentsLoader.\n",
    "\n",
    "    The loader is configured globally (doc_loader) and will:\n",
    "      - scan the directory,\n",
    "      - load supported files (PDF),\n",
    "      - attach basic metadata (e.g. source_path, source_name).\n",
    "    \"\"\"\n",
    "    if not pdf_dir.exists():\n",
    "        print(f\"[WARN] PDF directory does not exist: {pdf_dir}\")\n",
    "        return []\n",
    "\n",
    "    # Reuse the global loader; it will handle PDF files as well.\n",
    "    pdf_docs: List[Document] = doc_loader.load_documents(str(pdf_dir))\n",
    "    print(f\"[LOCAL LOAD] PDF docs loaded: {len(pdf_docs)} from {pdf_dir}\")\n",
    "    return pdf_docs\n",
    "\n",
    "\n",
    "def load_docx_docs(docx_dir: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load all DOCX documents from the given directory using IntergraxDocumentsLoader.\n",
    "\n",
    "    Because `doc_loader` was initialized with `docx_mode='paragraphs'`,\n",
    "    DOCX files will be loaded with finer-grained paragraph segmentation\n",
    "    (before we apply RAG splitting).\n",
    "    \"\"\"\n",
    "    if not docx_dir.exists():\n",
    "        print(f\"[WARN] DOCX directory does not exist: {docx_dir}\")\n",
    "        return []\n",
    "\n",
    "    docx_docs: List[Document] = doc_loader.load_documents(str(docx_dir))\n",
    "    print(f\"[LOCAL LOAD] DOCX docs loaded: {len(docx_docs)} from {docx_dir}\")\n",
    "    return docx_docs\n",
    "\n",
    "\n",
    "def load_all_local_docs() -> Tuple[List[Document], List[Document], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convenience helper for the notebook:\n",
    "\n",
    "    - Loads PDF docs from PDF_DIR,\n",
    "    - Loads DOCX docs from DOCX_DIR,\n",
    "    - Returns both lists plus a simple debug_info dict.\n",
    "    \"\"\"\n",
    "    pdf_docs = load_pdf_docs(PDF_DIR)\n",
    "    docx_docs = load_docx_docs(DOCX_DIR)\n",
    "\n",
    "    debug_info: Dict[str, Any] = {\n",
    "        \"pdf_docs_count\": len(pdf_docs),\n",
    "        \"docx_docs_count\": len(docx_docs),\n",
    "        \"pdf_dir\": str(PDF_DIR),\n",
    "        \"docx_dir\": str(DOCX_DIR),\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"[LOCAL LOAD] Total local docs -> \"\n",
    "        f\"PDF: {len(pdf_docs)}, DOCX: {len(docx_docs)}\"\n",
    "    )\n",
    "\n",
    "    return pdf_docs, docx_docs, debug_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5383c1",
   "metadata": {},
   "source": [
    "## 5. Source loading nodes (local PDF/DOCX + web)\n",
    "\n",
    "Now that we have:\n",
    "\n",
    "- a shared `HybridRagState`,\n",
    "- helpers for loading local documents (`load_all_local_docs()`),\n",
    "- a helper for fetching web documents as serialized dicts (`websearch_fetch_serialized()`),\n",
    "\n",
    "we can expose them as **LangGraph nodes**.\n",
    "\n",
    "We will create two nodes:\n",
    "\n",
    "1. `load_local_docs_node(state: HybridRagState) -> HybridRagState`  \n",
    "   - Loads PDF and DOCX documents from the configured directories,\n",
    "   - Stores them in `state[\"pdf_docs\"]` and `state[\"docx_docs\"]`,\n",
    "   - Updates `state[\"debug_info\"]` with basic counts and directory paths.\n",
    "\n",
    "2. `load_web_docs_node(state: HybridRagState) -> HybridRagState` (async)  \n",
    "   - Uses the user `question` from the state,\n",
    "   - Calls `websearch_fetch_serialized(question, top_k=...)`,\n",
    "   - Stores serialized web documents in `state[\"web_docs_serialized\"]`,\n",
    "   - Updates `state[\"debug_info\"]` with the number of web documents.\n",
    "\n",
    "These nodes will be the **first steps** of the hybrid RAG pipeline in the LangGraph graph.\n",
    "Later nodes will split documents, build the vectorstore, and generate the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4796d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def load_local_docs_node(state: HybridRagState) -> HybridRagState:\n",
    "    \"\"\"\n",
    "    LangGraph node:\n",
    "      - loads local PDF + DOCX documents,\n",
    "      - stores them in the state,\n",
    "      - updates debug_info with basic stats.\n",
    "    \"\"\"\n",
    "    pdf_docs, docx_docs, debug_local = load_all_local_docs()\n",
    "\n",
    "    # Merge with any existing debug info\n",
    "    debug = dict(state.get(\"debug_info\", {}))\n",
    "    debug.update(debug_local)\n",
    "\n",
    "    new_state: HybridRagState = {\n",
    "        **state,\n",
    "        \"pdf_docs\": pdf_docs,\n",
    "        \"docx_docs\": docx_docs,\n",
    "        \"debug_info\": debug,\n",
    "    }\n",
    "\n",
    "    return new_state\n",
    "\n",
    "\n",
    "async def load_web_docs_node(state: HybridRagState) -> HybridRagState:\n",
    "    \"\"\"\n",
    "    LangGraph node (async):\n",
    "      - uses the question from the state,\n",
    "      - runs web search via Intergrax WebSearchExecutor,\n",
    "      - stores serialized web docs in the state,\n",
    "      - updates debug_info with web_docs_count.\n",
    "    \"\"\"\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    if not question:\n",
    "        print(\"[WEB LOAD] Empty question in state; skipping web search.\")\n",
    "        web_docs_serialized: list[Dict[str, Any]] = []\n",
    "    else:\n",
    "        web_docs_serialized = await websearch_fetch_serialized(\n",
    "            question=question,\n",
    "            top_k=8,\n",
    "        )\n",
    "\n",
    "    debug = dict(state.get(\"debug_info\", {}))\n",
    "    debug.update(\n",
    "        {\n",
    "            \"web_docs_count\": len(web_docs_serialized),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    new_state: HybridRagState = {\n",
    "        **state,\n",
    "        \"web_docs_serialized\": web_docs_serialized,\n",
    "        \"debug_info\": debug,\n",
    "    }\n",
    "\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ef1dd",
   "metadata": {},
   "source": [
    "## 6. Building a unified, chunked corpus\n",
    "\n",
    "At this point we already have:\n",
    "\n",
    "- Local documents:\n",
    "  - `state[\"pdf_docs\"]`: PDF documents loaded as `Document` objects,\n",
    "  - `state[\"docx_docs\"]`: DOCX documents loaded as `Document` objects,\n",
    "- Web documents:\n",
    "  - `state[\"web_docs_serialized\"]`: web results as serialized dicts coming from\n",
    "    `websearch_fetch_serialized(...)`.\n",
    "\n",
    "In this step we:\n",
    "\n",
    "1. Convert serialized web documents into LangChain `Document` objects, so that all\n",
    "   sources (PDF, DOCX, web) share the same internal representation.\n",
    "2. Merge all sources into a single list of `Document` objects.\n",
    "3. Use `IntergraxDocumentsSplitter` to split the merged corpus into chunks suitable\n",
    "   for embedding and retrieval.\n",
    "4. Store the result in:\n",
    "   - `state[\"split_docs\"]`: list of chunked `Document` objects,\n",
    "   - `state[\"debug_info\"][\"split_docs_count\"]`: total number of chunks.\n",
    "\n",
    "The splitter will:\n",
    "\n",
    "- Preserve existing metadata on each document,\n",
    "- Optionally add its own metadata (e.g. `chunk_id`, `chunk_index`, `parent_id`, etc.),\n",
    "- Allow us to keep track of the origin of each chunk (`source_type`, `source_path`, `source_url`).\n",
    "\n",
    "These chunked documents will be embedded and inserted into the vectorstore in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3698760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def web_docs_to_documents(\n",
    "    web_docs_serialized: List[Dict[str, Any]],\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Convert serialized web documents (plain dicts) into LangChain Document objects.\n",
    "\n",
    "    We try to pick a reasonable text field:\n",
    "      - prefer 'content' or 'text' if available,\n",
    "      - fall back to 'snippet' as a last resort.\n",
    "\n",
    "    Metadata includes:\n",
    "      - source_type = 'web',\n",
    "      - url, title, snippet (if present),\n",
    "      - you can also attach TENANT/CORPUS/VERSION here if you want them at document level.\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    for d in web_docs_serialized:\n",
    "        text = (\n",
    "            d.get(\"content\")\n",
    "            or d.get(\"text\")\n",
    "            or d.get(\"snippet\")\n",
    "            or \"\"\n",
    "        )\n",
    "\n",
    "        if not text.strip():\n",
    "            # Skip empty documents\n",
    "            continue\n",
    "\n",
    "        metadata: Dict[str, Any] = {\n",
    "            \"source_type\": \"web\",\n",
    "            \"source_url\": d.get(\"url\"),\n",
    "            \"source_title\": d.get(\"title\"),\n",
    "            \"source_snippet\": d.get(\"snippet\"),\n",
    "            \"tenant\": TENANT,\n",
    "            \"corpus\": CORPUS,\n",
    "            \"version\": VERSION,\n",
    "        }\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata=metadata,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_unified_corpus(\n",
    "    pdf_docs: List[Document],\n",
    "    docx_docs: List[Document],\n",
    "    web_docs_serialized: List[Dict[str, Any]],\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Merge all sources (PDF, DOCX, web) into a single list of Document objects.\n",
    "\n",
    "    - Local documents keep their loader metadata,\n",
    "      but we also ensure tenant/corpus/version is present.\n",
    "    - Web documents are converted from serialized dicts into Documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Enrich local docs with tenant/corpus/version if missing\n",
    "    def ensure_base_meta(doc: Document) -> Document:\n",
    "        md = dict(doc.metadata or {})\n",
    "        md.setdefault(\"tenant\", TENANT)\n",
    "        md.setdefault(\"corpus\", CORPUS)\n",
    "        md.setdefault(\"version\", VERSION)\n",
    "        # Optionally mark the source type if not present\n",
    "        if \"source_type\" not in md:\n",
    "            md[\"source_type\"] = \"local\"\n",
    "        return Document(page_content=doc.page_content, metadata=md)\n",
    "\n",
    "    pdf_docs_enriched = [ensure_base_meta(d) for d in pdf_docs]\n",
    "    docx_docs_enriched = [ensure_base_meta(d) for d in docx_docs]\n",
    "\n",
    "    # Convert web docs to Document objects\n",
    "    web_docs = web_docs_to_documents(web_docs_serialized)\n",
    "\n",
    "    all_docs: List[Document] = pdf_docs_enriched + docx_docs_enriched + web_docs\n",
    "\n",
    "    print(\n",
    "        \"[UNIFIED CORPUS] Total documents before splitting -> \"\n",
    "        f\"PDF: {len(pdf_docs_enriched)}, \"\n",
    "        f\"DOCX: {len(docx_docs_enriched)}, \"\n",
    "        f\"WEB: {len(web_docs)}, \"\n",
    "        f\"TOTAL: {len(all_docs)}\"\n",
    "    )\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "def split_corpus_node(state: HybridRagState) -> HybridRagState:\n",
    "    \"\"\"\n",
    "    LangGraph node:\n",
    "      - builds a unified corpus from local + web sources,\n",
    "      - applies IntergraxDocumentsSplitter to produce chunked documents,\n",
    "      - stores chunks in `state['split_docs']`,\n",
    "      - updates debug_info with `split_docs_count`.\n",
    "    \"\"\"\n",
    "    pdf_docs = state.get(\"pdf_docs\", []) or []\n",
    "    docx_docs = state.get(\"docx_docs\", []) or []\n",
    "    web_docs_serialized = state.get(\"web_docs_serialized\", []) or []\n",
    "\n",
    "    # 1) Build unified Document list\n",
    "    all_docs = build_unified_corpus(\n",
    "        pdf_docs=pdf_docs,\n",
    "        docx_docs=docx_docs,\n",
    "        web_docs_serialized=web_docs_serialized,\n",
    "    )\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"[SPLIT] No documents to split; unified corpus is empty.\")\n",
    "        split_docs: List[Document] = []\n",
    "    else:\n",
    "        # 2) Split into chunks using IntergraxDocumentsSplitter\n",
    "        # You already initialized `splitter` earlier.\n",
    "        #\n",
    "        # Optionally you can pass a custom metadata callback similar to your ingest script:\n",
    "        #\n",
    "        # def add_meta(chunk_doc: Document, idx: int, total: int):\n",
    "        #     return {\"tenant\": TENANT, \"corpus\": CORPUS, \"version\": VERSION}\n",
    "        #\n",
    "        # split_docs = splitter.split_documents(\n",
    "        #     documents=all_docs,\n",
    "        #     call_custom_metadata=add_meta,\n",
    "        # )\n",
    "        #\n",
    "        # For now we rely on the splitter's default behaviour.\n",
    "        split_docs = splitter.split_documents(documents=all_docs)\n",
    "\n",
    "    debug = dict(state.get(\"debug_info\", {}))\n",
    "    debug.update(\n",
    "        {\n",
    "            \"split_docs_count\": len(split_docs),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    new_state: HybridRagState = {\n",
    "        **state,\n",
    "        \"split_docs\": split_docs,\n",
    "        \"debug_info\": debug,\n",
    "    }\n",
    "\n",
    "    print(f\"[SPLIT] Total chunks produced: {len(split_docs)}\")\n",
    "\n",
    "    return new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ba6ae",
   "metadata": {},
   "source": [
    "## 7. Indexing node (embeddings + Chroma vectorstore)\n",
    "\n",
    "Now that we have a unified, chunked corpus in `state[\"split_docs\"]`, we can:\n",
    "\n",
    "1. Compute embeddings for all chunks using `IntergraxEmbeddingManager`,\n",
    "2. Generate **stable, unique IDs** for each chunk (reusing splitter metadata if available),\n",
    "3. Deduplicate IDs inside the current batch (Chroma requires unique IDs),\n",
    "4. Insert everything into the configured `IntergraxVectorstoreManager` (Chroma backend).\n",
    "\n",
    "The indexing node will:\n",
    "\n",
    "- Read `state[\"split_docs\"]`,\n",
    "- Use `embed_manager.embed_documents(docs=split_docs)` to obtain:\n",
    "  - a list/array of embeddings,\n",
    "  - a list of (possibly normalized) `Document` objects,\n",
    "- Build an `ids` list:\n",
    "  - Prefer `chunk_id` from document metadata (if present),\n",
    "  - Fallback to a combination like `parent_id#ch{index}` or `source_path#ch{index}`,\n",
    "- Call `vectorstore.add_documents(...)` with:\n",
    "  - `documents`,\n",
    "  - `embeddings`,\n",
    "  - `ids`,\n",
    "  - a shared `base_metadata` containing `tenant`, `corpus`, `version`.\n",
    "\n",
    "On success it will set:\n",
    "\n",
    "- `state[\"vectorstore_ready\"] = True`,\n",
    "- `state[\"vectorstore_collection\"] = VS_COLLECTION_NAME`,\n",
    "- Update `state[\"debug_info\"]` with:\n",
    "  - `indexed_docs_count`,\n",
    "  - optional vectorstore count (`vectorstore.count()`), if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Indexing node (embeddings + Chroma vectorstore)\n",
    "# --------------------------------------------------\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def _build_chunk_ids(docs: List[Document]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build stable, unique IDs for chunk documents based on their metadata.\n",
    "\n",
    "    Preference order:\n",
    "      1. metadata[\"chunk_id\"] (if provided by the splitter),\n",
    "      2. \"{parent_id}#ch{chunk_index:04d}\",\n",
    "      3. \"{source_path or source_name or 'doc'}#ch{chunk_index:04d}\".\n",
    "    \"\"\"\n",
    "    ids: List[str] = []\n",
    "\n",
    "    for d in docs:\n",
    "        md = d.metadata or {}\n",
    "        cid = md.get(\"chunk_id\")\n",
    "\n",
    "        if not cid:\n",
    "            parent = (\n",
    "                md.get(\"parent_id\")\n",
    "                or md.get(\"source_path\")\n",
    "                or md.get(\"source_name\")\n",
    "                or \"doc\"\n",
    "            )\n",
    "            idx = int(md.get(\"chunk_index\", 0))\n",
    "            cid = f\"{parent}#ch{idx:04d}\"\n",
    "\n",
    "        ids.append(str(cid))\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def _dedup_ids_batch(\n",
    "    ids: List[str],\n",
    "    docs: List[Document],\n",
    "    embs: List[List[float]],\n",
    ") -> tuple[list[str], list[Document], list[List[float]]]:\n",
    "    \"\"\"\n",
    "    Deduplicate IDs within the current batch.\n",
    "\n",
    "    Chroma requires unique IDs per upsert; we keep the first occurrence.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    new_ids: list[str] = []\n",
    "    new_docs: list[Document] = []\n",
    "    new_embs: list[List[float]] = []\n",
    "\n",
    "    for i, _id in enumerate(ids):\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        new_ids.append(_id)\n",
    "        new_docs.append(docs[i])\n",
    "        new_embs.append(embs[i])\n",
    "\n",
    "    return new_ids, new_docs, new_embs\n",
    "\n",
    "\n",
    "def index_corpus_node(state: HybridRagState) -> HybridRagState:\n",
    "    \"\"\"\n",
    "    LangGraph node:\n",
    "      - embeds all split_docs with IntergraxEmbeddingManager,\n",
    "      - generates stable IDs for chunks,\n",
    "      - deduplicates IDs in the batch,\n",
    "      - ingests everything into the Chroma-based IntergraxVectorstoreManager,\n",
    "      - updates state with vectorstore flags and debug info.\n",
    "    \"\"\"\n",
    "    split_docs: List[Document] = state.get(\"split_docs\", []) or []\n",
    "\n",
    "    if not split_docs:\n",
    "        print(\"[INDEX] No split_docs in state; skipping indexing.\")\n",
    "        debug = dict(state.get(\"debug_info\", {}))\n",
    "        debug.update(\n",
    "            {\n",
    "                \"indexed_docs_count\": 0,\n",
    "                \"vectorstore_collection\": VS_COLLECTION_NAME,\n",
    "            }\n",
    "        )\n",
    "        new_state: HybridRagState = {\n",
    "            **state,\n",
    "            \"vectorstore_ready\": False,\n",
    "            \"vectorstore_collection\": VS_COLLECTION_NAME,\n",
    "            \"debug_info\": debug,\n",
    "        }\n",
    "        return new_state\n",
    "\n",
    "    # 1) Embed all chunks\n",
    "    print(f\"[INDEX] Embedding {len(split_docs)} chunks...\")\n",
    "    embeddings, documents = embed_manager.embed_documents(docs=split_docs)\n",
    "\n",
    "    # 2) Build stable IDs\n",
    "    ids = _build_chunk_ids(documents)\n",
    "\n",
    "    # 3) Deduplicate IDs inside the batch\n",
    "    ids, documents, embeddings = _dedup_ids_batch(ids, documents, embeddings)\n",
    "\n",
    "    # Optional double-check / warning for duplicates\n",
    "    c = Counter(ids)\n",
    "    dups = [k for k, v in c.items() if v > 1]\n",
    "    if dups:\n",
    "        print(f\"[WARN] Duplicate IDs after dedup? count={len(dups)}\")\n",
    "\n",
    "    # 4) Base metadata for the whole corpus\n",
    "    base_metadata = {\n",
    "        \"tenant\": TENANT,\n",
    "        \"corpus\": CORPUS,\n",
    "        \"version\": VERSION,\n",
    "    }\n",
    "\n",
    "    # 5) Ingest into vectorstore\n",
    "    print(\n",
    "        f\"[INDEX] Ingesting {len(documents)} chunks into vectorstore \"\n",
    "        f\"(collection={VS_COLLECTION_NAME}, persist={VS_PERSIST_DIR})...\"\n",
    "    )\n",
    "\n",
    "    vectorstore.add_documents(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings,\n",
    "        ids=ids,\n",
    "        batch_size=128,\n",
    "        base_metadata=base_metadata,\n",
    "    )\n",
    "\n",
    "    total_count = None\n",
    "    try:\n",
    "        total_count = vectorstore.count()\n",
    "        print(f\"[INDEX] Vectorstore total count after ingest: {total_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[INDEX] Could not read vectorstore count: {e}\")\n",
    "\n",
    "    debug = dict(state.get(\"debug_info\", {}))\n",
    "    debug.update(\n",
    "        {\n",
    "            \"indexed_docs_count\": len(documents),\n",
    "            \"vectorstore_collection\": VS_COLLECTION_NAME,\n",
    "            \"vectorstore_total_count\": total_count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    new_state: HybridRagState = {\n",
    "        **state,\n",
    "        \"vectorstore_ready\": True,\n",
    "        \"vectorstore_collection\": VS_COLLECTION_NAME,\n",
    "        \"debug_info\": debug,\n",
    "    }\n",
    "\n",
    "    return new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db5311",
   "metadata": {},
   "source": [
    "## 8. Retrieval and answer generation (IntergraxRagRetriever + LLM)\n",
    "\n",
    "With the vectorstore populated, we can now:\n",
    "\n",
    "1. Retrieve the most relevant chunks for a given user question using\n",
    "   `IntergraxRagRetriever`,\n",
    "2. Build a structured prompt (system + user messages) that:\n",
    "   - injects the retrieved chunks as context,\n",
    "   - clearly instructs the model to answer *only* based on this context,\n",
    "3. Use the existing `llm_adapter` (`OpenAIChatResponsesAdapter`) to generate\n",
    "   the final answer.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "- Instantiate a global `IntergraxRagRetriever` bound to the same vectorstore\n",
    "  and embedding manager used for indexing,\n",
    "- Define a helper function that:\n",
    "  - runs retrieval with appropriate filters (`tenant`, `corpus`, `version`),\n",
    "  - returns ranked hits with scores and metadata,\n",
    "- Define a small prompt builder that turns those hits into `ChatMessage` objects,\n",
    "- Implement a LangGraph node:\n",
    "\n",
    "  `rag_answer_node(state: HybridRagState) -> HybridRagState`\n",
    "\n",
    "  which will:\n",
    "  - read `state[\"question\"]`,\n",
    "  - run retrieval,\n",
    "  - generate an answer using `llm_adapter.generate_messages(...)`,\n",
    "  - store the answer in `state[\"answer\"]`,\n",
    "  - update `state[\"debug_info\"]` with retrieval stats and a small preview\n",
    "    of the hits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d53aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Retrieval and answer generation\n",
    "# ----------------------------------\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from intergrax.rag.rag_retriever import RagRetriever\n",
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "\n",
    "# --- Global RAG retriever (same vectorstore + embeddings as indexing) ---\n",
    "\n",
    "rag_retriever = RagRetriever(\n",
    "    vectorstore,\n",
    "    embed_manager,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "def run_rag_retrieval(\n",
    "    question: str,\n",
    "    top_k: int = TOP_K,\n",
    "    score_threshold: float = 0.15,\n",
    "    max_per_parent: int = 2,\n",
    "    use_mmr: bool = True,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run RAG retrieval against the current vectorstore with standard filters.\n",
    "\n",
    "    Uses the same pattern as your standalone retriever example:\n",
    "      - filters by TENANT/CORPUS/VERSION,\n",
    "      - can use MMR,\n",
    "      - returns a list of hit dicts with:\n",
    "        - 'content' (chunk text),\n",
    "        - 'metadata',\n",
    "        - 'similarity_score',\n",
    "        - 'rank', etc. (depending on your implementation).\n",
    "    \"\"\"\n",
    "    where_filter = {\n",
    "        \"$and\": [\n",
    "            {\"tenant\": {\"$eq\": TENANT}},\n",
    "            {\"corpus\": {\"$eq\": CORPUS}},\n",
    "            {\"version\": {\"$eq\": VERSION}},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    hits = rag_retriever.retrieve(\n",
    "        question=question,\n",
    "        top_k=top_k,\n",
    "        score_threshold=score_threshold,\n",
    "        where=where_filter,\n",
    "        max_per_parent=max_per_parent,\n",
    "        use_mmr=use_mmr,\n",
    "        include_embeddings=False,\n",
    "        prefetch_factor=5,\n",
    "    )\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "def build_rag_messages(\n",
    "    question: str,\n",
    "    hits: List[Dict[str, Any]],\n",
    "    max_context_chunks: int = 8,\n",
    ") -> List[ChatMessage]:\n",
    "    \"\"\"\n",
    "    Build system + user messages for the RAG answer.\n",
    "\n",
    "    - Takes top-N retrieved chunks,\n",
    "    - Builds a context section with numbered sources,\n",
    "    - Asks the model to answer strictly based on that context.\n",
    "    \"\"\"\n",
    "    # Limit the number of chunks going into the context\n",
    "    clipped_hits = hits[:max_context_chunks]\n",
    "\n",
    "    context_parts = []\n",
    "    for i, h in enumerate(clipped_hits, start=1):\n",
    "        source_type = h.get(\"metadata\", {}).get(\"source_type\", \"unknown\")\n",
    "        source_name = (\n",
    "            h.get(\"metadata\", {}).get(\"source_path\")\n",
    "            or h.get(\"metadata\", {}).get(\"source_name\")\n",
    "            or h.get(\"metadata\", {}).get(\"source_title\")\n",
    "            or f\"chunk_{i}\"\n",
    "        )\n",
    "        context_parts.append(\n",
    "            f\"[{i}] (source_type={source_type}, source={source_name})\\n{h['content']}\"\n",
    "        )\n",
    "\n",
    "    context_block = \"\\n\\n\".join(context_parts) if context_parts else \"(no context retrieved)\"\n",
    "\n",
    "    system_content = (\n",
    "        \"You are a strict RAG assistant. \"\n",
    "        \"Answer the user's question **only** using the provided context.\\n\\n\"\n",
    "        \"If the context does not contain enough information, say that explicitly \"\n",
    "        \"and avoid guessing or hallucinating.\\n\"\n",
    "    )\n",
    "\n",
    "    user_content = (\n",
    "        \"Context from documents and web sources:\\n\"\n",
    "        \"--------------------------------------\\n\"\n",
    "        f\"{context_block}\\n\\n\"\n",
    "        \"User question:\\n\"\n",
    "        \"--------------\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"Please provide a concise but detailed answer, with clear structure \"\n",
    "        \"(short summary first, then key points).\"\n",
    "    )\n",
    "\n",
    "    messages: List[ChatMessage] = [\n",
    "        ChatMessage(role=\"system\", content=system_content),\n",
    "        ChatMessage(role=\"user\", content=user_content),\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def rag_answer_node(state: HybridRagState) -> HybridRagState:\n",
    "    \"\"\"\n",
    "    LangGraph node:\n",
    "      - runs retrieval against the hybrid vectorstore,\n",
    "      - builds a RAG prompt,\n",
    "      - generates the final answer with the LLM adapter,\n",
    "      - updates state['answer'] and state['debug_info'].\n",
    "\n",
    "    Preconditions:\n",
    "      - `index_corpus_node` has already ingested the chunks into the vectorstore.\n",
    "    \"\"\"\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    if not question:\n",
    "        print(\"[RAG ANSWER] Empty question in state; returning without answer.\")\n",
    "        debug = dict(state.get(\"debug_info\", {}))\n",
    "        debug.update({\"rag_hits_count\": 0})\n",
    "        new_state: HybridRagState = {\n",
    "            **state,\n",
    "            \"answer\": \"\",\n",
    "            \"debug_info\": debug,\n",
    "        }\n",
    "        return new_state\n",
    "\n",
    "    if not state.get(\"vectorstore_ready\", False):\n",
    "        print(\"[RAG ANSWER] Vectorstore not marked as ready; attempting retrieval anyway.\")\n",
    "\n",
    "    # 1) Retrieve relevant chunks\n",
    "    hits = run_rag_retrieval(question=question, top_k=TOP_K)\n",
    "\n",
    "    print(f\"[RAG ANSWER] Retrieved {len(hits)} hits for question.\")\n",
    "\n",
    "    # 2) Build messages for the LLM\n",
    "    messages = build_rag_messages(question, hits, max_context_chunks=TOP_K)\n",
    "\n",
    "    # 3) Generate answer via Intergrax LLM adapter\n",
    "    answer_text = llm_adapter.generate_messages(\n",
    "        messages,\n",
    "        temperature=0.1,\n",
    "        max_tokens=None,\n",
    "    )\n",
    "\n",
    "    # 4) Update debug info with stats and a small preview\n",
    "    debug = dict(state.get(\"debug_info\", {}))\n",
    "    debug.update(\n",
    "        {\n",
    "            \"rag_hits_count\": len(hits),\n",
    "            \"rag_hits_preview\": [\n",
    "                {\n",
    "                    \"rank\": h.get(\"rank\"),\n",
    "                    \"similarity_score\": h.get(\"similarity_score\"),\n",
    "                    \"source_type\": h.get(\"metadata\", {}).get(\"source_type\"),\n",
    "                    \"source\": (\n",
    "                        h.get(\"metadata\", {}).get(\"source_path\")\n",
    "                        or h.get(\"metadata\", {}).get(\"source_name\")\n",
    "                        or h.get(\"metadata\", {}).get(\"source_title\")\n",
    "                    ),\n",
    "                }\n",
    "                for h in hits[:3]\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    new_state: HybridRagState = {\n",
    "        **state,\n",
    "        \"answer\": answer_text,\n",
    "        \"debug_info\": debug,\n",
    "    }\n",
    "\n",
    "    return new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31fa4d",
   "metadata": {},
   "source": [
    "## 9. Building the LangGraph pipeline\n",
    "\n",
    "Now that we have all the building blocks as standalone functions and nodes:\n",
    "\n",
    "- `load_local_docs_node(state)`,\n",
    "- `load_web_docs_node(state)`,\n",
    "- `split_corpus_node(state)`,\n",
    "- `index_corpus_node(state)`,\n",
    "- `rag_answer_node(state)`,\n",
    "\n",
    "we can combine them into a single `StateGraph[HybridRagState]`:\n",
    "\n",
    "1. The graph entry point will be `load_local_docs`,\n",
    "2. Then we invoke `load_web_docs` (async node),\n",
    "3. Then `split_corpus` to build chunked documents,\n",
    "4. Then `index_corpus` to embed and store chunks in the vectorstore,\n",
    "5. Finally `rag_answer` to run retrieval and generate the final answer.\n",
    "\n",
    "Because one of the nodes is async (`load_web_docs_node`), the compiled graph\n",
    "will be executed via `graph.ainvoke(...)` and the top-level helper\n",
    "`run_hybrid_rag(question: str)` will therefore be async as well.\n",
    "\n",
    "For convenience, we also provide a small synchronous wrapper using `asyncio.run(...)`\n",
    "for environments that do not support `await` at the top level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb92727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Building the LangGraph pipeline\n",
    "# ----------------------------------\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Dict, Any\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# --- Build the graph ---\n",
    "\n",
    "graph_builder = StateGraph(HybridRagState)\n",
    "\n",
    "# Register nodes\n",
    "graph_builder.add_node(\"load_local_docs\", load_local_docs_node)\n",
    "graph_builder.add_node(\"load_web_docs\", load_web_docs_node)\n",
    "graph_builder.add_node(\"split_corpus\", split_corpus_node)\n",
    "graph_builder.add_node(\"index_corpus\", index_corpus_node)\n",
    "graph_builder.add_node(\"rag_answer\", rag_answer_node)\n",
    "\n",
    "# Entry point\n",
    "graph_builder.set_entry_point(\"load_local_docs\")\n",
    "\n",
    "# Edges: linear flow for the basic demo\n",
    "graph_builder.add_edge(\"load_local_docs\", \"load_web_docs\")\n",
    "graph_builder.add_edge(\"load_web_docs\", \"split_corpus\")\n",
    "graph_builder.add_edge(\"split_corpus\", \"index_corpus\")\n",
    "graph_builder.add_edge(\"index_corpus\", \"rag_answer\")\n",
    "graph_builder.add_edge(\"rag_answer\", END)\n",
    "\n",
    "# Compile the graph (async-capable, because we have async nodes)\n",
    "hybrid_rag_graph = graph_builder.compile()\n",
    "\n",
    "print(\"Hybrid multi-source RAG graph compiled.\")\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(hybrid_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd59593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Helper: async runner for a single question\n",
    "# ----------------------------------------------\n",
    "\n",
    "async def run_hybrid_rag_async(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run the full hybrid RAG pipeline for a single question (async).\n",
    "\n",
    "    Steps:\n",
    "      - initialize HybridRagState with the question,\n",
    "      - run the LangGraph pipeline,\n",
    "      - return question, answer and debug_info.\n",
    "    \"\"\"\n",
    "    initial_state: HybridRagState = {\n",
    "        \"question\": question,\n",
    "        \"pdf_docs\": [],\n",
    "        \"docx_docs\": [],\n",
    "        \"web_docs_serialized\": [],\n",
    "        \"split_docs\": [],\n",
    "        \"vectorstore_ready\": False,\n",
    "        \"vectorstore_collection\": VS_COLLECTION_NAME,\n",
    "        \"answer\": \"\",\n",
    "        \"debug_info\": {},\n",
    "    }\n",
    "\n",
    "    result_state = await hybrid_rag_graph.ainvoke(initial_state)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": result_state.get(\"answer\", \"\"),\n",
    "        \"debug_info\": result_state.get(\"debug_info\", {}),\n",
    "    }\n",
    "\n",
    "\n",
    "# 9.2 Helper: sync wrapper for environments without top-level await\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def run_hybrid_rag(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Synchronous convenience wrapper around `run_hybrid_rag_async`.\n",
    "\n",
    "    Useful in plain Python scripts or notebooks that do not support\n",
    "    top-level `await`.\n",
    "    \"\"\"\n",
    "    return asyncio.run(run_hybrid_rag_async(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 Example usage\n",
    "# -----------------\n",
    "\n",
    "example_question = (\n",
    "    \"Summarize the key ideas from my local documents and recent web sources \"\n",
    "    \"about AI Agents in ERP/CRM software, and propose concrete next steps \"\n",
    "    \"to make this pipeline production-ready.\"\n",
    ")\n",
    "\n",
    "result = await run_hybrid_rag_async(example_question)\n",
    "\n",
    "print(\"=== QUESTION ===\")\n",
    "print(result[\"question\"])\n",
    "print(\"\\n=== ANSWER ===\")\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\n=== DEBUG INFO ===\")\n",
    "for key, value in result[\"debug_info\"].items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
