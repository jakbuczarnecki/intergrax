{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc9419d",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f53e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5134efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intergrax.llm.messages import ChatMessage\n",
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_planner import EnginePlanner\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Global test constants\n",
    "# -------------------------------------------------------\n",
    "\n",
    "USER_ID = \"demo-user-planner\"\n",
    "SESSION_ID = \"sess_planner_only_001\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Build LLM adapter and RuntimeConfig\n",
    "# IMPORTANT: Replace build_llm_adapter() with your actual builder.\n",
    "# This must be the same adapter/config used in your runtime.\n",
    "# -------------------------------------------------------\n",
    "\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112938c",
   "metadata": {},
   "source": [
    "# Test helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646ad6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import (\n",
    "    PlanIntent,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) TEST SET: question + expected intent (+ optional flags)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PlannerTestCase:\n",
    "    id: str\n",
    "    question: str\n",
    "    expected_intent: PlanIntent\n",
    "    expected_flags: Optional[Dict[str, Optional[bool]]] = None\n",
    "    use_project_context: bool = False\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PromptVariant:\n",
    "    id: str\n",
    "    system_prompt: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe9b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_state import RuntimeState\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import (\n",
    "    EnginePlan,\n",
    "    PlannerPromptConfig,\n",
    ")\n",
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "from intergrax.llm_adapters.llm_usage_track import LLMUsageTracker\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import BASE_PLANNER_SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "def build_planner_request(\n",
    "    *,\n",
    "    user_id: str,\n",
    "    session_id: str,\n",
    "    message: str,\n",
    "    instructions: str | None = None,\n",
    "    attachments: list | None = None,\n",
    ") -> RuntimeRequest:\n",
    "    return RuntimeRequest(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        message=message,\n",
    "        attachments=attachments or [],\n",
    "        instructions=instructions,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_planner_state(\n",
    "    *,\n",
    "    req: RuntimeRequest,\n",
    "    run_id: str,\n",
    "    config: RuntimeConfig,\n",
    "    base_history: str,\n",
    ") -> RuntimeState:\n",
    "    \"\"\"\n",
    "    Minimal RuntimeState for planner-only tests.\n",
    "    Uses only existing RuntimeState fields (no extra structures).\n",
    "    \"\"\"\n",
    "    state = RuntimeState(\n",
    "        request=req,\n",
    "        run_id=run_id,\n",
    "        llm_usage_tracker=LLMUsageTracker(run_id=run_id),\n",
    "    )\n",
    "\n",
    "    # Keep it runtime-like: register the same adapter label as core runtime does.\n",
    "    state.llm_usage_tracker.register_adapter(config.llm_adapter, label=\"core_adapter\")\n",
    "\n",
    "    # Base history for planner tests (string -> ChatMessage list)\n",
    "    if base_history and base_history.strip():\n",
    "        state.base_history = [ChatMessage(role=\"user\", content=base_history.strip())]\n",
    "    else:\n",
    "        state.base_history = []\n",
    "\n",
    "    # Optional: simulate memory-layer instruction fragments (keep None for now)\n",
    "    state.profile_user_instructions = None\n",
    "    state.profile_org_instructions = None\n",
    "\n",
    "    # Capabilities — explicitly set for planner tests\n",
    "    state.cap_rag_available = bool(config.enable_rag)\n",
    "    state.cap_user_ltm_available = bool(config.enable_user_longterm_memory)\n",
    "    state.cap_attachments_available = True\n",
    "    state.cap_websearch_available = bool(config.enable_websearch)\n",
    "    state.cap_tools_available = bool(config.tools_mode != \"off\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def _extract_plan_flags(plan: EnginePlan) -> Dict[str, bool | None]:\n",
    "    \"\"\"\n",
    "    Extract only the flags we test. No getattr: rely on EnginePlan contract.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"use_websearch\": plan.use_websearch,\n",
    "        \"use_tools\": plan.use_tools,\n",
    "        \"use_rag\": plan.use_rag,\n",
    "        \"use_user_longterm_memory\": plan.use_user_longterm_memory,\n",
    "        \"ask_clarifying_question\": plan.ask_clarifying_question,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "async def run_planner_testcase(\n",
    "    *,\n",
    "    tc: PlannerTestCase,\n",
    "    system_prompt: str,\n",
    "    base_history: str,\n",
    "    planner: EnginePlanner,\n",
    "    config: RuntimeConfig,\n",
    "    user_id: str = \"test-user\",\n",
    "    session_id: str = \"test-session\",\n",
    "    run_id: str,\n",
    "    instructions: str | None = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a single PlannerTestCase and return a structured result.\n",
    "\n",
    "    Result separates intent vs flags validation to make PASS/FAIL reasons explicit.\n",
    "    \"\"\"\n",
    "    req = build_planner_request(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        message=tc.question,\n",
    "        instructions=instructions,\n",
    "    )\n",
    "\n",
    "    state = build_planner_state(\n",
    "        req=req,\n",
    "        run_id=run_id,\n",
    "        config=config,\n",
    "        base_history=base_history,\n",
    "    )\n",
    "\n",
    "    prompt_cfg = PlannerPromptConfig(version=\"notebook\", system_prompt=system_prompt)\n",
    "\n",
    "    try:\n",
    "        plan = await planner.plan(\n",
    "            req=req,\n",
    "            state=state,\n",
    "            config=config,\n",
    "            prompt_config=prompt_cfg,\n",
    "            run_id=run_id,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Treat planner parsing/model-contract failures as a failed testcase\n",
    "        return {\n",
    "            \"id\": tc.id,\n",
    "            \"pass\": False,\n",
    "            \"intent_ok\": False,\n",
    "            \"flags_ok\": False,\n",
    "            \"intent_errors\": [f\"planner_error: {type(e).__name__}: {e}\"],\n",
    "            \"flag_errors\": [],\n",
    "            \"errors\": [f\"planner_error: {type(e).__name__}: {e}\"],\n",
    "            \"expected_intent\": tc.expected_intent,\n",
    "            \"got_intent\": None,\n",
    "            \"got_flags\": {},\n",
    "            \"debug\": {\n",
    "                \"json_len\": None,\n",
    "                \"raw_preview\": \"\",\n",
    "                \"prompt_version\": \"notebook\",\n",
    "                \"used_override\": True,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    intent_errors: List[str] = []\n",
    "    flag_errors: List[str] = []\n",
    "\n",
    "    # 1) Intent validation\n",
    "    if plan.intent != tc.expected_intent:\n",
    "        intent_errors.append(f\"intent expected={tc.expected_intent} got={plan.intent}\")\n",
    "\n",
    "    # 2) Flags validation (only if expected_flags provided)\n",
    "    if tc.expected_flags:\n",
    "        got_flags = _extract_plan_flags(plan)\n",
    "        for k, expected in tc.expected_flags.items():\n",
    "            if expected is None:\n",
    "                continue\n",
    "            if got_flags[k] != expected:\n",
    "                flag_errors.append(f\"{k} expected={expected} got={got_flags[k]}\")\n",
    "    else:\n",
    "        got_flags = _extract_plan_flags(plan)\n",
    "\n",
    "    errors = intent_errors + flag_errors\n",
    "\n",
    "    intent_ok = len(intent_errors) == 0\n",
    "    flags_ok = len(flag_errors) == 0\n",
    "    passed = intent_ok and flags_ok\n",
    "\n",
    "    # Minimal debug summary (avoid dumping full raw content in loops)\n",
    "    dbg = plan.debug\n",
    "    raw_preview = \"\"\n",
    "    if \"raw_preview\" in dbg and dbg[\"raw_preview\"]:\n",
    "        raw_preview = dbg[\"raw_preview\"][:220]\n",
    "\n",
    "    return {\n",
    "        \"id\": tc.id,\n",
    "        \"pass\": passed,\n",
    "        \"intent_ok\": intent_ok,\n",
    "        \"flags_ok\": flags_ok,\n",
    "        \"intent_errors\": intent_errors,\n",
    "        \"flag_errors\": flag_errors,\n",
    "        \"errors\": errors,\n",
    "        \"expected_intent\": tc.expected_intent,\n",
    "        \"got_intent\": plan.intent,\n",
    "        \"got_flags\": got_flags,\n",
    "        \"debug\": {\n",
    "            \"json_len\": dbg[\"json_len\"] if \"json_len\" in dbg else None,\n",
    "            \"raw_preview\": raw_preview,\n",
    "            \"prompt_version\": dbg[\"planner_prompt\"][\"version\"] if \"planner_prompt\" in dbg else None,\n",
    "            \"used_override\": dbg[\"planner_prompt\"][\"used_override\"] if \"planner_prompt\" in dbg else None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def print_test_result(test: PlannerTestCase, res: Dict[str, Any]) -> None:\n",
    "    status = \"PASS\" if res[\"pass\"] else \"FAIL\"\n",
    "\n",
    "    intent_status = \"OK\" if res[\"intent_ok\"] else \"FAIL\"\n",
    "    flags_status = \"OK\" if res[\"flags_ok\"] else \"FAIL\"\n",
    "\n",
    "    print(f\"[{res['id']}] {status}  (INTENT={intent_status}, FLAGS={flags_status})\")\n",
    "    print(f\"  question        : {test.question}\")\n",
    "    print(f\"  project_history : {test.use_project_context}\")\n",
    "    print(f\"  expected_intent : {res['expected_intent']}\")\n",
    "    print(f\"  got_intent      : {res['got_intent']}\")\n",
    "    print(f\"  got_flags       : {res['got_flags']}\")\n",
    "\n",
    "    if not res[\"intent_ok\"]:\n",
    "        for e in res[\"intent_errors\"]:\n",
    "            print(f\"  [intent] {e}\")\n",
    "\n",
    "    if not res[\"flags_ok\"]:\n",
    "        for e in res[\"flag_errors\"]:\n",
    "            print(f\"  [flags] {e}\")\n",
    "\n",
    "    if res[\"debug\"][\"raw_preview\"]:\n",
    "        print(f\"  raw_preview: {res['debug']['raw_preview']}\")\n",
    "\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea88fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS: List[PlannerTestCase] = [\n",
    "    # ------------------------------------------------------------------\n",
    "    # GENERIC\n",
    "    # ------------------------------------------------------------------\n",
    "    PlannerTestCase(\n",
    "        id=\"G01\",\n",
    "        question=\"Explain how to implement an async retry strategy in Python for API calls.\",\n",
    "        expected_intent=PlanIntent.GENERIC,\n",
    "        expected_flags={\"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"G02\",\n",
    "        question=\"What is the difference between a mutex and a semaphore? Give a concise explanation.\",\n",
    "        expected_intent=PlanIntent.GENERIC,\n",
    "        expected_flags={\"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"G03\",\n",
    "        question=\"Show an example of exponential backoff with jitter in Python (pseudocode is fine).\",\n",
    "        expected_intent=PlanIntent.GENERIC,\n",
    "        expected_flags={\"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"G04\",\n",
    "        question=\"Explain the trade-offs between REST and GraphQL for a backend API.\",\n",
    "        expected_intent=PlanIntent.GENERIC,\n",
    "        expected_flags={\"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    # Edge: mentions \"latest\" but still could be generic if framed as concept\n",
    "    PlannerTestCase(\n",
    "        id=\"G05\",\n",
    "        question=\"In general terms, what does 'tool calling' mean in modern LLM APIs?\",\n",
    "        expected_intent=PlanIntent.GENERIC,\n",
    "        expected_flags={\"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # FRESHNESS (must require up-to-date info)\n",
    "    # ------------------------------------------------------------------\n",
    "    PlannerTestCase(\n",
    "        id=\"F01\",\n",
    "        question=\"What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\",\n",
    "        expected_intent=PlanIntent.FRESHNESS,\n",
    "        expected_flags={\"use_websearch\": True, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"F02\",\n",
    "        question=\"Summarize the latest release notes for Python 3.13 and mention the release date.\",\n",
    "        expected_intent=PlanIntent.FRESHNESS,\n",
    "        expected_flags={\"use_websearch\": True, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"F03\",\n",
    "        question=\"What changed in the last 30 days in LangChain regarding agents? Provide a short dated summary.\",\n",
    "        expected_intent=PlanIntent.FRESHNESS,\n",
    "        expected_flags={\"use_websearch\": True, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    # Edge: explicit \"as of today\" + dates\n",
    "    PlannerTestCase(\n",
    "        id=\"F04\",\n",
    "        question=\"As of today, what is the current stable version of FastAPI and when was it released?\",\n",
    "        expected_intent=PlanIntent.FRESHNESS,\n",
    "        expected_flags={\"use_websearch\": True, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    # Edge: asks for \"verify\" => still websearch, not tools\n",
    "    PlannerTestCase(\n",
    "        id=\"F05\",\n",
    "        question=\"Verify the current Docker Compose file format version recommendation and cite the source date.\",\n",
    "        expected_intent=PlanIntent.FRESHNESS,\n",
    "        expected_flags={\"use_websearch\": True, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # PROJECT_ARCHITECTURE (depends on project context/preferences)\n",
    "    # Note: these are only meaningful with BASE_HISTORY_PROJECT or other injected context.\n",
    "    # ------------------------------------------------------------------\n",
    "    PlannerTestCase(\n",
    "        id=\"P01\",\n",
    "        question=\"Continue earlier: update the Step Planner architecture so it supports dynamic planning.\",\n",
    "        expected_intent=PlanIntent.PROJECT_ARCHITECTURE,\n",
    "        expected_flags={\"use_user_longterm_memory\": True, \"use_websearch\": False, \"use_tools\": False},\n",
    "        use_project_context=True,\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"P02\",\n",
    "        question=\"Given our Intergrax Drop-In Knowledge Runtime, where should we enforce planner canonicalization rules: in EnginePlanner or in Runtime?\",\n",
    "        expected_intent=PlanIntent.PROJECT_ARCHITECTURE,\n",
    "        expected_flags={\"use_user_longterm_memory\": True, \"use_websearch\": False, \"use_tools\": False},\n",
    "        use_project_context=True,\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"P03\",\n",
    "        question=\"For Intergrax, should we treat websearch as tools or keep it as a separate pipeline? Decide based on our existing architecture.\",\n",
    "        expected_intent=PlanIntent.PROJECT_ARCHITECTURE,\n",
    "        expected_flags={\"use_user_longterm_memory\": True, \"use_websearch\": False, \"use_tools\": False},\n",
    "        use_project_context=True,\n",
    "    ),\n",
    "    # Edge: project wording without context should not always trigger project_architecture,\n",
    "    # but with BASE_HISTORY_PROJECT it should.\n",
    "    PlannerTestCase(\n",
    "        id=\"P04\",\n",
    "        question=\"In our current notebook-based test approach, what is the best way to version planner prompts and track regressions?\",\n",
    "        expected_intent=PlanIntent.PROJECT_ARCHITECTURE,\n",
    "        expected_flags={\"use_user_longterm_memory\": True, \"use_websearch\": False, \"use_tools\": False},\n",
    "        use_project_context=True,\n",
    "    ),\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # CLARIFY (ambiguous/missing required info; must ask one clarifying question)\n",
    "    # ------------------------------------------------------------------\n",
    "    PlannerTestCase(\n",
    "        id=\"C01\",\n",
    "        question=\"Which one should I choose for my project?\",\n",
    "        expected_intent=PlanIntent.CLARIFY,\n",
    "        expected_flags={\"ask_clarifying_question\": True, \"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"C02\",\n",
    "        question=\"Help me optimize this. What should I change?\",\n",
    "        expected_intent=PlanIntent.CLARIFY,\n",
    "        expected_flags={\"ask_clarifying_question\": True, \"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"C03\",\n",
    "        question=\"How should we do it in Intergrax?\",\n",
    "        expected_intent=PlanIntent.CLARIFY,\n",
    "        expected_flags={\"ask_clarifying_question\": True, \"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    # Edge: contains \"project\" but still missing options/criteria -> clarify\n",
    "    PlannerTestCase(\n",
    "        id=\"C04\",\n",
    "        question=\"For my project, should I pick the first approach or the second one?\",\n",
    "        expected_intent=PlanIntent.CLARIFY,\n",
    "        expected_flags={\"ask_clarifying_question\": True, \"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "    # Edge: ambiguous target (library? language? platform?)\n",
    "    PlannerTestCase(\n",
    "        id=\"C05\",\n",
    "        question=\"Can you compare them and tell me what's better?\",\n",
    "        expected_intent=PlanIntent.CLARIFY,\n",
    "        expected_flags={\"ask_clarifying_question\": True, \"use_websearch\": False, \"use_tools\": False, \"use_user_longterm_memory\": False},\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391a508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_HISTORY_EMPTY: str = \"\"\n",
    "\n",
    "BASE_HISTORY_PROJECT: str = (\n",
    "    \"User: I am Artur. I build Intergrax and Mooff.\\n\"\n",
    "    \"User: We are working on Intergrax Drop-In Knowledge Runtime.\\n\"\n",
    "    \"User: We are implementing a Step Planner with modes: none/static/dynamic.\\n\"\n",
    "    \"User: Observed issues: LTM always-on in generic; websearch vs tools conflation.\\n\"\n",
    "    \"User: I prefer concise, technical answers. Never use emojis in code/docs.\\n\"\n",
    ")\n",
    "\n",
    "# async def run_all(*, system_prompt_variant: str):\n",
    "#     from tqdm.auto import tqdm\n",
    "\n",
    "#     llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "#     config = RuntimeConfig(\n",
    "#         llm_adapter=llm_adapter,\n",
    "#         enable_rag=True,\n",
    "#         enable_websearch=True,\n",
    "#         tools_mode=\"auto\",\n",
    "#         enable_user_longterm_memory=True,\n",
    "#     )\n",
    "\n",
    "#     planner = EnginePlanner(llm_adapter=llm_adapter)\n",
    "\n",
    "#     for test in tqdm(TESTS, desc=\"Planner tests\"):\n",
    "#         base_history = BASE_HISTORY_PROJECT if test.use_project_context else BASE_HISTORY_EMPTY\n",
    "\n",
    "#         res = await run_planner_testcase(\n",
    "#             tc=test,\n",
    "#             system_prompt=system_prompt_variant,\n",
    "#             base_history=base_history,\n",
    "#             planner=planner,\n",
    "#             config=config,\n",
    "#             run_id=test.id,\n",
    "#         )\n",
    "\n",
    "#         print_test_result(test, res)\n",
    "\n",
    "\n",
    "async def run_all(\n",
    "    *,\n",
    "    system_prompt_variant: str,\n",
    "    verbose: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "    config = RuntimeConfig(\n",
    "        llm_adapter=llm_adapter,\n",
    "        enable_rag=True,\n",
    "        enable_websearch=True,\n",
    "        tools_mode=\"auto\",\n",
    "        enable_user_longterm_memory=True,\n",
    "    )\n",
    "\n",
    "    planner = EnginePlanner(llm_adapter=llm_adapter)\n",
    "\n",
    "    per_test: List[Dict[str, Any]] = []\n",
    "\n",
    "    totals = {\n",
    "        \"total\": 0,\n",
    "        \"pass\": 0,\n",
    "        \"intent_ok\": 0,\n",
    "        \"flags_ok\": 0,\n",
    "    }\n",
    "\n",
    "    per_group: Dict[str, Dict[str, int]] = {}\n",
    "    fails_compact: List[Dict[str, Any]] = []\n",
    "\n",
    "    for test in tqdm(TESTS, desc=\"Planner tests\"):\n",
    "        base_history = BASE_HISTORY_PROJECT if test.use_project_context else BASE_HISTORY_EMPTY\n",
    "\n",
    "        res = await run_planner_testcase(\n",
    "            tc=test,\n",
    "            system_prompt=system_prompt_variant,\n",
    "            base_history=base_history,\n",
    "            planner=planner,\n",
    "            config=config,\n",
    "            run_id=test.id,\n",
    "        )\n",
    "\n",
    "        per_test.append(res)\n",
    "\n",
    "        # Totals\n",
    "        totals[\"total\"] += 1\n",
    "        if res[\"pass\"]:\n",
    "            totals[\"pass\"] += 1\n",
    "        if res[\"intent_ok\"]:\n",
    "            totals[\"intent_ok\"] += 1\n",
    "        if res[\"flags_ok\"]:\n",
    "            totals[\"flags_ok\"] += 1\n",
    "\n",
    "        # Group breakdown (G/F/P/C)\n",
    "        group = test.id[0]\n",
    "        if group not in per_group:\n",
    "            per_group[group] = {\"total\": 0, \"pass\": 0, \"intent_ok\": 0, \"flags_ok\": 0}\n",
    "        per_group[group][\"total\"] += 1\n",
    "        if res[\"pass\"]:\n",
    "            per_group[group][\"pass\"] += 1\n",
    "        if res[\"intent_ok\"]:\n",
    "            per_group[group][\"intent_ok\"] += 1\n",
    "        if res[\"flags_ok\"]:\n",
    "            per_group[group][\"flags_ok\"] += 1\n",
    "\n",
    "        # Compact fail diagnostics\n",
    "        if not res[\"pass\"]:\n",
    "            fails_compact.append(\n",
    "                {\n",
    "                    \"test_id\": res[\"id\"],\n",
    "                    \"group\": group,\n",
    "                    \"intent_ok\": res[\"intent_ok\"],\n",
    "                    \"flags_ok\": res[\"flags_ok\"],\n",
    "                    \"intent_errors\": res[\"intent_errors\"],\n",
    "                    \"flag_errors\": res[\"flag_errors\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if verbose:\n",
    "            print_test_result(test, res)\n",
    "\n",
    "    def pct(x: int, denom: int) -> float:\n",
    "        if denom <= 0:\n",
    "            return 0.0\n",
    "        return (100.0 * float(x)) / float(denom)\n",
    "\n",
    "    summary = {\n",
    "        \"totals\": totals,\n",
    "        \"accuracy\": pct(totals[\"pass\"], totals[\"total\"]),\n",
    "        \"intent_accuracy\": pct(totals[\"intent_ok\"], totals[\"total\"]),\n",
    "        \"flags_accuracy\": pct(totals[\"flags_ok\"], totals[\"total\"]),\n",
    "        \"per_group\": {\n",
    "            g: {\n",
    "                **st,\n",
    "                \"accuracy\": pct(st[\"pass\"], st[\"total\"]),\n",
    "                \"intent_accuracy\": pct(st[\"intent_ok\"], st[\"total\"]),\n",
    "                \"flags_accuracy\": pct(st[\"flags_ok\"], st[\"total\"]),\n",
    "            }\n",
    "            for g, st in per_group.items()\n",
    "        },\n",
    "        \"fails\": fails_compact,\n",
    "        # Keep full per_test only if you need deep drill-down later\n",
    "        \"per_test\": per_test,\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "async def run_prompt_variants(\n",
    "    *,\n",
    "    variants: List[PromptVariant],\n",
    "    verbose_each: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    for v in variants:\n",
    "        stats = await run_all(system_prompt_variant=v.system_prompt, verbose=verbose_each)\n",
    "        results.append(\n",
    "            {\n",
    "                \"variant_id\": v.id,\n",
    "                \"accuracy\": stats[\"accuracy\"],\n",
    "                \"intent_accuracy\": stats[\"intent_accuracy\"],\n",
    "                \"flags_accuracy\": stats[\"flags_accuracy\"],\n",
    "                \"stats\": stats,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort: overall pass rate, then flags, then intent\n",
    "    results.sort(\n",
    "        key=lambda r: (r[\"accuracy\"], r[\"flags_accuracy\"], r[\"intent_accuracy\"]),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25da86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt_variant = BASE_PLANNER_SYSTEM_PROMPT + \"\"\"\n",
    "\n",
    "# User long-term memory policy (STRICT, HARD RULE):\n",
    "# - use_user_longterm_memory MUST be true ONLY when intent is exactly 'project_architecture' AND the capability is available.\n",
    "# - For intents 'generic', 'freshness', and 'clarify': use_user_longterm_memory MUST be false.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# await run_all(system_prompt_variant=system_prompt_variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac936881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt_variant = BASE_PLANNER_SYSTEM_PROMPT + \"\"\"\n",
    "\n",
    "# User long-term memory policy (STRICT, HARD RULE):\n",
    "# - use_user_longterm_memory MUST be true ONLY when intent is exactly 'project_architecture' AND the capability is available.\n",
    "# - For intents 'generic', 'freshness', and 'clarify': use_user_longterm_memory MUST be false.\n",
    "\n",
    "# Websearch vs Tools policy (STRICT, HARD RULE):\n",
    "# - In this runtime, websearch is NOT part of tools. Websearch is a separate pipeline.\n",
    "# - Therefore, if use_websearch=true, then use_tools MUST be false.\n",
    "# - Set use_tools=true ONLY for non-websearch external actions handled by the tools pipeline.\n",
    "# \"\"\"\n",
    "\n",
    "# await run_all(system_prompt_variant=system_prompt_variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3bc3992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06aecc11e3f3461f9ca0bd4f785aaf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planner tests:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f390f09c4d4ef4864a9392a3b005bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planner tests:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8db73c49dc454bb97631a0e035ba6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planner tests:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5eb8e791a74db4b2119c7b5f8e52de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planner tests:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltm_strict_web_not_tools + clarify_strict + enum_guard + project architecture: acc=89.5% (intent=100.0%, flags=89.5%)\n",
      "ltm_strict_web_not_tools + clarify_strict + enum_guard: acc=84.2% (intent=89.5%, flags=84.2%)\n",
      "ltm_strict_web_not_tools: acc=73.7% (intent=73.7%, flags=73.7%)\n",
      "ltm_strict: acc=63.2% (intent=73.7%, flags=63.2%)\n"
     ]
    }
   ],
   "source": [
    "variants = [\n",
    "    # PromptVariant(id=\"base\", system_prompt=BASE_PLANNER_SYSTEM_PROMPT),\n",
    "\n",
    "    PromptVariant(id=\"ltm_strict\",system_prompt=BASE_PLANNER_SYSTEM_PROMPT + \"\"\"\n",
    "        User long-term memory policy (STRICT, HARD RULE):\n",
    "        - use_user_longterm_memory MUST be true ONLY when intent is exactly 'project_architecture' AND the capability is available.\n",
    "        - For intents 'generic', 'freshness', and 'clarify': use_user_longterm_memory MUST be false.\n",
    "        \"\"\"),\n",
    "\n",
    "    PromptVariant(id=\"ltm_strict_web_not_tools\", system_prompt=BASE_PLANNER_SYSTEM_PROMPT + \"\"\"\n",
    "        User long-term memory policy (STRICT, HARD RULE):\n",
    "        - use_user_longterm_memory MUST be true ONLY when intent is exactly 'project_architecture' AND the capability is available.\n",
    "        - For intents 'generic', 'freshness', and 'clarify': use_user_longterm_memory MUST be false.\n",
    "\n",
    "        Websearch vs Tools policy (STRICT, HARD RULE):\n",
    "        - In this runtime, websearch is NOT part of tools. Websearch is a separate pipeline.\n",
    "        - Therefore, if use_websearch=true, then use_tools MUST be false.\n",
    "        - Set use_tools=true ONLY for non-websearch external actions handled by the tools pipeline.\n",
    "        \"\"\",\n",
    "    ),\n",
    "\n",
    "    PromptVariant(id=\"ltm_strict_web_not_tools + clarify_strict + enum_guard\", system_prompt = BASE_PLANNER_SYSTEM_PROMPT + \"\"\"\n",
    "        Intent field constraints (STRICT):\n",
    "        - intent MUST be EXACTLY one of: \"generic\", \"freshness\", \"project_architecture\", \"clarify\".\n",
    "        - Do NOT output any other intent value (e.g., \"compare\", \"choose\", \"optimize\", \"decision\").\n",
    "\n",
    "        User long-term memory policy (STRICT, HARD RULE):\n",
    "        - use_user_longterm_memory MUST be true ONLY when intent is exactly \"project_architecture\" AND the capability is available.\n",
    "        - For intents \"generic\", \"freshness\", and \"clarify\": use_user_longterm_memory MUST be false.\n",
    "\n",
    "        Websearch vs Tools policy (STRICT, HARD RULE):\n",
    "        - In this runtime, websearch is NOT part of tools. Websearch is a separate pipeline.\n",
    "        - Therefore, if use_websearch=true, then use_tools MUST be false.\n",
    "        - Set use_tools=true ONLY for non-websearch external actions handled by the tools pipeline.\n",
    "\n",
    "        Clarify policy (STRICT, HARD RULE):\n",
    "        - Use intent=\"clarify\" when the user's request is ambiguous or missing required details to answer.\n",
    "        - Triggers: the question asks to choose/compare/optimize (\"which one\", \"compare\", \"what should I choose\", \"what should I change\"),\n",
    "        but the options and/or decision criteria are not provided.\n",
    "        - If intent=\"clarify\": ask_clarifying_question MUST be true and clarifying_question MUST be exactly one question.\n",
    "        - If intent!=\"clarify\": ask_clarifying_question MUST be false and clarifying_question MUST be null.\n",
    "        \"\"\"\n",
    "        ),\n",
    "\n",
    "    PromptVariant(id=\"ltm_strict_web_not_tools + clarify_strict + enum_guard + project architecture\", system_prompt = BASE_PLANNER_SYSTEM_PROMPT + \"\"\"\n",
    "\n",
    "Intent field constraints (STRICT):\n",
    "- intent MUST be EXACTLY one of: \"generic\", \"freshness\", \"project_architecture\", \"clarify\".\n",
    "- Do NOT output any other intent value (e.g., \"compare\", \"choose\", \"optimize\", \"decision\").\n",
    "\n",
    "User long-term memory policy (STRICT, HARD RULE):\n",
    "- use_user_longterm_memory MUST be true ONLY when intent is exactly \"project_architecture\" AND the capability is available.\n",
    "- For intents \"generic\", \"freshness\", and \"clarify\": use_user_longterm_memory MUST be false.\n",
    "\n",
    "Websearch vs Tools policy (STRICT, HARD RULE):\n",
    "- In this runtime, websearch is NOT part of tools. Websearch is a separate pipeline.\n",
    "- Therefore, if use_websearch=true, then use_tools MUST be false.\n",
    "- Set use_tools=true ONLY for non-websearch external actions handled by the tools pipeline.\n",
    "\n",
    "Clarify policy (STRICT, HARD RULE):\n",
    "- Use intent=\"clarify\" when the user's request is ambiguous or missing required details to answer.\n",
    "- Triggers: the question asks to choose/compare/optimize (\"which one\", \"compare\", \"what should I choose\", \"what should I change\"),\n",
    "  but the options and/or decision criteria are not provided.\n",
    "- If intent=\"clarify\": ask_clarifying_question MUST be true and clarifying_question MUST be exactly one question.\n",
    "- If intent!=\"clarify\": ask_clarifying_question MUST be false and clarifying_question MUST be null.\n",
    "\n",
    "Project architecture trigger (STRICT):\n",
    "- If the question explicitly references the user's project/system by name (e.g., \"Intergrax\", \"Mooff\", \"our runtime\", \"our architecture\", \"in our codebase\"),\n",
    "  intent MUST be \"project_architecture\".\n",
    "\n",
    "Clarify priority (STRICT):\n",
    "- If the request is underspecified and refers to an unspecified \"it/this/that\" or missing object,\n",
    "  intent MUST be \"clarify\" even if the project name is mentioned.\n",
    "- Example: \"How should we do it in Intergrax?\" -> intent=\"clarify\" (ask what \"it\" refers to).\n",
    "\"\"\"\n",
    "        ),\n",
    "]\n",
    "\n",
    "results = await run_prompt_variants(variants=variants, verbose_each=False)\n",
    "\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"{r['variant_id']}: \"\n",
    "        f\"acc={r['accuracy']:.1f}% \"\n",
    "        f\"(intent={r['intent_accuracy']:.1f}%, flags={r['flags_accuracy']:.1f}%)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988dd168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fails: 2\n",
      "- F01 group=F intent_ok=True flags_ok=False\n",
      "  flags : use_tools expected=False got=True\n",
      "- F05 group=F intent_ok=True flags_ok=False\n",
      "  flags : use_tools expected=False got=True\n"
     ]
    }
   ],
   "source": [
    "best = results[0][\"stats\"]\n",
    "print(\"Fails:\", len(best[\"fails\"]))\n",
    "for f in best[\"fails\"]:\n",
    "    print(f\"- {f['test_id']} group={f['group']} intent_ok={f['intent_ok']} flags_ok={f['flags_ok']}\")\n",
    "    for e in f[\"intent_errors\"]:\n",
    "        print(\"  intent:\", e)\n",
    "    for e in f[\"flag_errors\"]:\n",
    "        print(\"  flags :\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
