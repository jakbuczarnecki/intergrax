{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea320685",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c195ab",
   "metadata": {},
   "source": [
    "# 02 – Attachments & Ingestion Demo (Drop-In Knowledge Mode)\n",
    "\n",
    "This notebook demonstrates how the **Drop-In Knowledge Mode** runtime:\n",
    "\n",
    "1. Works with **sessions** and basic conversational memory.\n",
    "2. Accepts **attachments** via `AttachmentRef`.\n",
    "3. Uses the `AttachmentIngestionService` to:\n",
    "   - resolve attachment URIs to files,\n",
    "   - load and split documents with Intergrax RAG components,\n",
    "   - embed them,\n",
    "   - store them in the vector store.\n",
    "\n",
    "At this stage:\n",
    "\n",
    "- We **do not** yet perform RAG retrieval when answering.\n",
    "- The goal is to verify that:\n",
    "  - attachments are correctly stored in the session,\n",
    "  - ingestion runs without errors,\n",
    "  - chunks are stored in the vector store,\n",
    "  - ingestion details are visible in `debug_trace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "479afa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d508ff",
   "metadata": {},
   "source": [
    "## 1. Initialize the Drop-In Knowledge Mode runtime\n",
    "\n",
    "In this section we:\n",
    "\n",
    "1. Create an **in-memory session store** – suitable for notebooks and tests.\n",
    "2. Instantiate an **LLM adapter** using Ollama + LangChain.\n",
    "3. Configure:\n",
    "   - `IntergraxEmbeddingManager` (Ollama embeddings),\n",
    "   - `IntergraxVectorstoreManager` (Chroma as a vector store).\n",
    "4. Create a `RuntimeConfig` and `DropInKnowledgeRuntime` instance.\n",
    "\n",
    "We intentionally keep RAG turned off for now – we only care about ingestion,\n",
    "not retrieval or context building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37c5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=intergrax_docs_ingestion\n",
      "[intergraxVectorstoreManager] Existing count: 144\n",
      "Runtime initialized at 2025-12-27T08:24:06.658709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS\\AppData\\Local\\Temp\\ipykernel_18984\\662763891.py:58: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(\"Runtime initialized at\", datetime.utcnow().isoformat())\n"
     ]
    }
   ],
   "source": [
    "# 1) In-memory session store (no external DB, good for experiments)\n",
    "from intergrax.globals.settings import GLOBAL_SETTINGS\n",
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.ingestion.attachments import FileSystemAttachmentResolver\n",
    "from intergrax.runtime.drop_in_knowledge_mode.ingestion.ingestion_service import AttachmentIngestionService\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=InMemorySessionStorage()\n",
    ")\n",
    "\n",
    "# 2) LLM adapter based on Ollama + LangChain\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "# 3) High-level runtime configuration\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    enable_rag=False,\n",
    "    enable_websearch=False,\n",
    "    tools_mode=\"off\",\n",
    "    enable_user_profile_memory=True,\n",
    ")\n",
    "\n",
    "# 4) Vector store connection (same collection as ingestion)\n",
    "embedding_manager = EmbeddingManager(provider=\"ollama\")\n",
    "\n",
    "vectorstore_cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs_ingestion\",    \n",
    ")\n",
    "\n",
    "# 5) Ingestion service configuration\n",
    "ingestion_service = AttachmentIngestionService(\n",
    "        resolver=FileSystemAttachmentResolver(),\n",
    "        embedding_manager=embedding_manager,\n",
    "        vectorstore_manager=VectorstoreManager(\n",
    "            config=vectorstore_cfg\n",
    "        )\n",
    ")\n",
    "\n",
    "runtime = DropInKnowledgeRuntime(\n",
    "    config=config,    \n",
    "    session_manager=session_manager,\n",
    "    ingestion_service=ingestion_service,\n",
    ")\n",
    "\n",
    "print(\"Runtime initialized at\", datetime.utcnow().isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b60e9e",
   "metadata": {},
   "source": [
    "## 2. Prepare an AttachmentRef for a local project document\n",
    "\n",
    "Now we want to simulate what happens when a user **attaches a file** in a chat UI.\n",
    "\n",
    "In this example, we will:\n",
    "\n",
    "- point to a local file (for example `PROJECT_STRUCTURE.md` at the repo root),\n",
    "- wrap it in an `AttachmentRef`,\n",
    "- let the runtime handle ingestion of this attachment.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "- We use `Path(...).resolve().as_uri()` to generate a proper `file://` URI that\n",
    "  the `FileSystemAttachmentResolver` can understand.\n",
    "- In a real application, URIs could also be:\n",
    "  - `s3://...`,\n",
    "  - `db://attachments/<id>`,\n",
    "  - or any other scheme supported by a custom `AttachmentResolver`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e8a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttachmentRef prepared:\n",
      "  id       : project-structure-md\n",
      "  type     : markdown\n",
      "  uri      : file:///D:/Projekty/intergrax/PROJECT_STRUCTURE.md\n",
      "  metadata : {'description': 'Intergrax project structure document', 'scope': 'intergrax-docs-demo'}\n"
     ]
    }
   ],
   "source": [
    "# Adjust this path so that it points to a real file in your Intergrax repository.\n",
    "# For this demo, we assume PROJECT_STRUCTURE.md is located at the repo root.\n",
    "from intergrax.llm.messages import AttachmentRef\n",
    "\n",
    "\n",
    "project_root_file = Path(\"../../PROJECT_STRUCTURE.md\").resolve()\n",
    "\n",
    "if not project_root_file.exists():\n",
    "    raise FileNotFoundError(f\"Expected demo file does not exist: {project_root_file}\")\n",
    "\n",
    "attachment_uri = project_root_file.as_uri()\n",
    "\n",
    "attachment = AttachmentRef(\n",
    "    id=\"project-structure-md\",\n",
    "    type=\"markdown\",\n",
    "    uri=attachment_uri,\n",
    "    metadata={\n",
    "        \"description\": \"Intergrax project structure document\",\n",
    "        \"scope\": \"intergrax-docs-demo\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"AttachmentRef prepared:\")\n",
    "print(\"  id       :\", attachment.id)\n",
    "print(\"  type     :\", attachment.type)\n",
    "print(\"  uri      :\", attachment.uri)\n",
    "print(\"  metadata :\", attachment.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f596a6ff",
   "metadata": {},
   "source": [
    "## 3. First runtime call with an attachment\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Create a `RuntimeRequest` with:\n",
    "   - `user_id`,\n",
    "   - `session_id`,\n",
    "   - a user message,\n",
    "   - a list containing our `AttachmentRef`.\n",
    "2. Call `runtime.ask(request_1)`.\n",
    "3. Inspect:\n",
    "   - the model's answer,\n",
    "   - the `RouteInfo`,\n",
    "   - the `debug_trace[\"ingestion\"]` field.\n",
    "\n",
    "`debug_trace[\"ingestion\"]` should contain:\n",
    "\n",
    "- an entry per ingested attachment,\n",
    "- number of generated chunks,\n",
    "- how many vectors were stored,\n",
    "- metadata such as source path and session identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a119478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 144 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 144\n",
      "=== ANSWER 1 ===\n",
      "I've retrieved the attached document, which appears to be `PROJECT_STRUCTURE.md`. I'm now familiar with the structure of the Intergrax project, including its components, modules, and key responsibilities.\n",
      "\n",
      "What would you like me to help with regarding this project structure? Do you have specific questions or areas where you'd like clarification?\n",
      "\n",
      "=== ROUTE INFO ===\n",
      "RouteInfo(used_rag=False,\n",
      "          used_websearch=False,\n",
      "          used_tools=False,\n",
      "          used_user_profile=False,\n",
      "          used_user_longterm_memory=False,\n",
      "          strategy='llm_with_session_attachments',\n",
      "          extra={'attachments_chunks': 6, 'used_attachments_context': True})\n",
      "\n",
      "=== DEBUG TRACE: INGESTION ===\n",
      "[{'attachment_id': 'project-structure-md',\n",
      "  'attachment_type': 'markdown',\n",
      "  'metadata': {'session_id': 'demo-session-attachments-001',\n",
      "               'source_path': 'D:\\\\Projekty\\\\intergrax\\\\PROJECT_STRUCTURE.md',\n",
      "               'tenant_id': None,\n",
      "               'user_id': 'demo-user-attachments',\n",
      "               'workspace_id': None},\n",
      "  'num_chunks': 144,\n",
      "  'vector_ids_count': 144}]\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "\n",
    "\n",
    "user_id = \"demo-user-attachments\"\n",
    "session_id = \"demo-session-attachments-001\"\n",
    "\n",
    "request_1 = RuntimeRequest(\n",
    "    user_id=user_id,\n",
    "    session_id=session_id,\n",
    "    message=(\n",
    "        \"I am attaching a document that describes the Intergrax project \"\n",
    "        \"structure. Please ingest it and confirm when you're ready.\"\n",
    "    ),\n",
    "    attachments=[attachment],\n",
    "    metadata={\"source\": \"02_attachments_ingestion_demo\"},\n",
    ")\n",
    "\n",
    "answer_1 = await runtime.run(request_1)\n",
    "\n",
    "print(\"=== ANSWER 1 ===\")\n",
    "print(answer_1.answer)\n",
    "\n",
    "print(\"\\n=== ROUTE INFO ===\")\n",
    "pprint(answer_1.route)\n",
    "\n",
    "print(\"\\n=== DEBUG TRACE: INGESTION ===\")\n",
    "pprint(answer_1.debug_trace.get(\"ingestion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1276c3",
   "metadata": {},
   "source": [
    "## 4. Inspect the session state (messages and attachments)\n",
    "\n",
    "Next, we want to verify that the **session store** correctly persisted:\n",
    "\n",
    "- the user message,\n",
    "- the assistant response,\n",
    "- and the attachment references.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load the session by `session_id`.\n",
    "2. Inspect:\n",
    "   - basic session metadata,\n",
    "   - all `SessionMessage` objects,\n",
    "   - their `attachments` and `metadata` fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b55023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SESSION AFTER FIRST REQUEST ===\n",
      "Session ID : demo-session-attachments-001\n",
      "User ID    : demo-user-attachments\n",
      "Messages   : 2\n",
      "Created at : 2025-12-27 08:24:06.671216+00:00\n",
      "Updated at : 2025-12-27 08:24:16.529255+00:00\n",
      "\n",
      "=== MESSAGES ===\n",
      "[1] role='user', created_at=2025-12-27T08:24:11.281744+00:00\n",
      "     content=\"I am attaching a document that describes the Intergrax project structure. Please ingest it and confirm when you're ready.\"\n",
      "[2] role='assistant', created_at=2025-12-27T08:24:16.529255+00:00\n",
      "     content=\"I've retrieved the attached document, which appears to be `PROJECT_STRUCTURE.md`. I'm now familiar with the structure of the Intergrax project, including its components, modules, and key responsibilities.\\n\\nWhat would you like me to help with regarding this project structure? Do you have specific questions or areas where you'd like clarification?\"\n"
     ]
    }
   ],
   "source": [
    "session = await session_manager.get_session(session_id)\n",
    "messages = await session_manager.get_history(session_id=session_id)\n",
    "\n",
    "print(\"=== SESSION AFTER FIRST REQUEST ===\")\n",
    "print(\"Session ID :\", session.id)\n",
    "print(\"User ID    :\", session.user_id)\n",
    "print(\"Messages   :\", len(messages))\n",
    "print(\"Created at :\", session.created_at)\n",
    "print(\"Updated at :\", session.updated_at)\n",
    "\n",
    "print(\"\\n=== MESSAGES ===\")\n",
    "for idx, msg in enumerate(messages, start=1):\n",
    "    print(f\"[{idx}] role={msg.role!r}, created_at={msg.created_at}\")\n",
    "    print(f\"     content={msg.content!r}\")\n",
    "    if msg.attachments:\n",
    "        print(f\"     attachments ({len(msg.attachments)}):\")\n",
    "        for a in msg.attachments:\n",
    "            print(f\"       - id={a.id!r}, type={a.type!r}, uri={a.uri!r}\")\n",
    "    if msg.metadata:\n",
    "        print(f\"     metadata keys: {list(msg.metadata.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce5f6c",
   "metadata": {},
   "source": [
    "## 5. Inspect the LLM-facing chat history\n",
    "\n",
    "The runtime internally converts `ChatSession.messages` into a list of\n",
    "`ChatMessage` objects that are passed to the LLM adapter.\n",
    "\n",
    "Here we directly call the internal helper `_build_chat_history(session)` to:\n",
    "\n",
    "- verify how the conversation history looks before the next model call,\n",
    "- confirm that messages are preserved in order,\n",
    "- see how roles and content are exposed to the LLM.\n",
    "\n",
    "Note:\n",
    "- Attachments are not included directly in `ChatMessage` at this stage.\n",
    "  They are used by the ingestion pipeline and RAG, not embedded in raw text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0185f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILT CHAT HISTORY ===\n",
      "History length: 2\n",
      "[1] role='user', created_at=2025-12-27T08:24:11.281744+00:00\n",
      "     content=\"I am attaching a document that describes the Intergrax project structure. Please ingest it and confirm when you're ready.\"\n",
      "[2] role='assistant', created_at=2025-12-27T08:24:16.529255+00:00\n",
      "     content=\"I've retrieved the attached document, which appears to be `PROJECT_STRUCTURE.md`. I'm now familiar with the structure of the Intergrax project, including its components, modules, and key responsibilities.\\n\\nWhat would you like me to help with regarding this project structure? Do you have specific questions or areas where you'd like clarification?\"\n"
     ]
    }
   ],
   "source": [
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "\n",
    "history = await runtime._history_layer._build_chat_history(session)\n",
    "\n",
    "print(\"=== BUILT CHAT HISTORY ===\")\n",
    "print(\"History length:\", len(history))\n",
    "for idx, msg in enumerate(history, start=1):\n",
    "    assert isinstance(msg, ChatMessage)\n",
    "    print(f\"[{idx}] role={msg.role!r}, created_at={msg.created_at}\")\n",
    "    print(f\"     content={msg.content!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fafe33",
   "metadata": {},
   "source": [
    "## 6. Second runtime call in the same session (no new attachments)\n",
    "\n",
    "To mirror a typical ChatGPT-like workflow:\n",
    "\n",
    "1. First message: user uploads a document and asks the system to ingest it.\n",
    "2. Second message: user refers to that document and asks for a summary.\n",
    "\n",
    "Here we:\n",
    "\n",
    "- reuse the same `user_id` and `session_id`,\n",
    "- send a second message without additional attachments,\n",
    "- inspect the answer, route info, and debug trace.\n",
    "\n",
    "At this stage, the answer will not yet use RAG retrieval. However:\n",
    "\n",
    "- the ingestion has already populated the vector store,\n",
    "- the session has accumulated multiple messages,\n",
    "- the runtime is ready for the next step: **context builder + RAG**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8016e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER 2 ===\n",
      "Based on the `PROJECT_STRUCTURE.md` document, here's a summary of the main components of the Intergrax framework:\n",
      "\n",
      "1. **Domain: LLM instructions / Framework bundle**:\n",
      "\t* Contains all Python modules from the package directory\n",
      "\t* Provides a single source of truth for the Intergrax framework\n",
      "2. **Key Responsibilities:**\n",
      "\t* Navigation through the bundle using the MODULE MAP and INDEX\n",
      "\t* Rules for proposing changes to ensure preservation of existing architecture, naming, and conventions\n",
      "3. **Components:**\n",
      "\t* `main.py`: Entry point for the Intergrax framework, responsible for executing core functionality\n",
      "\t* `mcp\\__init__.py`: Multimedia Processing module (downloads videos, transcribes audio, extracts frames)\n",
      "\t* `intergrax\\openai\\__init__.py`: OpenAI adapter initialization and configuration\n",
      "\t* `intergrax\\runtime\\drop_in_knowledge_mode\\engine\\runtime_state.py`: RuntimeState class definition for managing runtime pipeline state\n",
      "\n",
      "Additionally, the document mentions several other components, including:\n",
      "\n",
      "* Utility modules (e.g., recursive file scanner)\n",
      "* RAG logic and conversational runtime engine (e.g., chat session management, LLM context building)\n",
      "* Logging and configuration modules\n",
      "* Various adapters and interfaces (e.g., OpenAI, Gemini, Ollama)\n",
      "\n",
      "These components appear to be the core building blocks of the Intergrax framework, enabling its functionality as a conversational AI platform.\n",
      "\n",
      "=== ROUTE INFO 2 ===\n",
      "RouteInfo(used_rag=False,\n",
      "          used_websearch=False,\n",
      "          used_tools=False,\n",
      "          used_user_profile=False,\n",
      "          used_user_longterm_memory=False,\n",
      "          strategy='llm_with_session_attachments',\n",
      "          extra={'attachments_chunks': 6, 'used_attachments_context': True})\n",
      "\n",
      "=== DEBUG TRACE 2: ATTACHMENTS ===\n",
      "{'hits_count': 6,\n",
      " 'provider': 'chroma',\n",
      " 'score_threshold': None,\n",
      " 'top_k': 6,\n",
      " 'used': True,\n",
      " 'where': {'session_id': 'demo-session-attachments-001',\n",
      "           'user_id': 'demo-user-attachments'}}\n",
      "attachments_chunks: 6\n"
     ]
    }
   ],
   "source": [
    "request_2 = RuntimeRequest(\n",
    "    user_id=user_id,\n",
    "    session_id=session_id,\n",
    "    message=(\n",
    "        \"Great. In the previous message I attached the project structure file. \"\n",
    "        \"Please summarize the main components of the Intergrax framework as \"\n",
    "        \"described in that document.\"\n",
    "    ),\n",
    "    attachments=[],  # no new attachments; we rely on the existing session context\n",
    "    metadata={\"source\": \"02_attachments_ingestion_demo_second_call\"},\n",
    ")\n",
    "\n",
    "answer_2 = await runtime.run(request_2)\n",
    "\n",
    "print(\"=== ANSWER 2 ===\")\n",
    "print(answer_2.answer)\n",
    "\n",
    "print(\"\\n=== ROUTE INFO 2 ===\")\n",
    "pprint(answer_2.route)\n",
    "\n",
    "print(\"\\n=== DEBUG TRACE 2: ATTACHMENTS ===\")\n",
    "pprint(answer_2.debug_trace.get(\"attachments\"))\n",
    "print(\"attachments_chunks:\", answer_2.debug_trace.get(\"attachments_chunks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bea79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs: 2\n",
      "====================================================================================================\n",
      "Run #1\n",
      "  ts_utc     : 2025-12-27T08:24:16.529255+00:00\n",
      "  run_id     : run_908399f0a4f449dd9b4856def74769da\n",
      "  session_id : demo-session-attachments-001\n",
      "  user_id    : demo-user-attachments\n",
      "LLMUsageReport(run_id=run_908399f0a4f449dd9b4856def74769da)\n",
      "Total:\n",
      "  calls        : 1\n",
      "  input_tokens : 1093\n",
      "  output_tokens: 66\n",
      "  total_tokens : 1159\n",
      "  duration_ms  : 4905\n",
      "  errors       : 0\n",
      "By provider/model:\n",
      "  - LLMProvider.OLLAMA:llama3.1:latest: calls=1 in=1093 out=66 total=1159 ms=4905 err=0\n",
      "Entries (registration order):\n",
      "  - core_adapter [LLMProvider.OLLAMA:llama3.1:latest] (LangChainOllamaAdapter)\n",
      "      calls=1 in=1093 out=66 total=1159 ms=4905 err=0 instance_id=2764292431488\n",
      "====================================================================================================\n",
      "Run #2\n",
      "  ts_utc     : 2025-12-27T08:24:23.798516+00:00\n",
      "  run_id     : run_5d271e64518243a1a57e4a596a506738\n",
      "  session_id : demo-session-attachments-001\n",
      "  user_id    : demo-user-attachments\n",
      "LLMUsageReport(run_id=run_5d271e64518243a1a57e4a596a506738)\n",
      "Total:\n",
      "  calls        : 1\n",
      "  input_tokens : 1377\n",
      "  output_tokens: 301\n",
      "  total_tokens : 1678\n",
      "  duration_ms  : 6959\n",
      "  errors       : 0\n",
      "By provider/model:\n",
      "  - LLMProvider.OLLAMA:llama3.1:latest: calls=1 in=1377 out=301 total=1678 ms=6959 err=0\n",
      "Entries (registration order):\n",
      "  - core_adapter [LLMProvider.OLLAMA:llama3.1:latest] (LangChainOllamaAdapter)\n",
      "      calls=1 in=1377 out=301 total=1678 ms=6959 err=0 instance_id=2764292431488\n"
     ]
    }
   ],
   "source": [
    "await runtime.print_usage_runs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
