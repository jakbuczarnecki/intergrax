{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea320685",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c195ab",
   "metadata": {},
   "source": [
    "# 02 – Attachments & Ingestion Demo (Drop-In Knowledge Mode)\n",
    "\n",
    "This notebook demonstrates how the **Drop-In Knowledge Mode** runtime:\n",
    "\n",
    "1. Works with **sessions** and basic conversational memory.\n",
    "2. Accepts **attachments** via `AttachmentRef`.\n",
    "3. Uses the `AttachmentIngestionService` to:\n",
    "   - resolve attachment URIs to files,\n",
    "   - load and split documents with Intergrax RAG components,\n",
    "   - embed them,\n",
    "   - store them in the vector store.\n",
    "\n",
    "At this stage:\n",
    "\n",
    "- We **do not** yet perform RAG retrieval when answering.\n",
    "- The goal is to verify that:\n",
    "  - attachments are correctly stored in the session,\n",
    "  - ingestion runs without errors,\n",
    "  - chunks are stored in the vector store,\n",
    "  - ingestion details are visible in `debug_trace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479afa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from intergrax.llm_adapters import LangChainOllamaAdapter\n",
    "from intergrax.llm.messages import ChatMessage, AttachmentRef\n",
    "\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VectorstoreManager, VSConfig\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d508ff",
   "metadata": {},
   "source": [
    "## 1. Initialize the Drop-In Knowledge Mode runtime\n",
    "\n",
    "In this section we:\n",
    "\n",
    "1. Create an **in-memory session store** – suitable for notebooks and tests.\n",
    "2. Instantiate an **LLM adapter** using Ollama + LangChain.\n",
    "3. Configure:\n",
    "   - `IntergraxEmbeddingManager` (Ollama embeddings),\n",
    "   - `IntergraxVectorstoreManager` (Chroma as a vector store).\n",
    "4. Create a `RuntimeConfig` and `DropInKnowledgeRuntime` instance.\n",
    "\n",
    "We intentionally keep RAG turned off for now – we only care about ingestion,\n",
    "not retrieval or context building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=intergrax_docs_drop_in_demo\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "Runtime initialized at 2025-11-24T12:09:53.632708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS\\AppData\\Local\\Temp\\ipykernel_24852\\1773200209.py:52: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(\"Runtime initialized at\", datetime.utcnow().isoformat())\n"
     ]
    }
   ],
   "source": [
    "# 1) In-memory session store (no external DB, good for experiments)\n",
    "from intergrax.globals.settings import GlobalSettings\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=InMemorySessionStorage()\n",
    ")\n",
    "\n",
    "# 2) LLM adapter based on Ollama + LangChain\n",
    "llm_adapter = LangChainOllamaAdapter(\n",
    "    chat=ChatOllama(\n",
    "        model=GlobalSettings.default_ollama_model\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) Embedding manager using Ollama (same setup as in other Intergrax RAG examples)\n",
    "embedding_manager = EmbeddingManager(\n",
    "    provider=\"ollama\",\n",
    "    model_name=GlobalSettings.default_ollama_embed_model,\n",
    "    assume_ollama_dim=1536,\n",
    ")\n",
    "\n",
    "# 4) Vector store manager (Chroma collection dedicated to this demo)\n",
    "vectorstore_cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs_drop_in_demo\",\n",
    ")\n",
    "\n",
    "vectorstore_manager = VectorstoreManager(\n",
    "    config=vectorstore_cfg,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 5) High-level runtime configuration\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    embedding_manager=embedding_manager,\n",
    "    vectorstore_manager=vectorstore_manager,\n",
    "    llm_label=\"llm-adapter-ollama\",\n",
    "    embedding_label=\"ollama-gte-qwen2-1.5b\",\n",
    "    vectorstore_label=\"chroma-intergrax-docs\",\n",
    "    enable_rag=False,         # RAG will be plugged in a later notebook\n",
    "    enable_websearch=False,\n",
    "    enable_tools=False,\n",
    "    enable_long_term_memory=False,\n",
    "    enable_user_profile_memory=True,\n",
    ")\n",
    "\n",
    "# 6) Drop-In Knowledge Mode runtime\n",
    "runtime = DropInKnowledgeRuntime(\n",
    "    config=config,\n",
    "    session_manager=session_manager,\n",
    "    # If your engine __init__ has an optional ingestion_service parameter,\n",
    "    # you can omit it here and let the engine build a default one internally.\n",
    ")\n",
    "\n",
    "print(\"Runtime initialized at\", datetime.utcnow().isoformat())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b60e9e",
   "metadata": {},
   "source": [
    "## 2. Prepare an AttachmentRef for a local project document\n",
    "\n",
    "Now we want to simulate what happens when a user **attaches a file** in a chat UI.\n",
    "\n",
    "In this example, we will:\n",
    "\n",
    "- point to a local file (for example `PROJECT_STRUCTURE.md` at the repo root),\n",
    "- wrap it in an `AttachmentRef`,\n",
    "- let the runtime handle ingestion of this attachment.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "- We use `Path(...).resolve().as_uri()` to generate a proper `file://` URI that\n",
    "  the `FileSystemAttachmentResolver` can understand.\n",
    "- In a real application, URIs could also be:\n",
    "  - `s3://...`,\n",
    "  - `db://attachments/<id>`,\n",
    "  - or any other scheme supported by a custom `AttachmentResolver`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e8a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttachmentRef prepared:\n",
      "  id       : project-structure-md\n",
      "  type     : markdown\n",
      "  uri      : file:///D:/Projekty/intergrax/PROJECT_STRUCTURE.md\n",
      "  metadata : {'description': 'Intergrax project structure document', 'scope': 'intergrax-docs-demo'}\n"
     ]
    }
   ],
   "source": [
    "# Adjust this path so that it points to a real file in your Intergrax repository.\n",
    "# For this demo, we assume PROJECT_STRUCTURE.md is located at the repo root.\n",
    "project_root_file = Path(\"../../PROJECT_STRUCTURE.md\").resolve()\n",
    "\n",
    "if not project_root_file.exists():\n",
    "    raise FileNotFoundError(f\"Expected demo file does not exist: {project_root_file}\")\n",
    "\n",
    "attachment_uri = project_root_file.as_uri()\n",
    "\n",
    "attachment = AttachmentRef(\n",
    "    id=\"project-structure-md\",\n",
    "    type=\"markdown\",\n",
    "    uri=attachment_uri,\n",
    "    metadata={\n",
    "        \"description\": \"Intergrax project structure document\",\n",
    "        \"scope\": \"intergrax-docs-demo\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"AttachmentRef prepared:\")\n",
    "print(\"  id       :\", attachment.id)\n",
    "print(\"  type     :\", attachment.type)\n",
    "print(\"  uri      :\", attachment.uri)\n",
    "print(\"  metadata :\", attachment.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f596a6ff",
   "metadata": {},
   "source": [
    "## 3. First runtime call with an attachment\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Create a `RuntimeRequest` with:\n",
    "   - `user_id`,\n",
    "   - `session_id`,\n",
    "   - a user message,\n",
    "   - a list containing our `AttachmentRef`.\n",
    "2. Call `runtime.ask(request_1)`.\n",
    "3. Inspect:\n",
    "   - the model's answer,\n",
    "   - the `RouteInfo`,\n",
    "   - the `debug_trace[\"ingestion\"]` field.\n",
    "\n",
    "`debug_trace[\"ingestion\"]` should contain:\n",
    "\n",
    "- an entry per ingested attachment,\n",
    "- number of generated chunks,\n",
    "- how many vectors were stored,\n",
    "- metadata such as source path and session identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 80 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 80\n",
      "=== ANSWER 1 ===\n",
      "However, I don't see any document attached to our conversation. As a text-based AI assistant, I don't have the ability to receive or access external files.\n",
      "\n",
      "If you'd like to share the information about the Intergrax project structure with me, you can copy and paste the contents of the document into this chat window, and I'll be happy to help you review it!\n",
      "\n",
      "=== ROUTE INFO ===\n",
      "RouteInfo(used_rag=False,\n",
      "          used_websearch=False,\n",
      "          used_tools=False,\n",
      "          used_long_term_memory=False,\n",
      "          used_user_profile=True,\n",
      "          strategy='llm_only_with_ingestion',\n",
      "          extra={})\n",
      "\n",
      "=== DEBUG TRACE: INGESTION ===\n",
      "[{'attachment_id': 'project-structure-md',\n",
      "  'attachment_type': 'markdown',\n",
      "  'metadata': {'session_id': 'demo-session-attachments-001',\n",
      "               'source_path': 'D:\\\\Projekty\\\\intergrax\\\\PROJECT_STRUCTURE.md',\n",
      "               'tenant_id': None,\n",
      "               'user_id': 'demo-user-attachments',\n",
      "               'workspace_id': None},\n",
      "  'num_chunks': 80,\n",
      "  'vector_ids_count': 80}]\n"
     ]
    }
   ],
   "source": [
    "user_id = \"demo-user-attachments\"\n",
    "session_id = \"demo-session-attachments-001\"\n",
    "\n",
    "request_1 = RuntimeRequest(\n",
    "    user_id=user_id,\n",
    "    session_id=session_id,\n",
    "    message=(\n",
    "        \"I am attaching a document that describes the Intergrax project \"\n",
    "        \"structure. Please ingest it and confirm when you're ready.\"\n",
    "    ),\n",
    "    attachments=[attachment],\n",
    "    metadata={\"source\": \"02_attachments_ingestion_demo\"},\n",
    ")\n",
    "\n",
    "answer_1 = await runtime.run(request_1)\n",
    "\n",
    "print(\"=== ANSWER 1 ===\")\n",
    "print(answer_1.answer)\n",
    "\n",
    "print(\"\\n=== ROUTE INFO ===\")\n",
    "pprint(answer_1.route)\n",
    "\n",
    "print(\"\\n=== DEBUG TRACE: INGESTION ===\")\n",
    "pprint(answer_1.debug_trace.get(\"ingestion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1276c3",
   "metadata": {},
   "source": [
    "## 4. Inspect the session state (messages and attachments)\n",
    "\n",
    "Next, we want to verify that the **session store** correctly persisted:\n",
    "\n",
    "- the user message,\n",
    "- the assistant response,\n",
    "- and the attachment references.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load the session by `session_id`.\n",
    "2. Inspect:\n",
    "   - basic session metadata,\n",
    "   - all `SessionMessage` objects,\n",
    "   - their `attachments` and `metadata` fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b55023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SESSION AFTER FIRST REQUEST ===\n",
      "Session ID : demo-session-attachments-001\n",
      "User ID    : demo-user-attachments\n",
      "Messages   : 2\n",
      "Created at : 2025-11-24 12:09:53.648304+00:00\n",
      "Updated at : 2025-11-24 12:10:00.342054+00:00\n",
      "\n",
      "=== MESSAGES ===\n",
      "[1] role='user', created_at=2025-11-24T12:09:56.320037+00:00\n",
      "     content=\"I am attaching a document that describes the Intergrax project structure. Please ingest it and confirm when you're ready.\"\n",
      "     attachments (1):\n",
      "       - id='project-structure-md', type='markdown', uri='file:///D:/Projekty/intergrax/PROJECT_STRUCTURE.md'\n",
      "     metadata keys: ['runtime_request_metadata']\n",
      "[2] role='assistant', created_at=2025-11-24T12:10:00.342054+00:00\n",
      "     content=\"However, I don't see any document attached to our conversation. As a text-based AI assistant, I don't have the ability to receive or access external files.\\n\\nIf you'd like to share the information about the Intergrax project structure with me, you can copy and paste the contents of the document into this chat window, and I'll be happy to help you review it!\"\n",
      "     metadata keys: ['placeholder']\n"
     ]
    }
   ],
   "source": [
    "session = await session_manager.get_session(session_id)\n",
    "\n",
    "print(\"=== SESSION AFTER FIRST REQUEST ===\")\n",
    "print(\"Session ID :\", session.id)\n",
    "print(\"User ID    :\", session.user_id)\n",
    "print(\"Messages   :\", len(session.messages))\n",
    "print(\"Created at :\", session.created_at)\n",
    "print(\"Updated at :\", session.updated_at)\n",
    "\n",
    "print(\"\\n=== MESSAGES ===\")\n",
    "for idx, msg in enumerate(session.messages, start=1):\n",
    "    print(f\"[{idx}] role={msg.role!r}, created_at={msg.created_at.isoformat()}\")\n",
    "    print(f\"     content={msg.content!r}\")\n",
    "    if msg.attachments:\n",
    "        print(f\"     attachments ({len(msg.attachments)}):\")\n",
    "        for a in msg.attachments:\n",
    "            print(f\"       - id={a.id!r}, type={a.type!r}, uri={a.uri!r}\")\n",
    "    if msg.metadata:\n",
    "        print(f\"     metadata keys: {list(msg.metadata.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce5f6c",
   "metadata": {},
   "source": [
    "## 5. Inspect the LLM-facing chat history\n",
    "\n",
    "The runtime internally converts `ChatSession.messages` into a list of\n",
    "`ChatMessage` objects that are passed to the LLM adapter.\n",
    "\n",
    "Here we directly call the internal helper `_build_chat_history(session)` to:\n",
    "\n",
    "- verify how the conversation history looks before the next model call,\n",
    "- confirm that messages are preserved in order,\n",
    "- see how roles and content are exposed to the LLM.\n",
    "\n",
    "Note:\n",
    "- Attachments are not included directly in `ChatMessage` at this stage.\n",
    "  They are used by the ingestion pipeline and RAG, not embedded in raw text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0185f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILT CHAT HISTORY ===\n",
      "History length: 2\n",
      "[1] role='user', created_at=2025-11-24T12:09:56.320037+00:00\n",
      "     content=\"I am attaching a document that describes the Intergrax project structure. Please ingest it and confirm when you're ready.\"\n",
      "[2] role='assistant', created_at=2025-11-24T12:10:00.342054+00:00\n",
      "     content=\"However, I don't see any document attached to our conversation. As a text-based AI assistant, I don't have the ability to receive or access external files.\\n\\nIf you'd like to share the information about the Intergrax project structure with me, you can copy and paste the contents of the document into this chat window, and I'll be happy to help you review it!\"\n"
     ]
    }
   ],
   "source": [
    "history = runtime._build_chat_history(session)\n",
    "\n",
    "print(\"=== BUILT CHAT HISTORY ===\")\n",
    "print(\"History length:\", len(history))\n",
    "for idx, msg in enumerate(history, start=1):\n",
    "    assert isinstance(msg, ChatMessage)\n",
    "    print(f\"[{idx}] role={msg.role!r}, created_at={msg.created_at}\")\n",
    "    print(f\"     content={msg.content!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fafe33",
   "metadata": {},
   "source": [
    "## 6. Second runtime call in the same session (no new attachments)\n",
    "\n",
    "To mirror a typical ChatGPT-like workflow:\n",
    "\n",
    "1. First message: user uploads a document and asks the system to ingest it.\n",
    "2. Second message: user refers to that document and asks for a summary.\n",
    "\n",
    "Here we:\n",
    "\n",
    "- reuse the same `user_id` and `session_id`,\n",
    "- send a second message without additional attachments,\n",
    "- inspect the answer, route info, and debug trace.\n",
    "\n",
    "At this stage, the answer will not yet use RAG retrieval. However:\n",
    "\n",
    "- the ingestion has already populated the vector store,\n",
    "- the session has accumulated multiple messages,\n",
    "- the runtime is ready for the next step: **context builder + RAG**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8016e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER 2 ===\n",
      "As I mentioned earlier, there is no attachment or document from you that I can access. Our conversation started with your request to confirm when I'm ready after attaching a document, but since I couldn't receive any attachment, I didn't have anything to review.\n",
      "\n",
      "If you'd like to share the project structure information by copying and pasting it into this chat window, I'll be happy to help summarize the main components of the Intergrax framework for you.\n",
      "\n",
      "=== ROUTE INFO 2 ===\n",
      "RouteInfo(used_rag=False,\n",
      "          used_websearch=False,\n",
      "          used_tools=False,\n",
      "          used_long_term_memory=False,\n",
      "          used_user_profile=True,\n",
      "          strategy='llm_only',\n",
      "          extra={})\n",
      "\n",
      "=== DEBUG TRACE 2: INGESTION ===\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "request_2 = RuntimeRequest(\n",
    "    user_id=user_id,\n",
    "    session_id=session_id,\n",
    "    message=(\n",
    "        \"Great. In the previous message I attached the project structure file. \"\n",
    "        \"Please summarize the main components of the Intergrax framework as \"\n",
    "        \"described in that document.\"\n",
    "    ),\n",
    "    attachments=[],  # no new attachments; we rely on the existing session context\n",
    "    metadata={\"source\": \"02_attachments_ingestion_demo_second_call\"},\n",
    ")\n",
    "\n",
    "answer_2 = await runtime.run(request_2)\n",
    "\n",
    "print(\"=== ANSWER 2 ===\")\n",
    "print(answer_2.answer)\n",
    "\n",
    "print(\"\\n=== ROUTE INFO 2 ===\")\n",
    "pprint(answer_2.route)\n",
    "\n",
    "print(\"\\n=== DEBUG TRACE 2: INGESTION ===\")\n",
    "pprint(answer_2.debug_trace.get(\"ingestion\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
