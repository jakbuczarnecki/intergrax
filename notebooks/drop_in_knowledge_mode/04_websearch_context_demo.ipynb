{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4013d12",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c664bf",
   "metadata": {},
   "source": [
    "# 04_websearch_context_demo.ipynb\n",
    "\n",
    "This notebook demonstrates how to use **RuntimeEngine** with:\n",
    "\n",
    "- session-based chat,\n",
    "- optional RAG (attachments ingested into a vector store),\n",
    "- **live web search** via `WebSearchExecutor`,\n",
    "\n",
    "to achieve a \"ChatGPT-like\" experience with browsing.\n",
    "\n",
    "The core configuration (LLM adapter, embeddings, vector store, runtime config)\n",
    "is initialized in a single cell, while the rest of the notebook focuses on\n",
    "testing and inspecting the web search behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and environment setup\n",
    "# --------------------------------\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Drop-in knowledge runtime pieces\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import RuntimeEngine\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "\n",
    "# Web search integration\n",
    "from intergrax.websearch.service.websearch_executor import WebSearchExecutor\n",
    "from intergrax.websearch.providers.google_cse_provider import GoogleCSEProvider\n",
    "# from intergrax.websearch.providers.bing_provider import BingWebProvider  # optional\n",
    "\n",
    "\n",
    "# 1.1 Load environment variables (API keys, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# Make sure the env vars for web search are available.\n",
    "# If you want to use Bing as well, uncomment the Bing provider later.\n",
    "os.environ[\"GOOGLE_CSE_API_KEY\"] = os.getenv(\"GOOGLE_CSE_API_KEY\") or \"\"\n",
    "os.environ[\"GOOGLE_CSE_CX\"] = os.getenv(\"GOOGLE_CSE_CX\") or \"\"\n",
    "os.environ[\"BING_SEARCH_V7_API_KEY\"] = os.getenv(\"BING_SEARCH_V7_API_KEY\") or \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162378b1",
   "metadata": {},
   "source": [
    "# Core runtime configuration (web search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<intergrax.runtime.drop_in_knowledge_mode.engine.runtime.DropInKnowledgeRuntime object at 0x000002550055CB30>\n"
     ]
    }
   ],
   "source": [
    "# 2. Core runtime configuration (web search)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 2.1 Session store – simple in-memory storage for chat messages & metadata.\n",
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime_context import RuntimeContext\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=InMemorySessionStorage()\n",
    ")\n",
    "\n",
    "\n",
    "# Web search executor – wraps one or more providers (Google CSE, Bing, etc.)\n",
    "websearch_executor = WebSearchExecutor(\n",
    "    providers=[\n",
    "        GoogleCSEProvider(),\n",
    "    ],\n",
    "    max_text_chars=None,\n",
    ")\n",
    "\n",
    "# RuntimeConfig – single source of truth for drop-in knowledge runtime.\n",
    "config = RuntimeConfig(\n",
    "    # LLM & embeddings & vector store\n",
    "    llm_adapter=llm_adapter,\n",
    "\n",
    "    # RAG settings\n",
    "    enable_rag=False,\n",
    "\n",
    "    # Web search settings – THIS is the feature under test in this notebook.\n",
    "    enable_websearch=True,\n",
    "    websearch_executor=websearch_executor,\n",
    "    max_docs_per_query=4,  # how many docs we inject into the system prompt\n",
    "\n",
    "    # Tools / memory – kept off here to isolate the web search behavior.\n",
    "    tools_mode=\"off\",\n",
    "    enable_user_profile_memory=True,\n",
    ")\n",
    "\n",
    "context = RuntimeContext(\n",
    "    config=config,\n",
    "    session_manager=session_manager,\n",
    ")\n",
    "\n",
    "# 2.7 Drop-in knowledge runtime – chat engine with RAG + web search.\n",
    "runtime = RuntimeEngine(context=context)\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc96fc4",
   "metadata": {},
   "source": [
    "### 3. Create a fresh chat session for this web search demo\n",
    "\n",
    "We create a brand new session in the in-memory store.\n",
    "All questions in this notebook will be sent under the same `session_id`\n",
    "so that the runtime can keep short-term chat history (like ChatGPT).\n",
    "\n",
    "Helper: `ask(question: str)` for interactive testing\n",
    "\n",
    "This helper:\n",
    "1. Builds a `RuntimeRequest` bound to the current session.\n",
    "2. Sends it through `RuntimeEngine.ask(...)`.\n",
    "3. Prints:\n",
    "   - The user question\n",
    "   - The model answer (truncated optionally)\n",
    "   - Routing info (was RAG used? was web search used?)\n",
    "   - Basic debug info (web search, RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d0301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created:\n",
      "- id: 9e5c1c73-b9cc-4be4-915e-bb8872f978de\n",
      "- user_id: demo-user-websearch\n",
      "- tenant_id: demo-tenant\n",
      "- workspace_id: demo-workspace\n",
      "\n",
      "================================================================================\n",
      "USER QUESTION:\n",
      "What are the most recent major changes to the OpenAI API regarding the Responses API and tool calling? Provide a concise technical summary with the date of the change.\n",
      "================================================================================\n",
      "\n",
      "ASSISTANT ANSWER:\n",
      "\n",
      "According to the OpenAI API documentation, here are some recent major changes:\n",
      "\n",
      "1. **Improved Response Formatting** (February 2022): The Responses API now returns more detailed response objects, including metadata about the generated text.\n",
      "2. **Async Support for Text Generation** (April 2022): The Completions API now supports asynchronous responses for text generation requests, allowing for improved performance and scalability.\n",
      "3. **New `max_tokens` Parameter** (May 2022): A new parameter was introduced to control the maximum number of tokens in generated text, improving flexibility and reducing potential security risks.\n",
      "4. **Enhanced Error Handling** (June 2022): The API now returns more informative error messages and codes, making it easier for developers to diagnose and fix issues.\n",
      "5. **Support for Custom Models** (July 2022): Developers can now upload and use custom models within the OpenAI API, enabling tailored language processing capabilities.\n",
      "\n",
      "Please note that these changes might have been subject to further updates or modifications since their announcement dates. It's always a good idea to consult the official OpenAI documentation for the most current information.\n",
      "\n",
      "--- ROUTING INFO ---\n",
      "strategy:           llm_with_websearch\n",
      "used_rag:           False\n",
      "used_websearch:     True\n",
      "used_tools:         False\n",
      "used_user_profile:  False\n",
      "\n",
      "--- DEBUG: RAG ---\n",
      "(no RAG debug info)\n",
      "\n",
      "--- DEBUG: WEBSEARCH ---\n",
      "{\n",
      "  \"raw_results_preview\": [\n",
      "    {\n",
      "      \"type\": \"WebSearchResult\",\n",
      "      \"title\": \"Strategie projektowania promptów  |  Gemini API  |  Google AI for Developers\",\n",
      "      \"url\": \"https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=pl\",\n",
      "      \"snippet_len\": 124,\n",
      "      \"text_len\": 36238\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"WebSearchResult\",\n",
      "      \"title\": \"JavaScript Jobs for December 2025 | Freelancer\",\n",
      "      \"url\": \"https://www.freelancer.pl/jobs/javascript\",\n",
      "      \"snippet_len\": 156,\n",
      "      \"text_len\": 37167\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"WebSearchResult\",\n",
      "      \"title\": \"Wired-for-AI-EY-and-Liberty-Global-report.pdf\",\n",
      "      \"url\": \"https://www.libertyglobal.com/wp-content/uploads/2024/02/Wired-for-AI-EY-and-Liberty-Global-report.pdf\",\n",
      "      \"snippet_len\": 159,\n",
      "      \"text_len\": 2434535\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"WebSearchResult\",\n",
      "      \"title\": \"PROGRESS IN POLISH ARTIFICIAL INTELLIGENCE RESEARCH 5\",\n",
      "      \"url\": \"https://pages.mini.pw.edu.pl/~estatic/pliki/PP-RAI_2024_proceedings.pdf\",\n",
      "      \"snippet_len\": 162,\n",
      "      \"text_len\": 11894\n",
      "    }\n",
      "  ],\n",
      "  \"context_blocks_count\": 1,\n",
      "  \"context_preview\": \"WEB SOURCES (MAP_REDUCE)\\nNo answer-relevant evidence extracted from the fetched pages.\\n\",\n",
      "  \"context_preview_chars\": 87,\n",
      "  \"docs_preview\": [\n",
      "    {\n",
      "      \"title\": \"Strategie projektowania promptów  |  Gemini API  |  Google AI for Developers\",\n",
      "      \"url\": \"https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=pl\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"JavaScript Jobs for December 2025 | Freelancer\",\n",
      "      \"url\": \"https://www.freelancer.pl/jobs/javascript\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Wired-for-AI-EY-and-Liberty-Global-report.pdf\",\n",
      "      \"url\": \"https://www.libertyglobal.com/wp-content/uploads/2024/02/Wired-for-AI-EY-and-Liberty-Global-report.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"PROGRESS IN POLISH ARTIFICIAL INTELLIGENCE RESEARCH 5\",\n",
      "      \"url\": \"https://pages.mini.pw.edu.pl/~estatic/pliki/PP-RAI_2024_proceedings.pdf\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.prompts.websearch_prompt_builder import DefaultWebSearchPromptBuilder\n",
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeAnswer, RuntimeRequest\n",
    "from intergrax.runtime.drop_in_knowledge_mode.tracing.trace_models import TraceComponent\n",
    "from intergrax.websearch.service.websearch_config import WebSearchConfig, WebSearchStrategyType\n",
    "from intergrax.websearch.service.websearch_context_generator import create_websearch_context_generator\n",
    "\n",
    "# 3.1 Create a fresh session for this demo.\n",
    "session = await session_manager.create_session(\n",
    "    session_id=str(uuid.uuid4()),\n",
    "    user_id=\"demo-user-websearch\",\n",
    "    tenant_id=\"demo-tenant\",\n",
    "    workspace_id=\"demo-workspace\",\n",
    "    metadata={\"notebook\": \"04_websearch_drop_in_web_demo\"},\n",
    ")\n",
    "\n",
    "print(\"Session created:\")\n",
    "print(f\"- id: {session.id}\")\n",
    "print(f\"- user_id: {session.user_id}\")\n",
    "print(f\"- tenant_id: {session.tenant_id}\")\n",
    "print(f\"- workspace_id: {session.workspace_id}\")\n",
    "\n",
    "\n",
    "async def ask(question: str, *, show_full_answer: bool = True) -> RuntimeAnswer:\n",
    "    \"\"\"\n",
    "    Convenience helper for this notebook.\n",
    "\n",
    "    - Sends the user's question through RuntimeEngine.\n",
    "    - Prints the answer and basic routing/debug information.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"USER QUESTION:\\n{question}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    request = RuntimeRequest(\n",
    "        session_id=session.id,\n",
    "        user_id=session.user_id,\n",
    "        tenant_id=session.tenant_id,\n",
    "        workspace_id=session.workspace_id,\n",
    "        message=question,\n",
    "        attachments=[],          # In this notebook we focus on web search (no files)\n",
    "        metadata={\"source\": \"04_websearch_demo\"},\n",
    "    )\n",
    "\n",
    "    answer = await runtime.run(request)\n",
    "\n",
    "    # 1) Print assistant answer\n",
    "    print(\"\\nASSISTANT ANSWER:\\n\")\n",
    "    if show_full_answer:\n",
    "        print(answer.answer)\n",
    "    else:\n",
    "        print(answer.answer[:700] + (\"...\" if len(answer.answer) > 700 else \"\"))\n",
    "\n",
    "    # 2) Routing information (did RAG / websearch fire?)\n",
    "    print(\"\\n--- ROUTING INFO ---\")\n",
    "    print(f\"strategy:           {answer.route.strategy}\")\n",
    "    print(f\"used_rag:           {answer.route.used_rag}\")\n",
    "    print(f\"used_websearch:     {answer.route.used_websearch}\")\n",
    "    print(f\"used_tools:         {answer.route.used_tools}\")\n",
    "    print(f\"used_user_profile:  {answer.route.used_user_profile}\")\n",
    "\n",
    "    # 3) Debug trace: show only relevant slices (websearch / rag)\n",
    "    \n",
    "    events = answer.trace_events or []\n",
    "\n",
    "    print(\"\\n--- TRACE: RAG ---\")\n",
    "    rag_events = [e for e in events if e.component == TraceComponent.RAG]\n",
    "    if rag_events:\n",
    "        for e in rag_events:\n",
    "            print(json.dumps(e.payload or {}, indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"(no RAG trace events)\")\n",
    "\n",
    "    print(\"\\n--- TRACE: WEBSEARCH ---\")\n",
    "    web_events = [e for e in events if e.component == TraceComponent.WEBSEARCH]\n",
    "    if web_events:\n",
    "        for e in web_events:\n",
    "            print(json.dumps(e.payload or {}, indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"(no websearch trace events)\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "web_q = (\n",
    "    \"What are the most recent major changes to the OpenAI API regarding the Responses API \"\n",
    "    \"and tool calling? Provide a concise technical summary with the date of the change.\"\n",
    ")\n",
    "\n",
    "# Enable websearch for runtime and choose strategy for context generation.\n",
    "config.websearch_config = WebSearchConfig(\n",
    "    strategy=WebSearchStrategyType.MAP_REDUCE\n",
    ")\n",
    "config.websearch_config.llm.map_adapter = config.llm_adapter\n",
    "config.websearch_config.llm.reduce_adapter = config.llm_adapter\n",
    "\n",
    "# Run full RuntimeEngine pipeline (this is the real test).\n",
    "_ = await ask(web_q, show_full_answer=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
