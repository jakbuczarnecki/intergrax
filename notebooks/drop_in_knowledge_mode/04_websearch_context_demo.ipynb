{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4013d12",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c664bf",
   "metadata": {},
   "source": [
    "# 04_websearch_context_demo.ipynb\n",
    "\n",
    "This notebook demonstrates how to use **DropInKnowledgeRuntime** with:\n",
    "\n",
    "- session-based chat,\n",
    "- optional RAG (attachments ingested into a vector store),\n",
    "- **live web search** via `WebSearchExecutor`,\n",
    "\n",
    "to achieve a \"ChatGPT-like\" experience with browsing.\n",
    "\n",
    "The core configuration (LLM adapter, embeddings, vector store, runtime config)\n",
    "is initialized in a single cell, while the rest of the notebook focuses on\n",
    "testing and inspecting the web search behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and environment setup\n",
    "# --------------------------------\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LLM adapter (Ollama backend via LangChain)\n",
    "from langchain_ollama import ChatOllama\n",
    "from intergrax.llm_adapters import LangChainOllamaAdapter\n",
    "\n",
    "# Embeddings + vector store\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "\n",
    "# Drop-in knowledge runtime pieces\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "\n",
    "# Web search integration\n",
    "from intergrax.websearch.service.websearch_executor import WebSearchExecutor\n",
    "from intergrax.websearch.providers.google_cse_provider import GoogleCSEProvider\n",
    "# from intergrax.websearch.providers.bing_provider import BingWebProvider  # optional\n",
    "\n",
    "\n",
    "# 1.1 Load environment variables (API keys, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# Make sure the env vars for web search are available.\n",
    "# If you want to use Bing as well, uncomment the Bing provider later.\n",
    "os.environ[\"GOOGLE_CSE_API_KEY\"] = os.getenv(\"GOOGLE_CSE_API_KEY\") or \"\"\n",
    "os.environ[\"GOOGLE_CSE_CX\"] = os.getenv(\"GOOGLE_CSE_CX\") or \"\"\n",
    "os.environ[\"BING_SEARCH_V7_API_KEY\"] = os.getenv(\"BING_SEARCH_V7_API_KEY\") or \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162378b1",
   "metadata": {},
   "source": [
    "# Core runtime configuration (LLM + embeddings + vector store + web search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=intergrax_docs_drop_in_demo\n",
      "[intergraxVectorstoreManager] Existing count: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<intergrax.runtime.drop_in_knowledge_mode.engine.DropInKnowledgeRuntime at 0x1ef66ff7b90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Core runtime configuration (LLM + embeddings + vector store + web search)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 2.1 Session store – simple in-memory storage for chat messages & metadata.\n",
    "from intergrax.globals.settings import GLOBAL_SETTINGS\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=InMemorySessionStorage()\n",
    ")\n",
    "\n",
    "# 2.2 LLM adapter – here we use Ollama through the LangChain adapter.\n",
    "llm_adapter = LangChainOllamaAdapter(\n",
    "    chat=ChatOllama(model=GLOBAL_SETTINGS.default_ollama_model)\n",
    ")\n",
    "\n",
    "# 2.3 Embedding manager – MUST match the model used during ingestion.\n",
    "embedding_manager = EmbeddingManager(\n",
    "    provider=\"ollama\",\n",
    "    model_name=GLOBAL_SETTINGS.default_ollama_embed_model,\n",
    "    assume_ollama_dim=1536,\n",
    ")\n",
    "\n",
    "# 2.4 Vector store configuration – same collection as in the RAG demo.\n",
    "vectorstore_cfg = VSConfig(\n",
    "    provider=\"chroma\",\n",
    "    collection_name=\"intergrax_docs_drop_in_demo\",\n",
    ")\n",
    "\n",
    "vectorstore_manager = VectorstoreManager(\n",
    "    config=vectorstore_cfg,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 2.5 Web search executor – wraps one or more providers (Google CSE, Bing, etc.)\n",
    "websearch_executor = WebSearchExecutor(\n",
    "    providers=[\n",
    "        GoogleCSEProvider(),\n",
    "    ],\n",
    "    max_text_chars=None,\n",
    ")\n",
    "\n",
    "# 2.6 RuntimeConfig – single source of truth for drop-in knowledge runtime.\n",
    "config = RuntimeConfig(\n",
    "    # LLM & embeddings & vector store\n",
    "    llm_adapter=llm_adapter,\n",
    "    embedding_manager=embedding_manager,\n",
    "    vectorstore_manager=vectorstore_manager,\n",
    "\n",
    "    # RAG settings (enabled to allow attachment-based context, as in 03 demo)\n",
    "    enable_rag=True,\n",
    "\n",
    "    # Web search settings – THIS is the feature under test in this notebook.\n",
    "    enable_websearch=True,\n",
    "    websearch_executor=websearch_executor,\n",
    "    max_docs_per_query=4,  # how many docs we inject into the system prompt\n",
    "\n",
    "    # Tools / memory – kept off here to isolate the web search behavior.\n",
    "    enable_tools=False,\n",
    "    enable_user_profile_memory=True,\n",
    "\n",
    "    # Optional tenant / workspace (can be overridden per request)\n",
    "    tenant_id=\"demo-tenant\",\n",
    "    workspace_id=\"demo-workspace\",\n",
    ")\n",
    "\n",
    "# 2.7 Drop-in knowledge runtime – chat engine with RAG + web search.\n",
    "runtime = DropInKnowledgeRuntime(\n",
    "    config=config,\n",
    "    session_manager=session_manager,\n",
    ")\n",
    "\n",
    "runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc96fc4",
   "metadata": {},
   "source": [
    "### 3. Create a fresh chat session for this web search demo\n",
    "\n",
    "We create a brand new session in the in-memory store.\n",
    "All questions in this notebook will be sent under the same `session_id`\n",
    "so that the runtime can keep short-term chat history (like ChatGPT).\n",
    "\n",
    "Helper: `ask(question: str)` for interactive testing\n",
    "\n",
    "This helper:\n",
    "1. Builds a `RuntimeRequest` bound to the current session.\n",
    "2. Sends it through `DropInKnowledgeRuntime.ask(...)`.\n",
    "3. Prints:\n",
    "   - The user question\n",
    "   - The model answer (truncated optionally)\n",
    "   - Routing info (was RAG used? was web search used?)\n",
    "   - Basic debug info (web search, RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d0301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created:\n",
      "- id: b24d1076-f4a7-4827-9042-ef7acd99bd7a\n",
      "- user_id: demo-user-websearch\n",
      "- tenant_id: demo-tenant\n",
      "- workspace_id: demo-workspace\n",
      "\n",
      "================================================================================\n",
      "USER QUESTION:\n",
      "Based on that, which framework would you recommend for a production-ready multi-agent system and why?\n",
      "================================================================================\n",
      "\n",
      "ASSISTANT ANSWER:\n",
      "\n",
      "The Skreaver framework in Rust (MIT licensed) developed by Oleksandr Husiev seems to be a suitable choice for building production-ready multi-agent systems. It is described as an \"agent orchestration framework\" that allows for the design and architecture of complex AI systems, making it well-suited for this type of project.\n",
      "\n",
      "--- ROUTING INFO ---\n",
      "strategy:           llm_with_websearch\n",
      "used_rag:           False\n",
      "used_websearch:     True\n",
      "used_tools:         False\n",
      "used_user_profile:  True\n",
      "\n",
      "--- DEBUG: RAG ---\n",
      "{\n",
      "  \"enabled\": true,\n",
      "  \"used\": false,\n",
      "  \"hits_count\": 0,\n",
      "  \"where_filter\": {\n",
      "    \"session_id\": \"b24d1076-f4a7-4827-9042-ef7acd99bd7a\",\n",
      "    \"user_id\": \"demo-user-websearch\",\n",
      "    \"tenant_id\": \"demo-tenant\",\n",
      "    \"workspace_id\": \"demo-workspace\"\n",
      "  },\n",
      "  \"top_k\": 4,\n",
      "  \"score_threshold\": null\n",
      "}\n",
      "\n",
      "--- DEBUG: WEBSEARCH ---\n",
      "{\n",
      "  \"num_docs\": 4,\n",
      "  \"top_urls\": [\n",
      "    \"https://pl.linkedin.com/in/piotrniedzwiedz\",\n",
      "    \"https://pl.linkedin.com/in/oleksandr-husiev-90497082\",\n",
      "    \"https://www.sciencedirect.com/science/article/pii/S0959652624008230\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RuntimeAnswer(answer='The Skreaver framework in Rust (MIT licensed) developed by Oleksandr Husiev seems to be a suitable choice for building production-ready multi-agent systems. It is described as an \"agent orchestration framework\" that allows for the design and architecture of complex AI systems, making it well-suited for this type of project.', citations=[], route=RouteInfo(used_rag=False, used_websearch=True, used_tools=False, used_long_term_memory=False, used_user_profile=True, strategy='llm_with_websearch', extra={}), tool_calls=[], stats=RuntimeStats(total_tokens=None, input_tokens=None, output_tokens=None, rag_tokens=None, websearch_tokens=None, tool_tokens=None, duration_ms=None, extra={}), raw_model_output=None, debug_trace={'session_id': 'b24d1076-f4a7-4827-9042-ef7acd99bd7a', 'user_id': 'demo-user-websearch', 'config': {'llm_label': 'llm-adapter-ollama', 'embedding_label': 'ollama-gte-qwen2-1.5b', 'vectorstore_label': 'chroma-intergrax-docs'}, 'rag': {'enabled': True, 'used': False, 'hits_count': 0, 'where_filter': {'session_id': 'b24d1076-f4a7-4827-9042-ef7acd99bd7a', 'user_id': 'demo-user-websearch', 'tenant_id': 'demo-tenant', 'workspace_id': 'demo-workspace'}, 'top_k': 4, 'score_threshold': None}, 'history_length': 1, 'rag_chunks': 0, 'websearch': {'num_docs': 4, 'top_urls': ['https://pl.linkedin.com/in/piotrniedzwiedz', 'https://pl.linkedin.com/in/oleksandr-husiev-90497082', 'https://www.sciencedirect.com/science/article/pii/S0959652624008230']}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeAnswer, RuntimeRequest\n",
    "\n",
    "# 3.1 Create a fresh session for this demo.\n",
    "session = await session_manager.create_session(\n",
    "    session_id=str(uuid.uuid4()),\n",
    "    user_id=\"demo-user-websearch\",\n",
    "    tenant_id=\"demo-tenant\",\n",
    "    workspace_id=\"demo-workspace\",\n",
    "    metadata={\"notebook\": \"04_websearch_drop_in_web_demo\"},\n",
    ")\n",
    "\n",
    "print(\"Session created:\")\n",
    "print(f\"- id: {session.id}\")\n",
    "print(f\"- user_id: {session.user_id}\")\n",
    "print(f\"- tenant_id: {session.tenant_id}\")\n",
    "print(f\"- workspace_id: {session.workspace_id}\")\n",
    "\n",
    "\n",
    "async def ask(question: str, *, show_full_answer: bool = True) -> RuntimeAnswer:\n",
    "    \"\"\"\n",
    "    Convenience helper for this notebook.\n",
    "\n",
    "    - Sends the user's question through DropInKnowledgeRuntime.\n",
    "    - Prints the answer and basic routing/debug information.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"USER QUESTION:\\n{question}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    request = RuntimeRequest(\n",
    "        session_id=session.id,\n",
    "        user_id=session.user_id,\n",
    "        tenant_id=session.tenant_id,\n",
    "        workspace_id=session.workspace_id,\n",
    "        message=question,\n",
    "        attachments=[],          # In this notebook we focus on web search (no files)\n",
    "        metadata={\"source\": \"04_websearch_demo\"},\n",
    "    )\n",
    "\n",
    "    answer = await runtime.run(request)\n",
    "\n",
    "    # 1) Print assistant answer\n",
    "    print(\"\\nASSISTANT ANSWER:\\n\")\n",
    "    if show_full_answer:\n",
    "        print(answer.answer)\n",
    "    else:\n",
    "        print(answer.answer[:700] + (\"...\" if len(answer.answer) > 700 else \"\"))\n",
    "\n",
    "    # 2) Routing information (did RAG / websearch fire?)\n",
    "    print(\"\\n--- ROUTING INFO ---\")\n",
    "    print(f\"strategy:           {answer.route.strategy}\")\n",
    "    print(f\"used_rag:           {answer.route.used_rag}\")\n",
    "    print(f\"used_websearch:     {answer.route.used_websearch}\")\n",
    "    print(f\"used_tools:         {answer.route.used_tools}\")\n",
    "    print(f\"used_user_profile:  {answer.route.used_user_profile}\")\n",
    "\n",
    "    # 3) Debug trace: show only relevant slices (websearch / rag)\n",
    "    debug = answer.debug_trace or {}\n",
    "\n",
    "    print(\"\\n--- DEBUG: RAG ---\")\n",
    "    rag_debug = debug.get(\"rag\")\n",
    "    if rag_debug:\n",
    "        print(json.dumps(rag_debug, indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"(no RAG debug info)\")\n",
    "\n",
    "    print(\"\\n--- DEBUG: WEBSEARCH ---\")\n",
    "    web_debug = debug.get(\"websearch\")\n",
    "    if web_debug:\n",
    "        print(json.dumps(web_debug, indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"(no websearch debug info)\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "answer_2 = await ask(\n",
    "    \"Based on that, which framework would you recommend for a production-ready multi-agent system and why?\"\n",
    ")\n",
    "\n",
    "answer_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
