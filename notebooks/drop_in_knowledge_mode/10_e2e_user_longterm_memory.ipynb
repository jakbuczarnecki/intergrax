{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7045abe3",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n",
    "\n",
    "# Notebook 10 — E2E: User Long-Term Memory (LTM)\n",
    "This notebook validates the engine-integrated user long-term memory path end-to-end:\n",
    "- persistence (profile → JSON snapshot),\n",
    "- modifications (edit + soft delete),\n",
    "- semantic retrieval via `SessionManager.search_user_longterm_memory(...)`,\n",
    "- engine injection via `_step_user_longterm_memory` (SYSTEM message),\n",
    "- multi-session simulation (Session 1 → Session 2) without refactors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3a875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16655afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=intergrax_user_ltm\n",
      "[intergraxVectorstoreManager] Existing count: 17\n",
      "BOOTSTRAP OK\n",
      "USER_ID: user_e2e_ltm_001\n",
      "SESSION_1: sess_e2e_ltm_001\n",
      "SESSION_2: sess_e2e_ltm_002\n"
     ]
    }
   ],
   "source": [
    "from intergrax.llm_adapters.llm_provider import LLMAdapterRegistry, LLMProvider\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "from intergrax.memory.user_profile_manager import UserProfileManager\n",
    "from intergrax.memory.stores.in_memory_user_profile_store import InMemoryUserProfileStore\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Test identifiers\n",
    "# ---------------------------------------------------------------------\n",
    "USER_ID = \"user_e2e_ltm_001\"\n",
    "SESSION_1 = \"sess_e2e_ltm_001\"\n",
    "SESSION_2 = \"sess_e2e_ltm_002\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LLM adapter (real adapter, no wrappers)\n",
    "# - assumes env is configured (OPENAI_API_KEY etc.)\n",
    "# ---------------------------------------------------------------------\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Embeddings + vectorstore (real managers)\n",
    "# Pick providers you actually use in your repo/env.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=\"ollama\",\n",
    ")\n",
    "\n",
    "vectorstore_manager = VectorstoreManager(\n",
    "    config=VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=\"intergrax_user_ltm\",\n",
    "    ),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Stores\n",
    "# ---------------------------------------------------------------------\n",
    "session_store = InMemorySessionStorage()\n",
    "user_profile_store = InMemoryUserProfileStore()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Managers\n",
    "# ---------------------------------------------------------------------\n",
    "user_profile_manager = UserProfileManager(\n",
    "    store=user_profile_store,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=vectorstore_manager,\n",
    ")\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=session_store,\n",
    "    user_profile_manager=user_profile_manager,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Runtime config (LTM enabled, RAG disabled to isolate the feature)\n",
    "# ---------------------------------------------------------------------\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    enable_user_profile_memory=True,\n",
    "    enable_org_profile_memory=False,\n",
    "    enable_user_longterm_memory=True,\n",
    "    enable_rag=False,\n",
    "    enable_websearch=False,\n",
    "    tools_mode=\"off\",\n",
    ")\n",
    "\n",
    "runtime = DropInKnowledgeRuntime(\n",
    "    config=config,\n",
    "    session_manager=session_manager,\n",
    "    ingestion_service=None,\n",
    "    context_builder=None,\n",
    "    rag_prompt_builder=None,\n",
    "    websearch_prompt_builder=None,\n",
    "    history_prompt_builder=None,\n",
    ")\n",
    "\n",
    "print(\"BOOTSTRAP OK\")\n",
    "print(\"USER_ID:\", USER_ID)\n",
    "print(\"SESSION_1:\", SESSION_1)\n",
    "print(\"SESSION_2:\", SESSION_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369418b4",
   "metadata": {},
   "source": [
    "# (Code cell) STEP 1: Seed LTM entries + search (UserProfileManager only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ece23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 18\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 19\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 20\n",
      "STEP 1 RESULT\n",
      "- debug.enabled : True\n",
      "- debug.used    : True\n",
      "- hits_count    : 3\n",
      "- reason        : hits\n",
      "  - entry_id=6e3f92f748514695a5229e2300edb8bc score=-0.1371 title='Preference: Style' deleted=False\n",
      "  - entry_id=f62b54f999bd4291938378d57f5c6edf score=-0.2065 title='Identity: Intergrax/Mooff' deleted=False\n",
      "  - entry_id=5dccefe9a9734d76b87e040291d45207 score=-0.4516 title='Decision: LTM ownership' deleted=False\n"
     ]
    }
   ],
   "source": [
    "from intergrax.memory.user_profile_memory import UserProfileMemoryEntry, MemoryKind, MemoryImportance\n",
    "\n",
    "async def step_1_seed_and_search() -> None:\n",
    "    # 1) Ensure session exists (SessionManager is the gate)        \n",
    "    await session_manager.get_or_create_session(user_id=USER_ID, session_id=SESSION_1)\n",
    "\n",
    "    # 2) Create deterministic LTM entries\n",
    "    e1 = UserProfileMemoryEntry(\n",
    "        content=\"User builds Intergrax and Mooff. Goal: ChatGPT-like runtime with memory, RAG, tools, multi-session.\",\n",
    "        session_id=SESSION_1,\n",
    "        kind=MemoryKind.USER_FACT,\n",
    "        importance=MemoryImportance.HIGH,\n",
    "        title=\"Identity: Intergrax/Mooff\",\n",
    "        metadata={\"tags\": [\"intergrax\", \"mooff\", \"runtime\"]},\n",
    "    )\n",
    "\n",
    "    e2 = UserProfileMemoryEntry(\n",
    "        content=\"User prefers concise, technical answers. Never use emojis in code/docs.\",\n",
    "        session_id=SESSION_1,\n",
    "        kind=MemoryKind.PREFERENCE,\n",
    "        importance=MemoryImportance.HIGH,\n",
    "        title=\"Preference: Style\",\n",
    "        metadata={\"tags\": [\"style\", \"formatting\"]},\n",
    "    )\n",
    "\n",
    "    e3 = UserProfileMemoryEntry(\n",
    "        content=\"Architecture decision: UserProfileManager owns LTM index; engine uses _step_user_longterm_memory.\",\n",
    "        session_id=SESSION_1,\n",
    "        kind=MemoryKind.OTHER,\n",
    "        importance=MemoryImportance.MEDIUM,\n",
    "        title=\"Decision: LTM ownership\",\n",
    "        metadata={\"tags\": [\"ltm\", \"architecture\"]},\n",
    "    )\n",
    "\n",
    "    # 3) Persist + index\n",
    "    await user_profile_manager.add_memory_entry(USER_ID, e1)\n",
    "    await user_profile_manager.add_memory_entry(USER_ID, e2)\n",
    "    await user_profile_manager.add_memory_entry(USER_ID, e3)\n",
    "\n",
    "    # 4) Search (explicitly no threshold)\n",
    "    res = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"What am I building and what is my preferred answer style?\",\n",
    "        top_k=5,\n",
    "        score_threshold=None,\n",
    "    )\n",
    "\n",
    "    dbg = res.get(\"debug\") or {}\n",
    "    print(\"STEP 1 RESULT\")\n",
    "    print(\"- debug.enabled :\", dbg.get(\"enabled\"))\n",
    "    print(\"- debug.used    :\", dbg.get(\"used\"))\n",
    "    print(\"- hits_count    :\", dbg.get(\"hits_count\"))\n",
    "    print(\"- reason        :\", dbg.get(\"reason\"))\n",
    "\n",
    "    hits = res.get(\"hits\") or []\n",
    "    scores = res.get(\"scores\") or []\n",
    "    for h, s in zip(hits, scores):\n",
    "        print(f\"  - entry_id={h.entry_id} score={s:.4f} title={h.title!r} deleted={getattr(h, 'deleted', False)}\")\n",
    "\n",
    "await step_1_seed_and_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bfa4a",
   "metadata": {},
   "source": [
    "# (Code cell) STEP 2: Update + soft delete + re-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db78a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 20\n",
      "STEP 2 RESULT\n",
      "- debug.used  : True\n",
      "- hits_count  : 2\n",
      "- reason      : hits\n",
      "- updated_entry_id: 6e3f92f748514695a5229e2300edb8bc FOUND\n",
      "- deleted_entry_id: f62b54f999bd4291938378d57f5c6edf NOT_FOUND (OK)\n",
      "  - entry_id=6e3f92f748514695a5229e2300edb8bc score=0.0542 title='Preference: Style' deleted=False\n",
      "  - entry_id=5dccefe9a9734d76b87e040291d45207 score=-0.1545 title='Decision: LTM ownership' deleted=False\n"
     ]
    }
   ],
   "source": [
    "async def step_2_update_delete_research() -> None:\n",
    "    # 1) Get current hits so we have stable entry_ids\n",
    "    res1 = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"What is my preferred answer style?\",\n",
    "        top_k=5,\n",
    "        score_threshold=None,\n",
    "    )\n",
    "\n",
    "    hits1 = res1.get(\"hits\") or []\n",
    "    if len(hits1) < 2:\n",
    "        raise RuntimeError(f\"Expected at least 2 hits, got {len(hits1)}\")\n",
    "\n",
    "    # Choose:\n",
    "    # - update the top hit (style preference)\n",
    "    # - delete the second hit (identity or similar)\n",
    "    entry_to_update = hits1[0]\n",
    "    entry_to_delete = hits1[1]\n",
    "\n",
    "    # 2) Update (append deterministic marker)\n",
    "    updated_text = (entry_to_update.content or \"\") + \" [UPDATED_IN_NOTEBOOK_STEP_2]\"\n",
    "    entry_to_update.content = updated_text\n",
    "    entry_to_update.modified = True\n",
    "\n",
    "    # Persist update (expects your manager/store to handle modified flag)\n",
    "    await user_profile_manager.update_memory_entry(USER_ID, entry_to_update.entry_id, content=updated_text)\n",
    "\n",
    "    # 3) Soft delete\n",
    "    entry_to_delete.deleted = True\n",
    "    await user_profile_manager.remove_memory_entry(USER_ID, entry_to_delete.entry_id)\n",
    "\n",
    "    # 4) Re-search: updated marker should be retrievable, deleted entry must not appear\n",
    "    res2 = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"UPDATED_IN_NOTEBOOK_STEP_2\",\n",
    "        top_k=10,\n",
    "        score_threshold=None,\n",
    "    )\n",
    "\n",
    "    dbg2 = res2.get(\"debug\") or {}\n",
    "    print(\"STEP 2 RESULT\")\n",
    "    print(\"- debug.used  :\", dbg2.get(\"used\"))\n",
    "    print(\"- hits_count  :\", dbg2.get(\"hits_count\"))\n",
    "    print(\"- reason      :\", dbg2.get(\"reason\"))\n",
    "\n",
    "    hits2 = res2.get(\"hits\") or []\n",
    "    ids2 = [h.entry_id for h in hits2]\n",
    "\n",
    "    print(\"- updated_entry_id:\", entry_to_update.entry_id, \"FOUND\" if entry_to_update.entry_id in ids2 else \"NOT_FOUND\")\n",
    "    print(\"- deleted_entry_id:\", entry_to_delete.entry_id, \"FOUND\" if entry_to_delete.entry_id in ids2 else \"NOT_FOUND (OK)\")\n",
    "\n",
    "    for h, s in zip(res2.get(\"hits\") or [], res2.get(\"scores\") or []):\n",
    "        print(f\"  - entry_id={h.entry_id} score={s:.4f} title={h.title!r} deleted={getattr(h, 'deleted', False)}\")\n",
    "\n",
    "await step_2_update_delete_research()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d143e",
   "metadata": {},
   "source": [
    "# (Code cell) STEP 3: Session 2 -> runtime.run() and verify debug trace contains LTM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f4d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3 ANSWER\n",
      "Based on your long-term memory, it appears that you are building something related to user profiles or management, given the mention of UserProfileManager. However, without more context, it's difficult to pinpoint exactly what project this is for.\n",
      "\n",
      "As for your preferred answer style, you stated that you prefer concise and technical answers, and you mentioned never using emojis in code/docs. \n",
      "\n",
      "Here are those specifics:\n",
      "\n",
      "- Project/Build: User Profile Management\n",
      "- Preferred Answer Style: Technical and Concise\n",
      "STEP 3.1 TYPE CHECK\n",
      "type(ans.answer) = <class 'str'>\n",
      "STEP 3.1 ANSWER TEXT\n",
      "Based on your long-term memory, it appears that you are building something related to user profiles or management, given the mention of UserProfileManager. However, without more context, it's difficult to pinpoint exactly what project this is for.\n",
      "\n",
      "As for your preferred answer style, you stated that you prefer concise and technical answers, and you mentioned never using emojis in code/docs. \n",
      "\n",
      "Here are those specifics:\n",
      "\n",
      "- Project/Build: User Profile Management\n",
      "- Preferred Answer Style: Technical and Concise\n",
      "answer_len = 511\n",
      "answer_preview = \"Based on your long-term memory, it appears that you are building something related to user profiles or management, given the mention of UserProfileManager. However, without more context, it's difficul\"\n",
      "repr(ans.answer) = \"Based on your long-term memory, it appears that you are building something related to user profiles or management, given the mention of UserProfileManager. However, without more context, it's difficult to pinpoint exactly what project this is for.\\n\\nAs for your preferred answer style, you stated that you prefer concise and technical answers, and you mentioned never using emojis in code/docs. \\n\\nHere are those specifics:\\n\\n- Project/Build: User Profile Management\\n- Preferred Answer Style: Technical and Concise\"\n",
      "\n",
      "STEP 3.1 SESSION 2 HISTORY (last 6)\n",
      "- user: \"Remind me what I'm building and what answer style I prefer. Include the UPDATED_IN_NOTEBOOK_STEP_2 marker if present.\"\n",
      "- assistant: \"Based on your long-term memory, it appears that you are building something related to user profiles or management, given the mention of UserProfileManager. However, without more context, it's difficul\"\n",
      "\n",
      "STEP 3 DEBUG\n",
      "- ltm.used      : True\n",
      "- ltm.reason    : hits\n",
      "- ltm.hits_count: 2\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "\n",
    "\n",
    "async def step_3_runtime_session_2() -> None:\n",
    "    # 1) Ensure session 2 exists\n",
    "    await session_manager.get_or_create_session(user_id=USER_ID, session_id=SESSION_2)\n",
    "\n",
    "    # 2) Build canonical request\n",
    "    req = RuntimeRequest(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_2,\n",
    "        message=\"Remind me what I'm building and what answer style I prefer. Include the UPDATED_IN_NOTEBOOK_STEP_2 marker if present.\",\n",
    "        attachments=[],   # no attachments in this test\n",
    "    )\n",
    "\n",
    "    # 3) Run runtime\n",
    "    ans = await runtime.run(req)\n",
    "\n",
    "    # 4) Print answer (use the canonical field names from your RuntimeAnswer)\n",
    "    print(\"STEP 3 ANSWER\")\n",
    "    print(ans.answer)\n",
    "\n",
    "    print(\"STEP 3.1 TYPE CHECK\")\n",
    "    print(\"type(ans.answer) =\", type(ans.answer))\n",
    "\n",
    "    print(\"STEP 3.1 ANSWER TEXT\")\n",
    "    print(ans.answer)\n",
    "    print(\"answer_len =\", len(ans.answer))\n",
    "    print(\"answer_preview =\", repr(ans.answer[:200]))\n",
    "\n",
    "    # Also print a shortened repr to see what's inside\n",
    "    print(\"repr(ans.answer) =\", repr(ans.answer))\n",
    "\n",
    "    # If your session manager exposes session/history, read it to see what was persisted.\n",
    "    # Adjust names to your actual API if needed.\n",
    "    sess2 = await session_manager.get_session(session_id=SESSION_2)\n",
    "    history = await session_manager.get_history(session_id=sess2.id)\n",
    "    print(\"\\nSTEP 3.1 SESSION 2 HISTORY (last 6)\")\n",
    "    for m in history[-6:]:\n",
    "        print(f\"- {m.role}: {m.content[:200]!r}\")\n",
    "\n",
    "    # 5) Verify debug trace (canonical fields, no hasattr)\n",
    "    dbg = ans.debug_trace\n",
    "    ltm = dbg.get(\"user_longterm_memory\")\n",
    "\n",
    "    print(\"\\nSTEP 3 DEBUG\")\n",
    "    print(\"- ltm.used      :\", ltm.get(\"used\"))\n",
    "    print(\"- ltm.reason    :\", ltm.get(\"reason\"))\n",
    "    print(\"- ltm.hits_count:\", ltm.get(\"hits_count\"))\n",
    "\n",
    "    # Optional: assert-like checks (fail fast)\n",
    "    if not ltm.get(\"used\"):\n",
    "        raise RuntimeError(f\"LTM not used. Reason: {ltm.get('reason')}\")\n",
    "    if (ltm.get(\"hits_count\") or 0) <= 0:\n",
    "        raise RuntimeError(\"LTM used=True but hits_count<=0 (unexpected).\")\n",
    "\n",
    "\n",
    "await step_3_runtime_session_2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4970f87",
   "metadata": {},
   "source": [
    "# STEP 4 — Negative cases + parameter checks (UserProfileManager)\n",
    "\n",
    "Goal:\n",
    "- Ensure empty/whitespace queries are treated as a no-op (used_longterm=False, hits=[]).\n",
    "- Verify `top_k` caps the number of returned hits.\n",
    "- Smoke-check `score_threshold` (best-effort, depends on vectorstore scoring scale) without asserting exact semantics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4d2bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4 START\n",
      "- empty_query= '' used_longterm= False reason= empty_query\n",
      "- empty_query= '   ' used_longterm= False reason= empty_query\n",
      "- empty_query= '\\n\\t' used_longterm= False reason= empty_query\n",
      "- top_k=1 hits_count= 0\n",
      "- score_threshold=0.9999 used_longterm= False hits_count= 0 reason= no_hits\n",
      "STEP 4 OK\n"
     ]
    }
   ],
   "source": [
    "async def step_4_negative_and_params() -> None:\n",
    "    print(\"STEP 4 START\")\n",
    "\n",
    "    # 4.1 Empty / whitespace queries should be treated as disabled/no-op\n",
    "    for q in [\"\", \"   \", \"\\n\\t\"]:\n",
    "        res = await user_profile_manager.search_longterm_memory(\n",
    "            user_id=USER_ID,\n",
    "            query=q,\n",
    "            top_k=5,\n",
    "            score_threshold=None,\n",
    "        )\n",
    "        dbg = res.get(\"debug\") or {}\n",
    "        print(\"- empty_query=\", repr(q), \"used_longterm=\", res.get(\"used_longterm\"), \"reason=\", dbg.get(\"reason\"))\n",
    "\n",
    "        assert res.get(\"used_longterm\") is False\n",
    "        assert (res.get(\"hits\") or []) == []\n",
    "\n",
    "    # 4.2 top_k should cap results\n",
    "    res_k1 = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"Intergrax Mooff answer style\",\n",
    "        top_k=1,\n",
    "        score_threshold=None,\n",
    "    )\n",
    "    hits_k1 = res_k1.get(\"hits\") or []\n",
    "    print(\"- top_k=1 hits_count=\", len(hits_k1))\n",
    "    assert len(hits_k1) <= 1\n",
    "\n",
    "    # 4.3 score_threshold smoke-check (do not assume scoring scale)\n",
    "    res_thr = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"Intergrax Mooff\",\n",
    "        top_k=10,\n",
    "        score_threshold=0.9999,\n",
    "    )\n",
    "    dbg_thr = res_thr.get(\"debug\") or {}\n",
    "    print(\n",
    "        \"- score_threshold=0.9999 used_longterm=\",\n",
    "        res_thr.get(\"used_longterm\"),\n",
    "        \"hits_count=\",\n",
    "        len(res_thr.get(\"hits\") or []),\n",
    "        \"reason=\",\n",
    "        dbg_thr.get(\"reason\"),\n",
    "    )\n",
    "\n",
    "    print(\"STEP 4 OK\")\n",
    "\n",
    "\n",
    "await step_4_negative_and_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc439f",
   "metadata": {},
   "source": [
    "# STEP 5 — Runtime \"user-last\" invariant (record adapter input)\n",
    "\n",
    "Goal:\n",
    "- Ensure the exact message list passed to `llm_adapter.generate_messages(...)` ends with:\n",
    "  - role == \"user\"\n",
    "  - content == RuntimeRequest.message (after strip)\n",
    "\n",
    "This prevents regressions where context injection accidentally becomes the last message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0cabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 LAST ROLE: user\n",
      "STEP 5 LAST PREVIEW: 'Check user-last invariant. Reply with a short OK.'\n",
      "STEP 5 OK\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence, Optional\n",
    "\n",
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "class RecordingLLMAdapter(LLMAdapter):\n",
    "    def __init__(self, inner: LLMAdapter) -> None:\n",
    "        self._inner = inner\n",
    "        self.last_messages: Optional[Sequence[ChatMessage]] = None\n",
    "\n",
    "    def generate_messages(\n",
    "        self,\n",
    "        messages: Sequence[ChatMessage],\n",
    "        *,\n",
    "        temperature: Optional[float] = None,\n",
    "        max_tokens: Optional[int] = None,\n",
    "    ) -> str:\n",
    "        self.last_messages = list(messages)\n",
    "        return self._inner.generate_messages(\n",
    "            messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "rec_adapter = RecordingLLMAdapter(runtime._config.llm_adapter)\n",
    "runtime._config.llm_adapter = rec_adapter\n",
    "\n",
    "\n",
    "async def step_5_user_last_invariant() -> None:\n",
    "    req = RuntimeRequest(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_2,\n",
    "        message=\"Check user-last invariant. Reply with a short OK.\",\n",
    "        attachments=[],\n",
    "    )\n",
    "    _ = await runtime.run(req)\n",
    "\n",
    "    msgs = list(rec_adapter.last_messages or [])\n",
    "    assert len(msgs) > 0, \"Adapter did not record messages\"\n",
    "\n",
    "    last = msgs[-1]\n",
    "    print(\"STEP 5 LAST ROLE:\", last.role)\n",
    "    print(\"STEP 5 LAST PREVIEW:\", repr((last.content or \"\")[:160]))\n",
    "\n",
    "    assert last.role == \"user\", f\"Expected last role user, got {last.role}\"\n",
    "    assert (last.content or \"\").strip() == req.message.strip(), \"Last user content does not match request message\"\n",
    "\n",
    "    print(\"STEP 5 OK\")\n",
    "\n",
    "\n",
    "await step_5_user_last_invariant()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d750a",
   "metadata": {},
   "source": [
    "# STEP 6 — Multi-session LTM persistence sanity\n",
    "\n",
    "Goal:\n",
    "- Create a new session and run the runtime again.\n",
    "- Verify that user LTM is still used (persisted outside the session scope).\n",
    "- Assert that LTM hits are returned in debug_trace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1a4193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6 ANSWER PREVIEW\n",
      "You're building a system that involves the User Profile Manager owning the LTM (Long-Term Memory) index, and an engine that utilizes the \"_step_user_longterm_memory\" functionality.\n",
      "\n",
      "As for your preferred answer style, you have specified that you prefer concise, technical answers without using emojis\n",
      "STEP 6 DEBUG\n",
      "- used      : True\n",
      "- reason    : hits\n",
      "- hits_count: 2\n",
      "STEP 6 OK\n"
     ]
    }
   ],
   "source": [
    "SESSION_3 = \"sess_e2e_ltm_003\"\n",
    "\n",
    "async def step_6_multi_session_ltm() -> None:\n",
    "    await session_manager.get_or_create_session(user_id=USER_ID, session_id=SESSION_3)\n",
    "\n",
    "    req = RuntimeRequest(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_3,\n",
    "        message=\"Remind me what I'm building and what answer style I prefer.\",\n",
    "        attachments=[],\n",
    "    )\n",
    "    ans = await runtime.run(req)\n",
    "\n",
    "    print(\"STEP 6 ANSWER PREVIEW\")\n",
    "    print(ans.answer[:300])\n",
    "\n",
    "    ltm = (ans.debug_trace or {}).get(\"user_longterm_memory\") or {}\n",
    "    print(\"STEP 6 DEBUG\")\n",
    "    print(\"- used      :\", ltm.get(\"used\"))\n",
    "    print(\"- reason    :\", ltm.get(\"reason\"))\n",
    "    print(\"- hits_count:\", ltm.get(\"hits_count\"))\n",
    "\n",
    "    assert ltm.get(\"used\") is True\n",
    "    assert (ltm.get(\"hits_count\") or 0) > 0\n",
    "\n",
    "    print(\"STEP 6 OK\")\n",
    "\n",
    "\n",
    "await step_6_multi_session_ltm()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
