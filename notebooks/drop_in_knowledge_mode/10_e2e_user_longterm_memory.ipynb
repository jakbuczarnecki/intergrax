{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7045abe3",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n",
    "\n",
    "# Notebook 10 — E2E: User Long-Term Memory (LTM)\n",
    "This notebook validates the engine-integrated user long-term memory path end-to-end:\n",
    "- persistence (profile → JSON snapshot),\n",
    "- modifications (edit + soft delete),\n",
    "- semantic retrieval via `SessionManager.search_user_longterm_memory(...)`,\n",
    "- engine injection via `_step_user_longterm_memory` (SYSTEM message),\n",
    "- multi-session simulation (Session 1 → Session 2) without refactors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3a875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16655afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=intergrax_user_ltm\n",
      "[intergraxVectorstoreManager] Existing count: 3\n",
      "BOOTSTRAP OK\n",
      "USER_ID: user_e2e_ltm_001\n",
      "SESSION_1: sess_e2e_ltm_001\n",
      "SESSION_2: sess_e2e_ltm_002\n"
     ]
    }
   ],
   "source": [
    "from intergrax.llm_adapters.base import LLMAdapter, LLMAdapterRegistry, LLMProvider\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "from intergrax.memory.user_profile_manager import UserProfileManager\n",
    "from intergrax.memory.stores.in_memory_user_profile_store import InMemoryUserProfileStore\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Test identifiers\n",
    "# ---------------------------------------------------------------------\n",
    "USER_ID = \"user_e2e_ltm_001\"\n",
    "SESSION_1 = \"sess_e2e_ltm_001\"\n",
    "SESSION_2 = \"sess_e2e_ltm_002\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LLM adapter (real adapter, no wrappers)\n",
    "# - assumes env is configured (OPENAI_API_KEY etc.)\n",
    "# ---------------------------------------------------------------------\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Embeddings + vectorstore (real managers)\n",
    "# Pick providers you actually use in your repo/env.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=\"ollama\",\n",
    ")\n",
    "\n",
    "vectorstore_manager = VectorstoreManager(\n",
    "    config=VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=\"intergrax_user_ltm\",\n",
    "    ),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Stores\n",
    "# ---------------------------------------------------------------------\n",
    "session_store = InMemorySessionStorage()\n",
    "user_profile_store = InMemoryUserProfileStore()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Managers\n",
    "# ---------------------------------------------------------------------\n",
    "user_profile_manager = UserProfileManager(\n",
    "    store=user_profile_store,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=vectorstore_manager,\n",
    ")\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=session_store,\n",
    "    user_profile_manager=user_profile_manager,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Runtime config (LTM enabled, RAG disabled to isolate the feature)\n",
    "# ---------------------------------------------------------------------\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    enable_user_profile_memory=True,\n",
    "    enable_org_profile_memory=False,\n",
    "    enable_user_longterm_memory=True,\n",
    "    enable_rag=False,\n",
    "    enable_websearch=False,\n",
    "    tools_mode=\"off\",\n",
    ")\n",
    "\n",
    "runtime = DropInKnowledgeRuntime(\n",
    "    config=config,\n",
    "    session_manager=session_manager,\n",
    "    ingestion_service=None,\n",
    "    context_builder=None,\n",
    "    rag_prompt_builder=None,\n",
    "    websearch_prompt_builder=None,\n",
    "    history_prompt_builder=None,\n",
    ")\n",
    "\n",
    "print(\"BOOTSTRAP OK\")\n",
    "print(\"USER_ID:\", USER_ID)\n",
    "print(\"SESSION_1:\", SESSION_1)\n",
    "print(\"SESSION_2:\", SESSION_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369418b4",
   "metadata": {},
   "source": [
    "# (Code cell) STEP 1: Seed LTM entries + search (UserProfileManager only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ece23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 4\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 5\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 6\n",
      "STEP 1 RESULT\n",
      "- debug.enabled : True\n",
      "- debug.used    : True\n",
      "- hits_count    : 3\n",
      "- reason        : hits\n",
      "  - entry_id=70964af4d7244cb5a4413fdc10243af4 score=-0.1371 title='Preference: Style' deleted=False\n",
      "  - entry_id=6396b7df62e94c55b4a847c55ab1d01d score=-0.2065 title='Identity: Intergrax/Mooff' deleted=False\n",
      "  - entry_id=14164063a9614cf2ac9b616c35b75531 score=-0.4516 title='Decision: LTM ownership' deleted=False\n"
     ]
    }
   ],
   "source": [
    "from intergrax.memory.user_profile_memory import UserProfileMemoryEntry, MemoryKind, MemoryImportance\n",
    "\n",
    "async def step_1_seed_and_search() -> None:\n",
    "    # 1) Ensure session exists (SessionManager is the gate)        \n",
    "    await session_manager.get_or_create_session(user_id=USER_ID, session_id=SESSION_1)\n",
    "\n",
    "    # 2) Create deterministic LTM entries\n",
    "    e1 = UserProfileMemoryEntry(\n",
    "        content=\"User builds Intergrax and Mooff. Goal: ChatGPT-like runtime with memory, RAG, tools, multi-session.\",\n",
    "        session_id=SESSION_1,\n",
    "        kind=MemoryKind.USER_FACT,\n",
    "        importance=MemoryImportance.HIGH,\n",
    "        title=\"Identity: Intergrax/Mooff\",\n",
    "        metadata={\"tags\": [\"intergrax\", \"mooff\", \"runtime\"]},\n",
    "    )\n",
    "\n",
    "    e2 = UserProfileMemoryEntry(\n",
    "        content=\"User prefers concise, technical answers. Never use emojis in code/docs.\",\n",
    "        session_id=SESSION_1,\n",
    "        kind=MemoryKind.PREFERENCE,\n",
    "        importance=MemoryImportance.HIGH,\n",
    "        title=\"Preference: Style\",\n",
    "        metadata={\"tags\": [\"style\", \"formatting\"]},\n",
    "    )\n",
    "\n",
    "    e3 = UserProfileMemoryEntry(\n",
    "        content=\"Architecture decision: UserProfileManager owns LTM index; engine uses _step_user_longterm_memory.\",\n",
    "        session_id=SESSION_1,\n",
    "        kind=MemoryKind.OTHER,\n",
    "        importance=MemoryImportance.MEDIUM,\n",
    "        title=\"Decision: LTM ownership\",\n",
    "        metadata={\"tags\": [\"ltm\", \"architecture\"]},\n",
    "    )\n",
    "\n",
    "    # 3) Persist + index\n",
    "    await user_profile_manager.add_memory_entry(USER_ID, e1)\n",
    "    await user_profile_manager.add_memory_entry(USER_ID, e2)\n",
    "    await user_profile_manager.add_memory_entry(USER_ID, e3)\n",
    "\n",
    "    # 4) Search (explicitly no threshold)\n",
    "    res = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"What am I building and what is my preferred answer style?\",\n",
    "        top_k=5,\n",
    "        score_threshold=None,\n",
    "    )\n",
    "\n",
    "    dbg = res.get(\"debug\") or {}\n",
    "    print(\"STEP 1 RESULT\")\n",
    "    print(\"- debug.enabled :\", dbg.get(\"enabled\"))\n",
    "    print(\"- debug.used    :\", dbg.get(\"used\"))\n",
    "    print(\"- hits_count    :\", dbg.get(\"hits_count\"))\n",
    "    print(\"- reason        :\", dbg.get(\"reason\"))\n",
    "\n",
    "    hits = res.get(\"hits\") or []\n",
    "    scores = res.get(\"scores\") or []\n",
    "    for h, s in zip(hits, scores):\n",
    "        print(f\"  - entry_id={h.entry_id} score={s:.4f} title={h.title!r} deleted={getattr(h, 'deleted', False)}\")\n",
    "\n",
    "await step_1_seed_and_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bfa4a",
   "metadata": {},
   "source": [
    "# (Code cell) STEP 2: Update + soft delete + re-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db78a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2 RESULT\n",
      "- debug.used  : True\n",
      "- hits_count  : 2\n",
      "- reason      : hits\n",
      "- updated_entry_id: 70964af4d7244cb5a4413fdc10243af4 FOUND\n",
      "- deleted_entry_id: 6396b7df62e94c55b4a847c55ab1d01d NOT_FOUND (OK)\n",
      "  - entry_id=14164063a9614cf2ac9b616c35b75531 score=-0.1545 title='Decision: LTM ownership' deleted=False\n",
      "  - entry_id=70964af4d7244cb5a4413fdc10243af4 score=-0.4100 title='Preference: Style' deleted=False\n"
     ]
    }
   ],
   "source": [
    "async def step_2_update_delete_research() -> None:\n",
    "    # 1) Get current hits so we have stable entry_ids\n",
    "    res1 = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"What is my preferred answer style?\",\n",
    "        top_k=5,\n",
    "        score_threshold=None,\n",
    "    )\n",
    "\n",
    "    hits1 = res1.get(\"hits\") or []\n",
    "    if len(hits1) < 2:\n",
    "        raise RuntimeError(f\"Expected at least 2 hits, got {len(hits1)}\")\n",
    "\n",
    "    # Choose:\n",
    "    # - update the top hit (style preference)\n",
    "    # - delete the second hit (identity or similar)\n",
    "    entry_to_update = hits1[0]\n",
    "    entry_to_delete = hits1[1]\n",
    "\n",
    "    # 2) Update (append deterministic marker)\n",
    "    updated_text = (entry_to_update.content or \"\") + \" [UPDATED_IN_NOTEBOOK_STEP_2]\"\n",
    "    entry_to_update.content = updated_text\n",
    "    entry_to_update.modified = True\n",
    "\n",
    "    # Persist update (expects your manager/store to handle modified flag)\n",
    "    await user_profile_manager.update_memory_entry(USER_ID, entry_to_update)\n",
    "\n",
    "    # 3) Soft delete\n",
    "    entry_to_delete.deleted = True\n",
    "    await user_profile_manager.remove_memory_entry(USER_ID, entry_to_delete.entry_id)\n",
    "\n",
    "    # 4) Re-search: updated marker should be retrievable, deleted entry must not appear\n",
    "    res2 = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"UPDATED_IN_NOTEBOOK_STEP_2\",\n",
    "        top_k=10,\n",
    "        score_threshold=None,\n",
    "    )\n",
    "\n",
    "    dbg2 = res2.get(\"debug\") or {}\n",
    "    print(\"STEP 2 RESULT\")\n",
    "    print(\"- debug.used  :\", dbg2.get(\"used\"))\n",
    "    print(\"- hits_count  :\", dbg2.get(\"hits_count\"))\n",
    "    print(\"- reason      :\", dbg2.get(\"reason\"))\n",
    "\n",
    "    hits2 = res2.get(\"hits\") or []\n",
    "    ids2 = [h.entry_id for h in hits2]\n",
    "\n",
    "    print(\"- updated_entry_id:\", entry_to_update.entry_id, \"FOUND\" if entry_to_update.entry_id in ids2 else \"NOT_FOUND\")\n",
    "    print(\"- deleted_entry_id:\", entry_to_delete.entry_id, \"FOUND\" if entry_to_delete.entry_id in ids2 else \"NOT_FOUND (OK)\")\n",
    "\n",
    "    for h, s in zip(res2.get(\"hits\") or [], res2.get(\"scores\") or []):\n",
    "        print(f\"  - entry_id={h.entry_id} score={s:.4f} title={h.title!r} deleted={getattr(h, 'deleted', False)}\")\n",
    "\n",
    "await step_2_update_delete_research()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d143e",
   "metadata": {},
   "source": [
    "# (Code cell) STEP 3: Session 2 -> runtime.run() and verify debug trace contains LTM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f4d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3 ANSWER\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-12-18T15:51:19.1444991Z', 'done': True, 'done_reason': 'stop', 'total_duration': 508742200, 'load_duration': 404404000, 'prompt_eval_count': 1072, 'prompt_eval_duration': 84641100, 'eval_count': 1, 'eval_duration': None, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None)} id='run--83f35ee3-f138-42ae-84e5-c4c4d579bdaf-0' usage_metadata={'input_tokens': 1072, 'output_tokens': 1, 'total_tokens': 1073}\n",
      "STEP 3.1 TYPE CHECK\n",
      "type(ans.answer) = <class 'str'>\n",
      "STEP 3.1 ANSWER TEXT\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-12-18T15:51:19.1444991Z', 'done': True, 'done_reason': 'stop', 'total_duration': 508742200, 'load_duration': 404404000, 'prompt_eval_count': 1072, 'prompt_eval_duration': 84641100, 'eval_count': 1, 'eval_duration': None, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None)} id='run--83f35ee3-f138-42ae-84e5-c4c4d579bdaf-0' usage_metadata={'input_tokens': 1072, 'output_tokens': 1, 'total_tokens': 1073}\n",
      "answer_len = 559\n",
      "answer_preview = \"content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-12-18T15:51:19.1444991Z', 'done': True, 'done_reason': 'stop', 'total_duration': 508742200, 'load_dur\"\n",
      "repr(ans.answer) = \"content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-12-18T15:51:19.1444991Z', 'done': True, 'done_reason': 'stop', 'total_duration': 508742200, 'load_duration': 404404000, 'prompt_eval_count': 1072, 'prompt_eval_duration': 84641100, 'eval_count': 1, 'eval_duration': None, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None)} id='run--83f35ee3-f138-42ae-84e5-c4c4d579bdaf-0' usage_metadata={'input_tokens': 1072, 'output_tokens': 1, 'total_tokens': 1073}\"\n",
      "\n",
      "STEP 3.1 SESSION 2 HISTORY (last 6)\n",
      "- user: \"Remind me what I'm building and what answer style I prefer. Include the UPDATED_IN_NOTEBOOK_STEP_2 marker if present.\"\n",
      "- assistant: \"content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-12-18T15:48:57.9607612Z', 'done': True, 'done_reason': 'stop', 'total_duration': 486227700, 'load_dur\"\n",
      "- user: \"Remind me what I'm building and what answer style I prefer. Include the UPDATED_IN_NOTEBOOK_STEP_2 marker if present.\"\n",
      "- assistant: \"content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-12-18T15:49:27.3158326Z', 'done': True, 'done_reason': 'stop', 'total_duration': 506950300, 'load_dur\"\n",
      "- user: \"Remind me what I'm building and what answer style I prefer. Include the UPDATED_IN_NOTEBOOK_STEP_2 marker if present.\"\n",
      "- assistant: \"content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-12-18T15:51:19.1444991Z', 'done': True, 'done_reason': 'stop', 'total_duration': 508742200, 'load_dur\"\n",
      "\n",
      "STEP 3 DEBUG\n",
      "- ltm.used      : True\n",
      "- ltm.reason    : hits\n",
      "- ltm.hits_count: 2\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "\n",
    "\n",
    "async def step_3_runtime_session_2() -> None:\n",
    "    # 1) Ensure session 2 exists\n",
    "    await session_manager.get_or_create_session(user_id=USER_ID, session_id=SESSION_2)\n",
    "\n",
    "    # 2) Build canonical request\n",
    "    req = RuntimeRequest(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_2,\n",
    "        message=\"Remind me what I'm building and what answer style I prefer. Include the UPDATED_IN_NOTEBOOK_STEP_2 marker if present.\",\n",
    "        attachments=[],   # no attachments in this test\n",
    "    )\n",
    "\n",
    "    # 3) Run runtime\n",
    "    ans = await runtime.run(req)\n",
    "\n",
    "    # 4) Print answer (use the canonical field names from your RuntimeAnswer)\n",
    "    print(\"STEP 3 ANSWER\")\n",
    "    print(ans.answer)\n",
    "\n",
    "    print(\"STEP 3.1 TYPE CHECK\")\n",
    "    print(\"type(ans.answer) =\", type(ans.answer))\n",
    "\n",
    "    print(\"STEP 3.1 ANSWER TEXT\")\n",
    "    print(ans.answer)\n",
    "    print(\"answer_len =\", len(ans.answer))\n",
    "    print(\"answer_preview =\", repr(ans.answer[:200]))\n",
    "\n",
    "    # Also print a shortened repr to see what's inside\n",
    "    print(\"repr(ans.answer) =\", repr(ans.answer))\n",
    "\n",
    "    # If your session manager exposes session/history, read it to see what was persisted.\n",
    "    # Adjust names to your actual API if needed.\n",
    "    sess2 = await session_manager.get_session(session_id=SESSION_2)\n",
    "    history = await session_manager.get_history(session_id=sess2.id)\n",
    "    print(\"\\nSTEP 3.1 SESSION 2 HISTORY (last 6)\")\n",
    "    for m in history[-6:]:\n",
    "        print(f\"- {m.role}: {m.content[:200]!r}\")\n",
    "\n",
    "    # 5) Verify debug trace (canonical fields, no hasattr)\n",
    "    dbg = ans.debug_trace\n",
    "    ltm = dbg.get(\"user_longterm_memory\")\n",
    "\n",
    "    print(\"\\nSTEP 3 DEBUG\")\n",
    "    print(\"- ltm.used      :\", ltm.get(\"used\"))\n",
    "    print(\"- ltm.reason    :\", ltm.get(\"reason\"))\n",
    "    print(\"- ltm.hits_count:\", ltm.get(\"hits_count\"))\n",
    "\n",
    "    # Optional: assert-like checks (fail fast)\n",
    "    if not ltm.get(\"used\"):\n",
    "        raise RuntimeError(f\"LTM not used. Reason: {ltm.get('reason')}\")\n",
    "    if (ltm.get(\"hits_count\") or 0) <= 0:\n",
    "        raise RuntimeError(\"LTM used=True but hits_count<=0 (unexpected).\")\n",
    "\n",
    "\n",
    "await step_3_runtime_session_2()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
