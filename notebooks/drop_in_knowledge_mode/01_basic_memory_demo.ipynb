{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c942348",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21db894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b1198",
   "metadata": {},
   "source": [
    "Basic sanity-check notebook for Drop-In Knowledge Mode runtime.\n",
    "\n",
    "Goals:\n",
    "  - Verify that DropInKnowledgeRuntime can:\n",
    "      * create or load a session,\n",
    "      * append user and assistant messages,\n",
    "      * build conversation history from SessionStore,\n",
    "      * return a RuntimeAnswer object.\n",
    "  - Use InMemorySessionStore for simplicity.\n",
    "  - Keep LLM integration as placeholder for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b92221",
   "metadata": {},
   "source": [
    "Create config, session store, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3966c12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Runtime initialized at 2025-12-26T10:15:46.885875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS\\AppData\\Local\\Temp\\ipykernel_10828\\287699158.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(\"Runtime initialized at\", datetime.utcnow().isoformat())\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "\n",
    "# Instantiate the in-memory session store\n",
    "session_manager = SessionManager(\n",
    "    storage=InMemorySessionStorage()\n",
    ")\n",
    "\n",
    "# Instantiate a LLM adapter\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "TENANT = \"intergrax\"\n",
    "CORPUS = \"intergrax-strategy\"\n",
    "VERSION = \"v1\"\n",
    "\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    enable_rag=False,        # disabled for this initial sanity check\n",
    "    enable_websearch=False,  # disabled for this initial sanity check\n",
    "    tools_mode=\"off\",      # disabled for this initial sanity check\n",
    "    enable_llm_usage_collection=True,\n",
    ")\n",
    "\n",
    "runtime = DropInKnowledgeRuntime(\n",
    "    config=config,\n",
    "    session_manager=session_manager,\n",
    ")\n",
    "\n",
    "print(\"Runtime initialized at\", datetime.utcnow().isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3fd97",
   "metadata": {},
   "source": [
    "First request: create a new session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c456f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER 1 ===\n",
      "Welcome to the Intergrax runtime! This is an exciting new platform for experimentation and innovation.\n",
      "\n",
      "I'm here to help you explore the capabilities of the Intergrax runtime and support you in achieving your goals. What would you like to do or discuss? You can ask me any questions, share ideas, or let's just chat about what Intergrax has to offer!\n",
      "\n",
      "Route info:\n",
      "RouteInfo(used_rag=False, used_websearch=False, used_tools=False, used_user_profile=False, used_user_longterm_memory=False, strategy='llm_only', extra={})\n",
      "\n",
      "Stats:\n",
      "RuntimeStats(total_tokens=None, input_tokens=None, output_tokens=None, rag_tokens=None, websearch_tokens=None, tool_tokens=None, duration_ms=None, extra={})\n",
      "\n",
      "Debug trace:\n",
      "{'session_id': 'demo-session-001', 'user_id': 'demo-user-001', 'memory_layer': {'implemented': True, 'has_user_profile_instructions': False, 'has_org_profile_instructions': False, 'enable_user_profile_memory': True, 'enable_org_profile_memory': True}, 'steps': [{'timestamp': '2025-12-26T10:15:46.899639+00:00', 'component': 'engine', 'step': 'memory_layer', 'message': 'Profile-based instructions loaded for session.', 'data': {'has_user_profile_instructions': False, 'has_org_profile_instructions': False, 'enable_user_profile_memory': True, 'enable_org_profile_memory': True}}, {'timestamp': '2025-12-26T10:15:47.002126+00:00', 'component': 'engine', 'step': 'history', 'message': 'Conversation history built for LLM.', 'data': {'history_length': 1, 'base_history_length': 1, 'history_includes_current_user': True}}, {'timestamp': '2025-12-26T10:15:49.172904+00:00', 'component': 'engine', 'step': 'core_llm', 'message': 'Core LLM adapter returned answer.', 'data': {'used_tools_answer': False, 'adapter_return_type': 'str', 'answer_len': 350, 'answer_is_empty': False}}, {'timestamp': '2025-12-26T10:15:49.172904+00:00', 'component': 'engine', 'step': 'persist_and_build_answer', 'message': 'Assistant answer persisted and RuntimeAnswer built.', 'data': {'session_id': 'demo-session-001', 'strategy': 'llm_only', 'used_rag': False, 'used_websearch': False, 'used_tools': False}}, {'timestamp': '2025-12-26T10:15:49.172904+00:00', 'component': 'engine', 'step': 'run_end', 'message': 'DropInKnowledgeRuntime.run() finished.', 'data': {'strategy': 'llm_only', 'used_rag': False, 'used_websearch': False, 'used_tools': False, 'used_user_longterm_memory': False, 'run_id': 'run_73aaf58a15d948868cf7a430a68450dc'}}], 'base_history_length': 1, 'history_tokens': {'raw_history_messages': 1, 'raw_history_tokens': 12, 'history_budget_tokens': 4096, 'strategy_requested': 'truncate_oldest', 'strategy_effective': 'truncate_oldest', 'truncated': False, 'summary_used': False, 'summary_tokens_budget': 0, 'tail_tokens_budget': 0}, 'history_length': 1, 'instructions': {'has_instructions': False, 'sources': {'request': False, 'user_profile': False, 'organization_profile': False}}, 'rag_chunks': 0, 'user_longterm_memory_hits': 0, 'user_longterm_memory': {}, 'llm_usage': {'run_id': 'run_73aaf58a15d948868cf7a430a68450dc', 'total': {'calls': 1, 'input_tokens': 12, 'output_tokens': 75, 'total_tokens': 87, 'duration_ms': 2170, 'errors': 0}, 'entries': [{'label': 'core_adapter', 'meta': {'adapter_type': 'LangChainOllamaAdapter', 'provider': <LLMProvider.OLLAMA: 'ollama'>, 'model': 'llama3.1:latest'}, 'stats': {'calls': 1, 'input_tokens': 12, 'output_tokens': 75, 'total_tokens': 87, 'duration_ms': 2170, 'errors': 0}, 'adapter_instance_id': 1243408166896}], 'by_provider_model': {'LLMProvider.OLLAMA:llama3.1:latest': {'calls': 1, 'input_tokens': 12, 'output_tokens': 75, 'total_tokens': 87, 'duration_ms': 2170, 'errors': 0}}, 'adapter_instance_ids': {'core_adapter': 1243408166896}}}\n"
     ]
    }
   ],
   "source": [
    "# Define user and session identifiers\n",
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "\n",
    "\n",
    "user_id = \"demo-user-001\"\n",
    "session_id = \"demo-session-001\"\n",
    "\n",
    "request_1 = RuntimeRequest(\n",
    "    user_id=user_id,\n",
    "    session_id=session_id,\n",
    "    message=\"Hello Intergrax runtime, this is my first message.\",\n",
    "    metadata={\"source\": \"notebook-demo\"},\n",
    ")\n",
    "\n",
    "answer_1 = await runtime.run(request_1)\n",
    "\n",
    "print(\"=== ANSWER 1 ===\")\n",
    "print(answer_1.answer)\n",
    "print(\"\\nRoute info:\")\n",
    "print(answer_1.route)\n",
    "print(\"\\nStats:\")\n",
    "print(answer_1.stats)\n",
    "print(\"\\nDebug trace:\")\n",
    "print(answer_1.debug_trace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38286f",
   "metadata": {},
   "source": [
    "Inspect session state after first request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a001334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SESSION AFTER FIRST REQUEST ===\n",
      "Session ID: demo-session-001\n",
      "User ID: demo-user-001\n",
      "Messages count: 2\n",
      "Created at: 2025-12-26 10:15:46.899639+00:00\n",
      "Updated at: 2025-12-26 10:15:49.172904+00:00\n",
      "\n",
      "Messages:\n",
      "  [1] role='user', created_at=2025-12-26T10:15:46.899639+00:00\n",
      "      content='Hello Intergrax runtime, this is my first message.'\n",
      "  [2] role='assistant', created_at=2025-12-26T10:15:49.172904+00:00\n",
      "      content=\"Welcome to the Intergrax runtime! This is an exciting new platform for experimentation and innovation.\\n\\nI'm here to help you explore the capabilities of the Intergrax runtime and support you in achieving your goals. What would you like to do or discuss? You can ask me any questions, share ideas, or let's just chat about what Intergrax has to offer!\"\n"
     ]
    }
   ],
   "source": [
    "session = await session_manager.get_session(answer_1.debug_trace[\"session_id\"])\n",
    "messages = await session_manager.get_history(session_id=answer_1.debug_trace[\"session_id\"])\n",
    "\n",
    "print(\"=== SESSION AFTER FIRST REQUEST ===\")\n",
    "print(\"Session ID:\", session.id)\n",
    "print(\"User ID:\", session.user_id)\n",
    "print(\"Messages count:\", len(messages))\n",
    "print(\"Created at:\", session.created_at)\n",
    "print(\"Updated at:\", session.updated_at)\n",
    "\n",
    "print(\"\\nMessages:\")\n",
    "for idx, msg in enumerate(messages, start=1):\n",
    "    print(f\"  [{idx}] role={msg.role!r}, created_at={msg.created_at}\")\n",
    "    print(f\"      content={msg.content!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963297b9",
   "metadata": {},
   "source": [
    "Second request: same session, history should grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd54387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER 2 ===\n",
      "You've sent a follow-up message. How are things progressing on the Intergrax runtime so far? Have you encountered any interesting developments or would you like some guidance on how to proceed? I'm here to help and provide support as needed. What's on your mind?\n",
      "\n",
      "Route info:\n",
      "RouteInfo(used_rag=False, used_websearch=False, used_tools=False, used_user_profile=False, used_user_longterm_memory=False, strategy='llm_only', extra={})\n",
      "\n",
      "Debug trace:\n",
      "{'session_id': 'demo-session-001', 'user_id': 'demo-user-001', 'memory_layer': {'implemented': True, 'has_user_profile_instructions': False, 'has_org_profile_instructions': False, 'enable_user_profile_memory': True, 'enable_org_profile_memory': True}, 'steps': [{'timestamp': '2025-12-26T10:15:49.188643+00:00', 'component': 'engine', 'step': 'memory_layer', 'message': 'Profile-based instructions loaded for session.', 'data': {'has_user_profile_instructions': False, 'has_org_profile_instructions': False, 'enable_user_profile_memory': True, 'enable_org_profile_memory': True}}, {'timestamp': '2025-12-26T10:15:49.188643+00:00', 'component': 'engine', 'step': 'history', 'message': 'Conversation history built for LLM.', 'data': {'history_length': 3, 'base_history_length': 3, 'history_includes_current_user': True}}, {'timestamp': '2025-12-26T10:15:50.700592+00:00', 'component': 'engine', 'step': 'core_llm', 'message': 'Core LLM adapter returned answer.', 'data': {'used_tools_answer': False, 'adapter_return_type': 'str', 'answer_len': 262, 'answer_is_empty': False}}, {'timestamp': '2025-12-26T10:15:50.700592+00:00', 'component': 'engine', 'step': 'persist_and_build_answer', 'message': 'Assistant answer persisted and RuntimeAnswer built.', 'data': {'session_id': 'demo-session-001', 'strategy': 'llm_only', 'used_rag': False, 'used_websearch': False, 'used_tools': False}}, {'timestamp': '2025-12-26T10:15:50.700592+00:00', 'component': 'engine', 'step': 'run_end', 'message': 'DropInKnowledgeRuntime.run() finished.', 'data': {'strategy': 'llm_only', 'used_rag': False, 'used_websearch': False, 'used_tools': False, 'used_user_longterm_memory': False, 'run_id': 'run_a4904699d0104c80a30a869f5a833664'}}], 'base_history_length': 3, 'history_tokens': {'raw_history_messages': 3, 'raw_history_tokens': 97, 'history_budget_tokens': 4096, 'strategy_requested': 'truncate_oldest', 'strategy_effective': 'truncate_oldest', 'truncated': False, 'summary_used': False, 'summary_tokens_budget': 0, 'tail_tokens_budget': 0}, 'history_length': 3, 'instructions': {'has_instructions': False, 'sources': {'request': False, 'user_profile': False, 'organization_profile': False}}, 'rag_chunks': 0, 'user_longterm_memory_hits': 0, 'user_longterm_memory': {}, 'llm_usage': {'run_id': 'run_a4904699d0104c80a30a869f5a833664', 'total': {'calls': 1, 'input_tokens': 97, 'output_tokens': 55, 'total_tokens': 152, 'duration_ms': 1512, 'errors': 0}, 'entries': [{'label': 'core_adapter', 'meta': {'adapter_type': 'LangChainOllamaAdapter', 'provider': <LLMProvider.OLLAMA: 'ollama'>, 'model': 'llama3.1:latest'}, 'stats': {'calls': 1, 'input_tokens': 97, 'output_tokens': 55, 'total_tokens': 152, 'duration_ms': 1512, 'errors': 0}, 'adapter_instance_id': 1243408166896}], 'by_provider_model': {'LLMProvider.OLLAMA:llama3.1:latest': {'calls': 1, 'input_tokens': 97, 'output_tokens': 55, 'total_tokens': 152, 'duration_ms': 1512, 'errors': 0}}, 'adapter_instance_ids': {'core_adapter': 1243408166896}}}\n",
      "\n",
      "=== SESSION AFTER SECOND REQUEST ===\n",
      "Session ID: demo-session-001\n",
      "Messages count: 4\n",
      "  [1] role='user', created_at=2025-12-26T10:15:46.899639+00:00\n",
      "      content='Hello Intergrax runtime, this is my first message.'\n",
      "  [2] role='assistant', created_at=2025-12-26T10:15:49.172904+00:00\n",
      "      content=\"Welcome to the Intergrax runtime! This is an exciting new platform for experimentation and innovation.\\n\\nI'm here to help you explore the capabilities of the Intergrax runtime and support you in achieving your goals. What would you like to do or discuss? You can ask me any questions, share ideas, or let's just chat about what Intergrax has to offer!\"\n",
      "  [3] role='user', created_at=2025-12-26T10:15:49.188643+00:00\n",
      "      content='This is my second message in the same session.'\n",
      "  [4] role='assistant', created_at=2025-12-26T10:15:50.700592+00:00\n",
      "      content=\"You've sent a follow-up message. How are things progressing on the Intergrax runtime so far? Have you encountered any interesting developments or would you like some guidance on how to proceed? I'm here to help and provide support as needed. What's on your mind?\"\n"
     ]
    }
   ],
   "source": [
    "request_2 = RuntimeRequest(\n",
    "    user_id=user_id,\n",
    "    session_id=session_id,\n",
    "    message=\"This is my second message in the same session.\",\n",
    "    metadata={\"source\": \"notebook-demo\"},\n",
    ")\n",
    "\n",
    "answer_2 = await runtime.run(request_2)\n",
    "\n",
    "print(\"=== ANSWER 2 ===\")\n",
    "print(answer_2.answer)\n",
    "print(\"\\nRoute info:\")\n",
    "print(answer_2.route)\n",
    "print(\"\\nDebug trace:\")\n",
    "print(answer_2.debug_trace)\n",
    "\n",
    "session_2 = await session_manager.get_session(answer_2.debug_trace[\"session_id\"])\n",
    "messages = await session_manager.get_history(session_id=answer_1.debug_trace[\"session_id\"])\n",
    "\n",
    "print(\"\\n=== SESSION AFTER SECOND REQUEST ===\")\n",
    "print(\"Session ID:\", session_2.id)\n",
    "print(\"Messages count:\", len(messages))\n",
    "for idx, msg in enumerate(messages, start=1):\n",
    "    print(f\"  [{idx}] role={msg.role!r}, created_at={msg.created_at}\")\n",
    "    print(f\"      content={msg.content!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b1f22",
   "metadata": {},
   "source": [
    "Verify that history length is limited by config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac31515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILT CHAT HISTORY ===\n",
      "History length: 4\n",
      "  [1] role='user', created_at=2025-12-26T10:15:46.899639+00:00\n",
      "      content='Hello Intergrax runtime, this is my first message.'\n",
      "  [2] role='assistant', created_at=2025-12-26T10:15:49.172904+00:00\n",
      "      content=\"Welcome to the Intergrax runtime! This is an exciting new platform for experimentation and innovation.\\n\\nI'm here to help you explore the capabilities of the Intergrax runtime and support you in achieving your goals. What would you like to do or discuss? You can ask me any questions, share ideas, or let's just chat about what Intergrax has to offer!\"\n",
      "  [3] role='user', created_at=2025-12-26T10:15:49.188643+00:00\n",
      "      content='This is my second message in the same session.'\n",
      "  [4] role='assistant', created_at=2025-12-26T10:15:50.700592+00:00\n",
      "      content=\"You've sent a follow-up message. How are things progressing on the Intergrax runtime so far? Have you encountered any interesting developments or would you like some guidance on how to proceed? I'm here to help and provide support as needed. What's on your mind?\"\n"
     ]
    }
   ],
   "source": [
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "# For debugging: call the history builder directly\n",
    "session_for_history = await session_manager.get_session(answer_2.debug_trace[\"session_id\"])\n",
    "history = await runtime._history_layer._build_chat_history(session_for_history)\n",
    "\n",
    "print(\"=== BUILT CHAT HISTORY ===\")\n",
    "print(\"History length:\", len(history))\n",
    "for idx, msg in enumerate(history, start=1):\n",
    "    assert isinstance(msg, ChatMessage)\n",
    "    print(f\"  [{idx}] role={msg.role!r}, created_at={msg.created_at}\")\n",
    "    print(f\"      content={msg.content!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs: 2\n",
      "[1] 2025-12-26T10:15:49.172904+00:00 run_id=run_73aaf58a15d948868cf7a430a68450dc calls=1 total_tokens=87\n",
      "[2] 2025-12-26T10:15:50.700592+00:00 run_id=run_a4904699d0104c80a30a869f5a833664 calls=1 total_tokens=152\n"
     ]
    }
   ],
   "source": [
    "runs = await runtime.get_llm_usage_runs()\n",
    "print(\"runs:\", len(runs))\n",
    "for r in runs[-5:]:\n",
    "    total = r.report.total    \n",
    "    print(f\"[{r.seq}] {r.ts_utc.isoformat()} run_id={r.run_id} calls={total.calls} total_tokens={total.total_tokens}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
