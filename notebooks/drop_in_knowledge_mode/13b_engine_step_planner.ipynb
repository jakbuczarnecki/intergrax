{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8cf755",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n",
    "\n",
    "# StepPlanner – Dry-Run Test Notebook (STATIC vs DYNAMIC)\n",
    "\n",
    "## Session Goal\n",
    "Validate StepPlanner behavior without executing any tools:\n",
    "- model validations\n",
    "- correctness of built plans (order, depends_on, mode)\n",
    "- consistency between STATIC and DYNAMIC planning for the same step types\n",
    "\n",
    "## Testing Strategy\n",
    "We will add notebook cells incrementally (one cell per message).\n",
    "Configuration (imports, sys.path, planner instances, shared constants) will be collected into a single dedicated configuration cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023ddf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a30f84",
   "metadata": {},
   "source": [
    "## LLM-driven StepPlanner tests (EnginePlan → ExecutionPlan)\n",
    "\n",
    "We will test **real plans generated by the LLM** via `EnginePlanner.plan()`.\n",
    "\n",
    "Pipeline:\n",
    "1) Build `EnginePlan` using `build_plan(...)` (LLM-based)\n",
    "2) Convert to `ExecutionPlan` using `StepPlanner.build_from_engine_plan(...)`\n",
    "\n",
    "We will test both:\n",
    "- **STATIC** (full plan built upfront)\n",
    "- **DYNAMIC** (one-step plan based on `engine_plan.next_step`)\n",
    "\n",
    "Key assertions:\n",
    "- mode correctness (EXECUTE vs ITERATE)\n",
    "- step order and `depends_on` correctness (for STATIC)\n",
    "- single-step plans (for DYNAMIC)\n",
    "- plan contains required capabilities inferred from LLM decision (websearch/ltm/rag/tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b6baa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "from intergrax.llm.messages import ChatMessage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import EnginePlan\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import (\n",
    "    ExecutionPlan,\n",
    "    PlanBuildMode,\n",
    ")\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.plan_builder_helper import build_plan\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.step_planner import StepPlanner, StepPlannerConfig\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PlannerContext:\n",
    "    \"\"\"\n",
    "    All inputs required to run LLM-based planning and then build an ExecutionPlan.\n",
    "    Keep this explicit and strongly typed.\n",
    "    \"\"\"\n",
    "    config: RuntimeConfig\n",
    "    step_planner: StepPlanner\n",
    "\n",
    "\n",
    "def make_planner_context(*, config: RuntimeConfig, step_cfg: Optional[StepPlannerConfig] = None) -> PlannerContext:\n",
    "    \"\"\"\n",
    "    Factory for a deterministic StepPlanner instance + runtime config.\n",
    "    \"\"\"\n",
    "    return PlannerContext(\n",
    "        config=config,\n",
    "        step_planner=StepPlanner(cfg=step_cfg),\n",
    "    )\n",
    "\n",
    "\n",
    "async def llm_engine_plan(\n",
    "    *,\n",
    "    ctx: PlannerContext,\n",
    "    message: str,\n",
    "    user_id: str,\n",
    "    session_id: Optional[str] = None,\n",
    "    run_id: Optional[str] = None,\n",
    "    base_history: Optional[List[ChatMessage]] = None,\n",
    ") -> EnginePlan:\n",
    "    \"\"\"\n",
    "    LLM-based: EnginePlanner.plan() -> EnginePlan\n",
    "    Uses existing helper `build_plan(...)` which creates RuntimeRequest/RuntimeState internally.\n",
    "    \"\"\"\n",
    "    return await build_plan(\n",
    "        config=ctx.config,\n",
    "        message=message,\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        run_id=run_id,\n",
    "        base_history=base_history,\n",
    "    )\n",
    "\n",
    "\n",
    "def execution_plan_from_engine_plan(\n",
    "    *,\n",
    "    ctx: PlannerContext,\n",
    "    user_message: str,\n",
    "    engine_plan: EnginePlan,\n",
    "    build_mode: PlanBuildMode,\n",
    "    plan_id: Optional[str] = None,\n",
    ") -> ExecutionPlan:\n",
    "    \"\"\"\n",
    "    Deterministic: StepPlanner converts EnginePlan -> ExecutionPlan.\n",
    "    \"\"\"\n",
    "    return ctx.step_planner.build_from_engine_plan(\n",
    "        user_message=user_message,\n",
    "        engine_plan=engine_plan,\n",
    "        plan_id=plan_id,\n",
    "        build_mode=build_mode,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b583a4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This cell builds:\n",
    "- the LLM adapter\n",
    "- `RuntimeConfig` used by the LLM-driven EnginePlanner helper (`build_plan`)\n",
    "- `StepPlannerConfig` (deterministic, no LLM inside)\n",
    "- `PlannerContext` (typed bundle passed into helper functions)\n",
    "\n",
    "All configuration is explicit and centralized in this single cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6503900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM adapter (same pattern as engine planner notebook) ---\n",
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import OutputFormat, WebSearchStrategy\n",
    "\n",
    "\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "# --- Runtime config used by build_plan(...) / EnginePlanner planning ---\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    enable_rag=True,\n",
    "    enable_websearch=True,\n",
    "    tools_mode=\"auto\",\n",
    "    enable_user_longterm_memory=True,\n",
    ")\n",
    "\n",
    "# --- StepPlanner deterministic config ---\n",
    "step_cfg = StepPlannerConfig(\n",
    "    final_answer_style=\"concise_technical\",\n",
    "    final_format=OutputFormat.MARKDOWN,\n",
    "    step_max_chars=2000,\n",
    "    web_top_k=5,\n",
    "    web_max_results=5,\n",
    "    web_recency_days=30,\n",
    "    web_strategy=WebSearchStrategy.HYBRID,\n",
    "    max_total_steps=6,\n",
    "    max_total_tool_calls=3,\n",
    "    max_total_web_queries=5,\n",
    "    max_total_chars_context=12000,\n",
    "    max_total_tokens_output=None,\n",
    ")\n",
    "\n",
    "# --- Typed planner context ---\n",
    "ctx = make_planner_context(config=config, step_cfg=step_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f4657",
   "metadata": {},
   "source": [
    "## E2E STATIC test 1: Freshness query\n",
    "\n",
    "We test the full planning pipeline:\n",
    "\n",
    "LLM (EnginePlanner) → EnginePlan → StepPlanner → ExecutionPlan (STATIC)\n",
    "\n",
    "Expectations:\n",
    "- LLM sets `use_websearch=True` and `intent=FRESHNESS`\n",
    "- StepPlanner builds a full EXECUTE plan\n",
    "- Plan contains WEBSEARCH as a pre-step\n",
    "- Plan ends with FINALIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018d2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGINE PLAN:\n",
      "  intent    : PlanIntent.FRESHNESS\n",
      "  next_step : EngineNextStep.WEBSEARCH\n",
      "  use_web   : True\n",
      "  use_ltm   : False\n",
      "  use_rag   : False\n",
      "  use_tools : False\n",
      "\n",
      "EXECUTION PLAN:\n",
      "  mode  : PlanMode.EXECUTE\n",
      "  intent: PlanIntent.FRESHNESS\n",
      "  steps : [<StepId.WEBSEARCH: 'websearch'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "OK: E2E STATIC Freshness test passed.\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import (\n",
    "    PlanBuildMode,\n",
    "    PlanMode,\n",
    "    StepId,\n",
    ")\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import PlanIntent\n",
    "\n",
    "TEST_MSG = \"What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\"\n",
    "\n",
    "engine_plan = await llm_engine_plan(\n",
    "    ctx=ctx,\n",
    "    message=TEST_MSG,\n",
    "    user_id=\"e2e-user\",\n",
    "    session_id=\"e2e-session-static-freshness\",\n",
    "    run_id=\"e2e-static-freshness-001\",\n",
    ")\n",
    "\n",
    "print(\"ENGINE PLAN:\")\n",
    "print(\"  intent    :\", engine_plan.intent)\n",
    "print(\"  next_step :\", engine_plan.next_step)\n",
    "print(\"  use_web   :\", engine_plan.use_websearch)\n",
    "print(\"  use_ltm   :\", engine_plan.use_user_longterm_memory)\n",
    "print(\"  use_rag   :\", engine_plan.use_rag)\n",
    "print(\"  use_tools :\", engine_plan.use_tools)\n",
    "\n",
    "plan = execution_plan_from_engine_plan(\n",
    "    ctx=ctx,\n",
    "    user_message=TEST_MSG,\n",
    "    engine_plan=engine_plan,\n",
    "    build_mode=PlanBuildMode.STATIC,\n",
    "    plan_id=\"e2e-static-freshness-001\",\n",
    ")\n",
    "\n",
    "print(\"\\nEXECUTION PLAN:\")\n",
    "print(\"  mode  :\", plan.mode)\n",
    "print(\"  intent:\", plan.intent)\n",
    "print(\"  steps :\", [s.step_id for s in plan.steps])\n",
    "\n",
    "# --- Assertions ---\n",
    "\n",
    "assert plan.mode == PlanMode.EXECUTE, f\"Expected EXECUTE plan, got {plan.mode}\"\n",
    "assert plan.steps[-1].step_id == StepId.FINAL, \"STATIC plan must end with FINAL\"\n",
    "\n",
    "step_ids = [s.step_id for s in plan.steps]\n",
    "\n",
    "# Capability alignment with LLM decision\n",
    "if engine_plan.use_websearch:\n",
    "    assert StepId.WEBSEARCH in step_ids, \"LLM requested websearch but plan has no WEBSEARCH step\"\n",
    "if engine_plan.use_rag:\n",
    "    assert StepId.RAG in step_ids, \"LLM requested rag but plan has no RAG step\"\n",
    "if engine_plan.use_tools:\n",
    "    assert StepId.TOOLS in step_ids, \"LLM requested tools but plan has no TOOLS step\"\n",
    "if engine_plan.use_user_longterm_memory:\n",
    "    assert StepId.LTM_SEARCH in step_ids, \"LLM requested LTM but plan has no LTM_SEARCH step\"\n",
    "\n",
    "print(\"OK: E2E STATIC Freshness test passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53f25c",
   "metadata": {},
   "source": [
    "## E2E STATIC test 2: Project architecture question (LTM expected)\n",
    "\n",
    "Goal: verify the planner can distinguish project-internal questions from freshness questions.\n",
    "\n",
    "Pipeline:\n",
    "LLM (EnginePlanner) → EnginePlan → StepPlanner → ExecutionPlan (STATIC)\n",
    "\n",
    "Expectations:\n",
    "- LLM selects `intent=PROJECT_ARCHITECTURE`\n",
    "- LLM sets `use_user_longterm_memory=True` (project context retrieval)\n",
    "- LLM should NOT require websearch for an internal architecture question\n",
    "- StepPlanner builds EXECUTE plan with LTM_SEARCH as a pre-step (if LTM is requested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f903711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGINE PLAN:\n",
      "  intent    : PlanIntent.GENERIC\n",
      "  next_step : EngineNextStep.SYNTHESIZE\n",
      "  use_web   : True\n",
      "  use_ltm   : False\n",
      "  use_rag   : True\n",
      "  use_tools : False\n",
      "\n",
      "EXECUTION PLAN:\n",
      "  mode  : PlanMode.EXECUTE\n",
      "  intent: PlanIntent.GENERIC\n",
      "  steps : [<StepId.RAG: 'rag'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Project-arch usefulness check failed: unexpected_websearch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Hard usefulness expectations\u001b[39;00m\n\u001b[0;32m     68\u001b[0m ok, reason \u001b[38;5;241m=\u001b[39m assert_project_arch_usefulness(engine_plan)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ok, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject-arch usefulness check failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Soft metric: intent label\u001b[39;00m\n\u001b[0;32m     72\u001b[0m intent_ok \u001b[38;5;241m=\u001b[39m (engine_plan\u001b[38;5;241m.\u001b[39mintent \u001b[38;5;241m==\u001b[39m PlanIntent\u001b[38;5;241m.\u001b[39mPROJECT_ARCHITECTURE)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Project-arch usefulness check failed: unexpected_websearch"
     ]
    }
   ],
   "source": [
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "BASE_HISTORY_PROJECT: str = (\n",
    "    \"User: I am Artur. I build Intergrax and Mooff.\\n\"\n",
    "    \"User: We are working on Intergrax Drop-In Knowledge Runtime.\\n\"\n",
    "    \"User: We are implementing a Step Planner with modes: none/static/dynamic.\\n\"\n",
    "    \"User: Observed issues: LTM always-on in generic; websearch vs tools conflation.\\n\"\n",
    "    \"User: I prefer concise, technical answers. Never use emojis in code/docs.\\n\"\n",
    ")\n",
    "\n",
    "TEST_MSG = \"In Intergrax Drop-In Knowledge Runtime, explain how StepPlanner integrates with EnginePlanner in DYNAMIC mode.\"\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def assert_project_arch_usefulness(engine_plan: EnginePlan) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Returns (ok, reason). Hard assertions are applied in the caller.\n",
    "    \"\"\"\n",
    "    # Must NOT use websearch or tools for internal architecture questions\n",
    "    if engine_plan.use_websearch:\n",
    "        return False, \"unexpected_websearch\"\n",
    "    if engine_plan.use_tools:\n",
    "        return False, \"unexpected_tools\"\n",
    "\n",
    "    # Must use some internal knowledge path: LTM or RAG\n",
    "    if not (engine_plan.use_user_longterm_memory or engine_plan.use_rag):\n",
    "        return False, \"no_internal_retrieval_selected\"\n",
    "\n",
    "    return True, \"ok\"\n",
    "\n",
    "TEST_MSG = \"In Intergrax Drop-In Knowledge Runtime, explain how StepPlanner integrates with EnginePlanner in DYNAMIC mode.\"\n",
    "\n",
    "engine_plan = await llm_engine_plan(\n",
    "    ctx=ctx,\n",
    "    message=TEST_MSG,\n",
    "    user_id=\"e2e-user\",\n",
    "    session_id=\"e2e-session-static-project-arch\",\n",
    "    run_id=\"e2e-static-project-arch-003\",\n",
    "    base_history=[ChatMessage(role=\"user\", content=BASE_HISTORY_PROJECT)],\n",
    ")\n",
    "\n",
    "print(\"ENGINE PLAN:\")\n",
    "print(\"  intent    :\", engine_plan.intent)\n",
    "print(\"  next_step :\", engine_plan.next_step)\n",
    "print(\"  use_web   :\", engine_plan.use_websearch)\n",
    "print(\"  use_ltm   :\", engine_plan.use_user_longterm_memory)\n",
    "print(\"  use_rag   :\", engine_plan.use_rag)\n",
    "print(\"  use_tools :\", engine_plan.use_tools)\n",
    "\n",
    "plan = execution_plan_from_engine_plan(\n",
    "    ctx=ctx,\n",
    "    user_message=TEST_MSG,\n",
    "    engine_plan=engine_plan,\n",
    "    build_mode=PlanBuildMode.STATIC,\n",
    "    plan_id=\"e2e-static-project-arch-003\",\n",
    ")\n",
    "\n",
    "print(\"\\nEXECUTION PLAN:\")\n",
    "print(\"  mode  :\", plan.mode)\n",
    "print(\"  intent:\", plan.intent)\n",
    "print(\"  steps :\", [s.step_id for s in plan.steps])\n",
    "\n",
    "# Hard invariants\n",
    "assert plan.mode == PlanMode.EXECUTE\n",
    "assert plan.steps[-1].step_id == StepId.FINAL\n",
    "\n",
    "# Hard usefulness expectations\n",
    "ok, reason = assert_project_arch_usefulness(engine_plan)\n",
    "assert ok, f\"Project-arch usefulness check failed: {reason}\"\n",
    "\n",
    "# Soft metric: intent label\n",
    "intent_ok = (engine_plan.intent == PlanIntent.PROJECT_ARCHITECTURE)\n",
    "print(\"\\nMETRIC intent_ok =\", intent_ok, \"| expected:\", PlanIntent.PROJECT_ARCHITECTURE, \"| got:\", engine_plan.intent)\n",
    "\n",
    "print(\"OK: E2E STATIC Project architecture usefulness passed (intent label tracked separately).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59449201",
   "metadata": {},
   "source": [
    "## E2E DYNAMIC test 1: Freshness query (one-step ITERATE plan)\n",
    "\n",
    "Pipeline:\n",
    "LLM (EnginePlanner) → EnginePlan(next_step=WEBSEARCH) → StepPlanner → ExecutionPlan (DYNAMIC)\n",
    "\n",
    "Expectations:\n",
    "- `ExecutionPlan.mode == ITERATE`\n",
    "- `len(steps) == 1`\n",
    "- the single step matches `engine_plan.next_step`\n",
    "  - for WEBSEARCH: step_id == WEBSEARCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53499a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGINE PLAN:\n",
      "  message   : What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\n",
      "  intent    : PlanIntent.FRESHNESS\n",
      "  next_step : EngineNextStep.WEBSEARCH\n",
      "  use_web   : True\n",
      "  use_ltm   : False\n",
      "  use_rag   : False\n",
      "  use_tools : False\n",
      "\n",
      "EXECUTION PLAN:\n",
      "  mode  : PlanMode.ITERATE\n",
      "  intent: PlanIntent.FRESHNESS\n",
      "  steps : [<StepId.WEBSEARCH: 'websearch'>]\n",
      "OK: E2E DYNAMIC Freshness one-step plan passed.\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import (\n",
    "    PlanBuildMode,\n",
    "    PlanMode,\n",
    "    StepId,\n",
    ")\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import EngineNextStep\n",
    "\n",
    "TEST_MSG = \"What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\"\n",
    "\n",
    "engine_plan = await llm_engine_plan(\n",
    "    ctx=ctx,\n",
    "    message=TEST_MSG,\n",
    "    user_id=\"e2e-user\",\n",
    "    session_id=\"e2e-session-dynamic-freshness\",\n",
    "    run_id=\"e2e-dynamic-freshness-001\",\n",
    ")\n",
    "\n",
    "print(\"ENGINE PLAN:\")\n",
    "print(\"  message   :\", TEST_MSG)\n",
    "print(\"  intent    :\", engine_plan.intent)\n",
    "print(\"  next_step :\", engine_plan.next_step)\n",
    "print(\"  use_web   :\", engine_plan.use_websearch)\n",
    "print(\"  use_ltm   :\", engine_plan.use_user_longterm_memory)\n",
    "print(\"  use_rag   :\", engine_plan.use_rag)\n",
    "print(\"  use_tools :\", engine_plan.use_tools)\n",
    "\n",
    "plan = execution_plan_from_engine_plan(\n",
    "    ctx=ctx,\n",
    "    user_message=TEST_MSG,\n",
    "    engine_plan=engine_plan,\n",
    "    build_mode=PlanBuildMode.DYNAMIC,\n",
    "    plan_id=\"e2e-dynamic-freshness-001\",\n",
    ")\n",
    "\n",
    "print(\"\\nEXECUTION PLAN:\")\n",
    "print(\"  mode  :\", plan.mode)\n",
    "print(\"  intent:\", plan.intent)\n",
    "print(\"  steps :\", [s.step_id for s in plan.steps])\n",
    "\n",
    "# --- Assertions ---\n",
    "assert plan.mode == PlanMode.ITERATE, f\"Expected ITERATE, got {plan.mode}\"\n",
    "assert len(plan.steps) == 1, f\"DYNAMIC plan must have exactly 1 step, got {len(plan.steps)}\"\n",
    "\n",
    "single = plan.steps[0].step_id\n",
    "\n",
    "# Ensure StepPlanner respected the next_step mapping\n",
    "if engine_plan.next_step == EngineNextStep.WEBSEARCH:\n",
    "    assert single == StepId.WEBSEARCH, f\"Expected WEBSEARCH step, got {single}\"\n",
    "elif engine_plan.next_step == EngineNextStep.RAG:\n",
    "    assert single == StepId.RAG, f\"Expected RAG step, got {single}\"\n",
    "elif engine_plan.next_step == EngineNextStep.TOOLS:\n",
    "    assert single == StepId.TOOLS, f\"Expected TOOLS step, got {single}\"\n",
    "elif engine_plan.next_step == EngineNextStep.SYNTHESIZE:\n",
    "    assert single == StepId.DRAFT, f\"Expected DRAFT step, got {single}\"\n",
    "elif engine_plan.next_step == EngineNextStep.VERIFY:\n",
    "    assert single == StepId.VERIFY, f\"Expected VERIFY step, got {single}\"\n",
    "elif engine_plan.next_step == EngineNextStep.FINALIZE:\n",
    "    assert single == StepId.FINAL, f\"Expected FINAL step, got {single}\"\n",
    "elif engine_plan.next_step == EngineNextStep.CLARIFY:\n",
    "    assert single == StepId.CLARIFY, f\"Expected CLARIFY step, got {single}\"\n",
    "else:\n",
    "    raise AssertionError(f\"Unhandled EngineNextStep: {engine_plan.next_step}\")\n",
    "\n",
    "print(\"OK: E2E DYNAMIC Freshness one-step plan passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8a499",
   "metadata": {},
   "source": [
    "## Dataset-driven tests, extended with StepPlanner validation\n",
    "\n",
    "For each test case:\n",
    "1) Run LLM EnginePlanner to produce `EnginePlan`\n",
    "2) Build `ExecutionPlan` via StepPlanner in two modes:\n",
    "   - STATIC  (full plan)\n",
    "   - DYNAMIC (single-step plan)\n",
    "\n",
    "We validate:\n",
    "- EnginePlan: intent / flags / next_step vs expected\n",
    "- ExecutionPlan STATIC: correct mode + required steps + tail\n",
    "- ExecutionPlan DYNAMIC: mode ITERATE + exactly 1 step matching expected next_step mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2700652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tests: 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Set, Dict, Tuple\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import (\n",
    "    PlanIntent,\n",
    "    EngineNextStep,\n",
    ")\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import StepId\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EngineExpect:\n",
    "    intent: Optional[PlanIntent] = None\n",
    "    next_step: Optional[EngineNextStep] = None\n",
    "\n",
    "    # Flags: if None -> do not assert\n",
    "    use_websearch: Optional[bool] = None\n",
    "    use_ltm: Optional[bool] = None\n",
    "    use_rag: Optional[bool] = None\n",
    "    use_tools: Optional[bool] = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StepExpect:\n",
    "    # STATIC expectations (subset-based, not exact chain unless you want)\n",
    "    static_must_include: Set[StepId]\n",
    "    static_forbid: Set[StepId]\n",
    "\n",
    "    # DYNAMIC expectations: single step expected after mapping next_step\n",
    "    dynamic_single_step: StepId\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PlannerTestCase:\n",
    "    id: str                 # e.g. \"F01\", \"P01\" like 12b groups\n",
    "    question: str\n",
    "    use_project_context: bool\n",
    "    engine: EngineExpect\n",
    "    step: StepExpect\n",
    "\n",
    "\n",
    "# Minimal starter set; you can expand to your full suite like 12b\n",
    "TESTS: List[PlannerTestCase] = [\n",
    "    PlannerTestCase(\n",
    "        id=\"F01\",\n",
    "        question=\"What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\",\n",
    "        use_project_context=False,\n",
    "        engine=EngineExpect(\n",
    "            intent=PlanIntent.FRESHNESS,\n",
    "            next_step=EngineNextStep.WEBSEARCH,\n",
    "            use_websearch=True,\n",
    "            use_rag=False,\n",
    "            use_tools=False,\n",
    "            use_ltm=False,\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.WEBSEARCH, StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.LTM_SEARCH, StepId.TOOLS, StepId.RAG},\n",
    "            dynamic_single_step=StepId.WEBSEARCH,\n",
    "        ),\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"G01\",\n",
    "        question=\"Explain the main differences between REST and GraphQL.\",\n",
    "        use_project_context=False,\n",
    "        engine=EngineExpect(\n",
    "            intent=PlanIntent.GENERIC,\n",
    "            use_websearch=False,  # prefer no web for this general question\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.WEBSEARCH},  # allow RAG/tools depending on your policies; keep it light\n",
    "            dynamic_single_step=StepId.DRAFT,  # likely SYNTHESIZE; if engine picks differently, this will flag it\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Loaded tests: {len(TESTS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f441f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import (\n",
    "    PlanBuildMode,\n",
    "    PlanMode,\n",
    "    ExecutionPlan,\n",
    ")\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import EnginePlan\n",
    "\n",
    "\n",
    "def _engine_errors(got: EnginePlan, exp: EngineExpect) -> List[str]:\n",
    "    errs: List[str] = []\n",
    "\n",
    "    if exp.intent is not None and got.intent != exp.intent:\n",
    "        errs.append(f\"intent expected={exp.intent} got={got.intent}\")\n",
    "\n",
    "    if exp.next_step is not None and got.next_step != exp.next_step:\n",
    "        errs.append(f\"next_step expected={exp.next_step} got={got.next_step}\")\n",
    "\n",
    "    def chk_flag(name: str, got_val: bool, exp_val: Optional[bool]) -> None:\n",
    "        if exp_val is None:\n",
    "            return\n",
    "        if got_val != exp_val:\n",
    "            errs.append(f\"{name} expected={exp_val} got={got_val}\")\n",
    "\n",
    "    chk_flag(\"use_websearch\", got.use_websearch, exp.use_websearch)\n",
    "    chk_flag(\"use_ltm\", got.use_user_longterm_memory, exp.use_ltm)\n",
    "    chk_flag(\"use_rag\", got.use_rag, exp.use_rag)\n",
    "    chk_flag(\"use_tools\", got.use_tools, exp.use_tools)\n",
    "\n",
    "    return errs\n",
    "\n",
    "\n",
    "def _step_errors_static(plan: ExecutionPlan, exp: StepExpect) -> List[str]:\n",
    "    errs: List[str] = []\n",
    "    if plan.mode != PlanMode.EXECUTE:\n",
    "        errs.append(f\"static mode expected=EXECUTE got={plan.mode}\")\n",
    "\n",
    "    step_ids = [s.step_id for s in plan.steps]\n",
    "    missing = [s for s in exp.static_must_include if s not in step_ids]\n",
    "    if missing:\n",
    "        errs.append(f\"static missing steps: {missing}\")\n",
    "\n",
    "    forbidden = [s for s in exp.static_forbid if s in step_ids]\n",
    "    if forbidden:\n",
    "        errs.append(f\"static forbidden steps present: {forbidden}\")\n",
    "\n",
    "    if not plan.steps or plan.steps[-1].step_id != StepId.FINAL:\n",
    "        errs.append(\"static plan must end with FINAL\")\n",
    "\n",
    "    # forward dependency check (crucial correctness property)\n",
    "    seen = set()\n",
    "    for st in plan.steps:\n",
    "        for dep in (st.depends_on or []):\n",
    "            if dep not in seen:\n",
    "                errs.append(f\"forward dependency: {st.step_id} depends_on {dep} (not seen yet)\")\n",
    "        seen.add(st.step_id)\n",
    "\n",
    "    return errs\n",
    "\n",
    "\n",
    "def _map_next_step_to_step_id(next_step: EngineNextStep) -> StepId:\n",
    "    if next_step == EngineNextStep.WEBSEARCH:\n",
    "        return StepId.WEBSEARCH\n",
    "    if next_step == EngineNextStep.RAG:\n",
    "        return StepId.RAG\n",
    "    if next_step == EngineNextStep.TOOLS:\n",
    "        return StepId.TOOLS\n",
    "    if next_step == EngineNextStep.SYNTHESIZE:\n",
    "        return StepId.DRAFT\n",
    "    if next_step == EngineNextStep.FINALIZE:\n",
    "        return StepId.FINAL\n",
    "    if next_step == EngineNextStep.CLARIFY:\n",
    "        return StepId.CLARIFY\n",
    "    raise ValueError(f\"Unhandled EngineNextStep: {next_step}\")\n",
    "\n",
    "\n",
    "def _step_errors_dynamic(plan: ExecutionPlan, exp: StepExpect, engine_plan: EnginePlan) -> List[str]:\n",
    "    errs: List[str] = []\n",
    "    if plan.mode != PlanMode.ITERATE:\n",
    "        errs.append(f\"dynamic mode expected=ITERATE got={plan.mode}\")\n",
    "\n",
    "    if len(plan.steps) != 1:\n",
    "        errs.append(f\"dynamic len(steps) expected=1 got={len(plan.steps)}\")\n",
    "        return errs  # can't validate further reliably\n",
    "\n",
    "    expected_single = exp.dynamic_single_step\n",
    "    got_single = plan.steps[0].step_id\n",
    "    if got_single != expected_single:\n",
    "        errs.append(f\"dynamic single step expected={expected_single} got={got_single}\")\n",
    "\n",
    "    # Also ensure it matches engine_plan.next_step mapping (independent safety net)\n",
    "    mapped = _map_next_step_to_step_id(engine_plan.next_step)\n",
    "    if got_single != mapped:\n",
    "        errs.append(f\"dynamic mismatch vs engine next_step mapping: mapped={mapped} got={got_single}\")\n",
    "\n",
    "    return errs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659f66f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded3190a7cab408698d92f5cd85309e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Engine+Step planner tests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "TEST F01 PASS\n",
      "STATIC : [<StepId.WEBSEARCH: 'websearch'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.WEBSEARCH: 'websearch'>]\n",
      "\n",
      "====================\n",
      "TEST G01 PASS\n",
      "STATIC : [<StepId.RAG: 'rag'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.DRAFT: 'draft'>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'totals': {'total': 2, 'pass': 2},\n",
       " 'accuracy': 100.0,\n",
       " 'per_group': {'F': {'total': 1, 'pass': 1, 'accuracy': 100.0},\n",
       "  'G': {'total': 1, 'pass': 1, 'accuracy': 100.0}},\n",
       " 'fails': [],\n",
       " 'per_test': [{'id': 'F01',\n",
       "   'pass': True,\n",
       "   'engine_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.FRESHNESS: 'freshness'>, reasoning_summary='User wants recent changes to OpenAI Responses API and tool calling; use websearch for freshness.', ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.WEBSEARCH: 'websearch'>, use_websearch=True, use_user_longterm_memory=False, use_rag=False, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'freshness', 'next_step': 'websearch', 'reasoning_summary': 'User wants recent changes to OpenAI Responses API and tool calling; use websearch for freshness.', 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_json_len': 369, 'next_step_raw': 'websearch', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_raw_len': 369, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"freshness\",\\n  \"next_step\": \"websearch\",\\n  \"reasoning_summary\": \"User wants recent changes to OpenAI Responses API and tool calling; use websearch for freshness.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}', 'planner_raw_tail_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"freshness\",\\n  \"next_step\": \"websearch\",\\n  \"reasoning_summary\": \"User wants recent changes to OpenAI Responses API and tool calling; use websearch for freshness.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.WEBSEARCH: 'websearch'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.WEBSEARCH: 'websearch'>]},\n",
       "  {'id': 'G01',\n",
       "   'pass': True,\n",
       "   'engine_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.GENERIC: 'generic'>, reasoning_summary='User query is clear and specific, can answer with a reasonable technical response without asking follow-ups.', ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.SYNTHESIZE: 'synthesize'>, use_websearch=False, use_user_longterm_memory=False, use_rag=True, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'generic', 'next_step': 'synthesize', 'reasoning_summary': 'User query is clear and specific, can answer with a reasonable technical response without asking follow-ups.', 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': False, 'use_user_longterm_memory': False, 'use_rag': True, 'use_tools': False}, 'planner_json_len': 380, 'next_step_raw': 'synthesize', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': False, 'use_user_longterm_memory': False, 'use_rag': True, 'use_tools': False}, 'planner_raw_len': 380, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"generic\",\\n  \"next_step\": \"synthesize\",\\n  \"reasoning_summary\": \"User query is clear and specific, can answer with a reasonable technical response without asking follow-ups.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": true,\\n  \"use_tools\": false\\n}', 'planner_raw_tail_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"generic\",\\n  \"next_step\": \"synthesize\",\\n  \"reasoning_summary\": \"User query is clear and specific, can answer with a reasonable technical response without asking follow-ups.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": true,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.RAG: 'rag'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.DRAFT: 'draft'>]}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "\n",
    "BASE_HISTORY_EMPTY: str = \"\"\n",
    "\n",
    "BASE_HISTORY_PROJECT: str = (\n",
    "    \"User: I am Artur. I build Intergrax and Mooff.\\n\"\n",
    "    \"User: We are working on Intergrax Drop-In Knowledge Runtime.\\n\"\n",
    "    \"User: We are implementing a Step Planner with modes: none/static/dynamic.\\n\"\n",
    "    \"User: Observed issues: LTM always-on in generic; websearch vs tools conflation.\\n\"\n",
    "    \"User: I prefer concise, technical answers. Never use emojis in code/docs.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "async def run_case(tc: PlannerTestCase) -> Dict[str, Any]:\n",
    "    base_history_text = BASE_HISTORY_PROJECT if tc.use_project_context else BASE_HISTORY_EMPTY\n",
    "    base_history = [ChatMessage(role=\"user\", content=base_history_text)] if base_history_text else []\n",
    "\n",
    "    engine_plan = await llm_engine_plan(\n",
    "        ctx=ctx,\n",
    "        message=tc.question,\n",
    "        user_id=\"suite-user\",\n",
    "        session_id=f\"suite-session-{tc.id}\",\n",
    "        run_id=f\"suite-{tc.id}\",\n",
    "        base_history=base_history,\n",
    "    )\n",
    "\n",
    "    # Build both plans\n",
    "    plan_static = execution_plan_from_engine_plan(\n",
    "        ctx=ctx,\n",
    "        user_message=tc.question,\n",
    "        engine_plan=engine_plan,\n",
    "        build_mode=PlanBuildMode.STATIC,\n",
    "        plan_id=f\"suite-static-{tc.id}\",\n",
    "    )\n",
    "\n",
    "    plan_dynamic = execution_plan_from_engine_plan(\n",
    "        ctx=ctx,\n",
    "        user_message=tc.question,\n",
    "        engine_plan=engine_plan,\n",
    "        build_mode=PlanBuildMode.DYNAMIC,\n",
    "        plan_id=f\"suite-dynamic-{tc.id}\",\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    eng_errs = _engine_errors(engine_plan, tc.engine)\n",
    "    static_errs = _step_errors_static(plan_static, tc.step)\n",
    "    dynamic_errs = _step_errors_dynamic(plan_dynamic, tc.step, engine_plan)\n",
    "\n",
    "    # Pass criteria: all must be empty\n",
    "    passed = (not eng_errs) and (not static_errs) and (not dynamic_errs)\n",
    "\n",
    "    return {\n",
    "        \"id\": tc.id,\n",
    "        \"pass\": passed,\n",
    "        \"engine_errors\": eng_errs,\n",
    "        \"static_errors\": static_errs,\n",
    "        \"dynamic_errors\": dynamic_errs,\n",
    "        \"engine_plan\": engine_plan,\n",
    "        \"static_steps\": [s.step_id for s in plan_static.steps],\n",
    "        \"dynamic_steps\": [s.step_id for s in plan_dynamic.steps],\n",
    "    }\n",
    "\n",
    "\n",
    "async def run_all(verbose: bool = False) -> Dict[str, Any]:\n",
    "    totals = {\"total\": 0, \"pass\": 0}\n",
    "    per_group: Dict[str, Dict[str, int]] = {}\n",
    "    fails: List[Dict[str, Any]] = []\n",
    "    per_test: List[Dict[str, Any]] = []\n",
    "\n",
    "    for tc in tqdm(TESTS, desc=\"Engine+Step planner tests\"):\n",
    "        res = await run_case(tc)\n",
    "        per_test.append(res)\n",
    "\n",
    "        totals[\"total\"] += 1\n",
    "        if res[\"pass\"]:\n",
    "            totals[\"pass\"] += 1\n",
    "        else:\n",
    "            fails.append(\n",
    "                {\n",
    "                    \"id\": res[\"id\"],\n",
    "                    \"group\": res[\"id\"][0],\n",
    "                    \"engine_errors\": res[\"engine_errors\"],\n",
    "                    \"static_errors\": res[\"static_errors\"],\n",
    "                    \"dynamic_errors\": res[\"dynamic_errors\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        g = tc.id[0]\n",
    "        if g not in per_group:\n",
    "            per_group[g] = {\"total\": 0, \"pass\": 0}\n",
    "        per_group[g][\"total\"] += 1\n",
    "        if res[\"pass\"]:\n",
    "            per_group[g][\"pass\"] += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n====================\")\n",
    "            print(\"TEST\", res[\"id\"], \"PASS\" if res[\"pass\"] else \"FAIL\")\n",
    "            print(\"STATIC :\", res[\"static_steps\"])\n",
    "            print(\"DYNAMIC:\", res[\"dynamic_steps\"])\n",
    "            if res[\"engine_errors\"]:\n",
    "                print(\"ENGINE ERR:\", res[\"engine_errors\"])\n",
    "            if res[\"static_errors\"]:\n",
    "                print(\"STATIC ERR:\", res[\"static_errors\"])\n",
    "            if res[\"dynamic_errors\"]:\n",
    "                print(\"DYNAMIC ERR:\", res[\"dynamic_errors\"])\n",
    "\n",
    "    def pct(x: int, denom: int) -> float:\n",
    "        return 0.0 if denom <= 0 else (100.0 * float(x)) / float(denom)\n",
    "\n",
    "    summary = {\n",
    "        \"totals\": totals,\n",
    "        \"accuracy\": pct(totals[\"pass\"], totals[\"total\"]),\n",
    "        \"per_group\": {g: {**st, \"accuracy\": pct(st[\"pass\"], st[\"total\"])} for g, st in per_group.items()},\n",
    "        \"fails\": fails,\n",
    "        \"per_test\": per_test,\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "await run_all(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6847fd",
   "metadata": {},
   "source": [
    "## Metrics summary (12b-style) + StepPlanner breakdown\n",
    "\n",
    "We compute:\n",
    "- `intent_ok`  (EnginePlan.intent vs expected when specified)\n",
    "- `flags_ok`   (EnginePlan flags vs expected when specified)\n",
    "- `static_ok`  (STATIC ExecutionPlan validation)\n",
    "- `dynamic_ok` (DYNAMIC ExecutionPlan validation)\n",
    "- `pass`       (all of the above true)\n",
    "\n",
    "We also produce:\n",
    "- per-group breakdown (F/G/P/C)\n",
    "- compact failure list with the failing component(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abbea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea28a2a0b552477faec496c2060bef4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Engine+Step planner tests (metrics):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "TEST F01 PASS\n",
      "ENGINE intent: PlanIntent.FRESHNESS next_step: EngineNextStep.WEBSEARCH\n",
      "ENGINE flags : web True ltm False rag False tools False\n",
      "STATIC : [<StepId.WEBSEARCH: 'websearch'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.WEBSEARCH: 'websearch'>]\n",
      "\n",
      "====================\n",
      "TEST G01 FAIL\n",
      "ENGINE intent: PlanIntent.GENERIC next_step: EngineNextStep.SYNTHESIZE\n",
      "ENGINE flags : web True ltm False rag False tools False\n",
      "STATIC : [<StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.DRAFT: 'draft'>]\n",
      "FLAGS ERR : ['use_websearch expected=False got=True']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'totals': {'total': 2,\n",
       "  'pass': 1,\n",
       "  'intent_ok': 2,\n",
       "  'flags_ok': 1,\n",
       "  'next_step_ok': 2,\n",
       "  'static_ok': 2,\n",
       "  'dynamic_ok': 2},\n",
       " 'accuracy': 50.0,\n",
       " 'intent_accuracy': 100.0,\n",
       " 'flags_accuracy': 50.0,\n",
       " 'next_step_accuracy': 100.0,\n",
       " 'static_accuracy': 100.0,\n",
       " 'dynamic_accuracy': 100.0,\n",
       " 'per_group': {'F': {'total': 1,\n",
       "   'pass': 1,\n",
       "   'intent_ok': 1,\n",
       "   'flags_ok': 1,\n",
       "   'next_step_ok': 1,\n",
       "   'static_ok': 1,\n",
       "   'dynamic_ok': 1,\n",
       "   'accuracy': 100.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 100.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 100.0},\n",
       "  'G': {'total': 1,\n",
       "   'pass': 0,\n",
       "   'intent_ok': 1,\n",
       "   'flags_ok': 0,\n",
       "   'next_step_ok': 1,\n",
       "   'static_ok': 1,\n",
       "   'dynamic_ok': 1,\n",
       "   'accuracy': 0.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 0.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 100.0}},\n",
       " 'fails': [{'test_id': 'G01',\n",
       "   'group': 'G',\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': False,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': ['use_websearch expected=False got=True'],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': []}],\n",
       " 'per_test': [{'id': 'F01',\n",
       "   'pass': True,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.FRESHNESS: 'freshness'>, reasoning_summary=\"The intent is 'freshness' because the user asks for recent changes to the OpenAI Responses API, which requires up-to-date external information. The next step is 'websearch' as it's the most suitable initial action.\", ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.WEBSEARCH: 'websearch'>, use_websearch=True, use_user_longterm_memory=False, use_rag=False, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'freshness', 'next_step': 'websearch', 'reasoning_summary': \"The intent is 'freshness' because the user asks for recent changes to the OpenAI Responses API, which requires up-to-date external information. The next step is 'websearch' as it's the most suitable initial action.\", 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_json_len': 487, 'next_step_raw': 'websearch', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_raw_len': 487, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"freshness\",\\n  \"next_step\": \"websearch\",\\n  \"reasoning_summary\": \"The intent is \\'freshness\\' because the user asks for recent changes to the OpenAI Responses API, which requires up-to-date external information. The next step is \\'websearch\\' as it\\'s the most suitable initial action.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\"', 'planner_raw_tail_preview': '_summary\": \"The intent is \\'freshness\\' because the user asks for recent changes to the OpenAI Responses API, which requires up-to-date external information. The next step is \\'websearch\\' as it\\'s the most suitable initial action.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.WEBSEARCH: 'websearch'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.WEBSEARCH: 'websearch'>]},\n",
       "  {'id': 'G01',\n",
       "   'pass': False,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': False,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': ['use_websearch expected=False got=True'],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.GENERIC: 'generic'>, reasoning_summary='The user query is a general question about the differences between REST and GraphQL, which can be answered with existing knowledge.', ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.SYNTHESIZE: 'synthesize'>, use_websearch=True, use_user_longterm_memory=False, use_rag=False, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'generic', 'next_step': 'synthesize', 'reasoning_summary': 'The user query is a general question about the differences between REST and GraphQL, which can be answered with existing knowledge.', 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_json_len': 403, 'next_step_raw': 'synthesize', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_raw_len': 403, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"generic\",\\n  \"next_step\": \"synthesize\",\\n  \"reasoning_summary\": \"The user query is a general question about the differences between REST and GraphQL, which can be answered with existing knowledge.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": fals', 'planner_raw_tail_preview': ' \"version\": \"1.0\",\\n  \"intent\": \"generic\",\\n  \"next_step\": \"synthesize\",\\n  \"reasoning_summary\": \"The user query is a general question about the differences between REST and GraphQL, which can be answered with existing knowledge.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.DRAFT: 'draft'>]}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import EnginePlan\n",
    "\n",
    "\n",
    "def _intent_ok(got: EnginePlan, exp: EngineExpect) -> Tuple[bool, List[str]]:\n",
    "    errs: List[str] = []\n",
    "    if exp.intent is not None and got.intent != exp.intent:\n",
    "        errs.append(f\"intent expected={exp.intent} got={got.intent}\")\n",
    "    return (len(errs) == 0, errs)\n",
    "\n",
    "\n",
    "def _flags_ok(got: EnginePlan, exp: EngineExpect) -> Tuple[bool, List[str]]:\n",
    "    errs: List[str] = []\n",
    "\n",
    "    def chk(name: str, got_val: bool, exp_val: Optional[bool]) -> None:\n",
    "        if exp_val is None:\n",
    "            return\n",
    "        if got_val != exp_val:\n",
    "            errs.append(f\"{name} expected={exp_val} got={got_val}\")\n",
    "\n",
    "    chk(\"use_websearch\", got.use_websearch, exp.use_websearch)\n",
    "    chk(\"use_ltm\", got.use_user_longterm_memory, exp.use_ltm)\n",
    "    chk(\"use_rag\", got.use_rag, exp.use_rag)\n",
    "    chk(\"use_tools\", got.use_tools, exp.use_tools)\n",
    "\n",
    "    return (len(errs) == 0, errs)\n",
    "\n",
    "\n",
    "def _next_step_ok(got: EnginePlan, exp: EngineExpect) -> Tuple[bool, List[str]]:\n",
    "    errs: List[str] = []\n",
    "    if exp.next_step is not None and got.next_step != exp.next_step:\n",
    "        errs.append(f\"next_step expected={exp.next_step} got={got.next_step}\")\n",
    "    return (len(errs) == 0, errs)\n",
    "\n",
    "\n",
    "async def run_case_metrics(tc: PlannerTestCase) -> Dict[str, Any]:\n",
    "    base_history_text = BASE_HISTORY_PROJECT if tc.use_project_context else BASE_HISTORY_EMPTY\n",
    "    base_history = [ChatMessage(role=\"user\", content=base_history_text)] if base_history_text else []\n",
    "\n",
    "    engine_plan = await llm_engine_plan(\n",
    "        ctx=ctx,\n",
    "        message=tc.question,\n",
    "        user_id=\"suite-user\",\n",
    "        session_id=f\"suite-session-{tc.id}\",\n",
    "        run_id=f\"suite-{tc.id}\",\n",
    "        base_history=base_history,\n",
    "    )\n",
    "\n",
    "    # Engine metrics\n",
    "    intent_ok, intent_errs = _intent_ok(engine_plan, tc.engine)\n",
    "    flags_ok, flag_errs = _flags_ok(engine_plan, tc.engine)\n",
    "    next_step_ok, next_step_errs = _next_step_ok(engine_plan, tc.engine)\n",
    "\n",
    "    # Build both plans\n",
    "    plan_static = execution_plan_from_engine_plan(\n",
    "        ctx=ctx,\n",
    "        user_message=tc.question,\n",
    "        engine_plan=engine_plan,\n",
    "        build_mode=PlanBuildMode.STATIC,\n",
    "        plan_id=f\"suite-static-{tc.id}\",\n",
    "    )\n",
    "    plan_dynamic = execution_plan_from_engine_plan(\n",
    "        ctx=ctx,\n",
    "        user_message=tc.question,\n",
    "        engine_plan=engine_plan,\n",
    "        build_mode=PlanBuildMode.DYNAMIC,\n",
    "        plan_id=f\"suite-dynamic-{tc.id}\",\n",
    "    )\n",
    "\n",
    "    # StepPlanner validations\n",
    "    static_errs = _step_errors_static(plan_static, tc.step)\n",
    "    dynamic_errs = _step_errors_dynamic(plan_dynamic, tc.step, engine_plan)\n",
    "\n",
    "    static_ok = (len(static_errs) == 0)\n",
    "    dynamic_ok = (len(dynamic_errs) == 0)\n",
    "\n",
    "    passed = intent_ok and flags_ok and next_step_ok and static_ok and dynamic_ok\n",
    "\n",
    "    return {\n",
    "        \"id\": tc.id,\n",
    "        \"pass\": passed,\n",
    "        \"intent_ok\": intent_ok,\n",
    "        \"flags_ok\": flags_ok,\n",
    "        \"next_step_ok\": next_step_ok,\n",
    "        \"static_ok\": static_ok,\n",
    "        \"dynamic_ok\": dynamic_ok,\n",
    "        \"intent_errors\": intent_errs,\n",
    "        \"flag_errors\": flag_errs,\n",
    "        \"next_step_errors\": next_step_errs,\n",
    "        \"static_errors\": static_errs,\n",
    "        \"dynamic_errors\": dynamic_errs,\n",
    "        \"engine_plan\": engine_plan,\n",
    "        \"static_steps\": [s.step_id for s in plan_static.steps],\n",
    "        \"dynamic_steps\": [s.step_id for s in plan_dynamic.steps],\n",
    "    }\n",
    "\n",
    "\n",
    "async def run_all_metrics(tests: List[PlannerTestCase], verbose: bool = False) -> Dict[str, Any]:\n",
    "    totals = {\n",
    "        \"total\": 0,\n",
    "        \"pass\": 0,\n",
    "        \"intent_ok\": 0,\n",
    "        \"flags_ok\": 0,\n",
    "        \"next_step_ok\": 0,\n",
    "        \"static_ok\": 0,\n",
    "        \"dynamic_ok\": 0,\n",
    "    }\n",
    "\n",
    "    per_group: Dict[str, Dict[str, int]] = {}\n",
    "    fails_compact: List[Dict[str, Any]] = []\n",
    "    per_test: List[Dict[str, Any]] = []\n",
    "\n",
    "    for tc in tqdm(tests, desc=\"Engine+Step planner tests (metrics)\"):\n",
    "        res = await run_case_metrics(tc)\n",
    "        per_test.append(res)\n",
    "\n",
    "        totals[\"total\"] += 1\n",
    "        if res[\"pass\"]:\n",
    "            totals[\"pass\"] += 1\n",
    "        if res[\"intent_ok\"]:\n",
    "            totals[\"intent_ok\"] += 1\n",
    "        if res[\"flags_ok\"]:\n",
    "            totals[\"flags_ok\"] += 1\n",
    "        if res[\"next_step_ok\"]:\n",
    "            totals[\"next_step_ok\"] += 1\n",
    "        if res[\"static_ok\"]:\n",
    "            totals[\"static_ok\"] += 1\n",
    "        if res[\"dynamic_ok\"]:\n",
    "            totals[\"dynamic_ok\"] += 1\n",
    "\n",
    "        group = tc.id[0]\n",
    "        if group not in per_group:\n",
    "            per_group[group] = {\n",
    "                \"total\": 0,\n",
    "                \"pass\": 0,\n",
    "                \"intent_ok\": 0,\n",
    "                \"flags_ok\": 0,\n",
    "                \"next_step_ok\": 0,\n",
    "                \"static_ok\": 0,\n",
    "                \"dynamic_ok\": 0,\n",
    "            }\n",
    "\n",
    "        per_group[group][\"total\"] += 1\n",
    "        for k in [\"pass\", \"intent_ok\", \"flags_ok\", \"next_step_ok\", \"static_ok\", \"dynamic_ok\"]:\n",
    "            if res[k]:\n",
    "                per_group[group][k] += 1\n",
    "\n",
    "        if not res[\"pass\"]:\n",
    "            fails_compact.append(\n",
    "                {\n",
    "                    \"test_id\": res[\"id\"],\n",
    "                    \"group\": group,\n",
    "                    \"intent_ok\": res[\"intent_ok\"],\n",
    "                    \"flags_ok\": res[\"flags_ok\"],\n",
    "                    \"next_step_ok\": res[\"next_step_ok\"],\n",
    "                    \"static_ok\": res[\"static_ok\"],\n",
    "                    \"dynamic_ok\": res[\"dynamic_ok\"],\n",
    "                    \"intent_errors\": res[\"intent_errors\"],\n",
    "                    \"flag_errors\": res[\"flag_errors\"],\n",
    "                    \"next_step_errors\": res[\"next_step_errors\"],\n",
    "                    \"static_errors\": res[\"static_errors\"],\n",
    "                    \"dynamic_errors\": res[\"dynamic_errors\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n====================\")\n",
    "            print(\"TEST\", res[\"id\"], \"PASS\" if res[\"pass\"] else \"FAIL\")\n",
    "            print(\"ENGINE intent:\", res[\"engine_plan\"].intent, \"next_step:\", res[\"engine_plan\"].next_step)\n",
    "            print(\"ENGINE flags :\", \"web\", res[\"engine_plan\"].use_websearch,\n",
    "                  \"ltm\", res[\"engine_plan\"].use_user_longterm_memory,\n",
    "                  \"rag\", res[\"engine_plan\"].use_rag,\n",
    "                  \"tools\", res[\"engine_plan\"].use_tools)\n",
    "            print(\"STATIC :\", res[\"static_steps\"])\n",
    "            print(\"DYNAMIC:\", res[\"dynamic_steps\"])\n",
    "            if res[\"intent_errors\"]:\n",
    "                print(\"INTENT ERR:\", res[\"intent_errors\"])\n",
    "            if res[\"flag_errors\"]:\n",
    "                print(\"FLAGS ERR :\", res[\"flag_errors\"])\n",
    "            if res[\"next_step_errors\"]:\n",
    "                print(\"STEP ERR  :\", res[\"next_step_errors\"])\n",
    "            if res[\"static_errors\"]:\n",
    "                print(\"STATIC ERR:\", res[\"static_errors\"])\n",
    "            if res[\"dynamic_errors\"]:\n",
    "                print(\"DYN ERR   :\", res[\"dynamic_errors\"])\n",
    "\n",
    "    def pct(x: int, denom: int) -> float:\n",
    "        return 0.0 if denom <= 0 else (100.0 * float(x)) / float(denom)\n",
    "\n",
    "    summary = {\n",
    "        \"totals\": totals,\n",
    "        \"accuracy\": pct(totals[\"pass\"], totals[\"total\"]),\n",
    "        \"intent_accuracy\": pct(totals[\"intent_ok\"], totals[\"total\"]),\n",
    "        \"flags_accuracy\": pct(totals[\"flags_ok\"], totals[\"total\"]),\n",
    "        \"next_step_accuracy\": pct(totals[\"next_step_ok\"], totals[\"total\"]),\n",
    "        \"static_accuracy\": pct(totals[\"static_ok\"], totals[\"total\"]),\n",
    "        \"dynamic_accuracy\": pct(totals[\"dynamic_ok\"], totals[\"total\"]),\n",
    "        \"per_group\": {\n",
    "            g: {\n",
    "                **st,\n",
    "                \"accuracy\": pct(st[\"pass\"], st[\"total\"]),\n",
    "                \"intent_accuracy\": pct(st[\"intent_ok\"], st[\"total\"]),\n",
    "                \"flags_accuracy\": pct(st[\"flags_ok\"], st[\"total\"]),\n",
    "                \"next_step_accuracy\": pct(st[\"next_step_ok\"], st[\"total\"]),\n",
    "                \"static_accuracy\": pct(st[\"static_ok\"], st[\"total\"]),\n",
    "                \"dynamic_accuracy\": pct(st[\"dynamic_ok\"], st[\"total\"]),\n",
    "            }\n",
    "            for g, st in per_group.items()\n",
    "        },\n",
    "        \"fails\": fails_compact,\n",
    "        \"per_test\": per_test,\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "await run_all_metrics(tests=TESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7464d",
   "metadata": {},
   "source": [
    "## Expand the test suite (starter pack)\n",
    "\n",
    "We add a minimal but representative set of cases to cover:\n",
    "- CLARIFY routing\n",
    "- TOOLS routing\n",
    "- RAG routing\n",
    "- FRESHNESS / WEBSEARCH routing\n",
    "- PROJECT_ARCHITECTURE with project context (soft intent, hard capability expectations)\n",
    "\n",
    "Each case includes:\n",
    "- expected EnginePlan intent/flags/next_step (as strict as we can be today)\n",
    "- expected StepPlanner STATIC plan (must include / forbid)\n",
    "- expected StepPlanner DYNAMIC plan (single step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540242de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tests: 7\n",
      "Added: ['F01', 'G01', 'C01', 'T01', 'R01', 'F02', 'P01']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5333ab91aaa4628aa7d368b266e3b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Engine+Step planner tests (metrics):   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "TEST F01 PASS\n",
      "ENGINE intent: PlanIntent.FRESHNESS next_step: EngineNextStep.WEBSEARCH\n",
      "ENGINE flags : web True ltm False rag False tools False\n",
      "STATIC : [<StepId.WEBSEARCH: 'websearch'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.WEBSEARCH: 'websearch'>]\n",
      "\n",
      "====================\n",
      "TEST G01 PASS\n",
      "ENGINE intent: PlanIntent.GENERIC next_step: EngineNextStep.SYNTHESIZE\n",
      "ENGINE flags : web False ltm False rag False tools False\n",
      "STATIC : [<StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.DRAFT: 'draft'>]\n",
      "\n",
      "====================\n",
      "TEST C01 PASS\n",
      "ENGINE intent: PlanIntent.CLARIFY next_step: EngineNextStep.CLARIFY\n",
      "ENGINE flags : web False ltm False rag False tools False\n",
      "STATIC : [<StepId.CLARIFY: 'clarify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.CLARIFY: 'clarify'>]\n",
      "\n",
      "====================\n",
      "TEST T01 PASS\n",
      "ENGINE intent: PlanIntent.GENERIC next_step: EngineNextStep.TOOLS\n",
      "ENGINE flags : web False ltm True rag False tools True\n",
      "STATIC : [<StepId.TOOLS: 'tools'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.TOOLS: 'tools'>]\n",
      "\n",
      "====================\n",
      "TEST R01 PASS\n",
      "ENGINE intent: PlanIntent.PROJECT_ARCHITECTURE next_step: EngineNextStep.RAG\n",
      "ENGINE flags : web False ltm True rag True tools False\n",
      "STATIC : [<StepId.LTM_SEARCH: 'ltm_search'>, <StepId.RAG: 'rag'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.RAG: 'rag'>]\n",
      "\n",
      "====================\n",
      "TEST F02 PASS\n",
      "ENGINE intent: PlanIntent.FRESHNESS next_step: EngineNextStep.WEBSEARCH\n",
      "ENGINE flags : web True ltm False rag False tools False\n",
      "STATIC : [<StepId.WEBSEARCH: 'websearch'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.WEBSEARCH: 'websearch'>]\n",
      "\n",
      "====================\n",
      "TEST P01 FAIL\n",
      "ENGINE intent: PlanIntent.PROJECT_ARCHITECTURE next_step: EngineNextStep.RAG\n",
      "ENGINE flags : web False ltm True rag True tools False\n",
      "STATIC : [<StepId.LTM_SEARCH: 'ltm_search'>, <StepId.RAG: 'rag'>, <StepId.DRAFT: 'draft'>, <StepId.VERIFY: 'verify'>, <StepId.FINAL: 'final'>]\n",
      "DYNAMIC: [<StepId.RAG: 'rag'>]\n",
      "DYN ERR   : ['dynamic single step expected=StepId.DRAFT got=StepId.RAG']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'totals': {'total': 7,\n",
       "  'pass': 6,\n",
       "  'intent_ok': 7,\n",
       "  'flags_ok': 7,\n",
       "  'next_step_ok': 7,\n",
       "  'static_ok': 7,\n",
       "  'dynamic_ok': 6},\n",
       " 'accuracy': 85.71428571428571,\n",
       " 'intent_accuracy': 100.0,\n",
       " 'flags_accuracy': 100.0,\n",
       " 'next_step_accuracy': 100.0,\n",
       " 'static_accuracy': 100.0,\n",
       " 'dynamic_accuracy': 85.71428571428571,\n",
       " 'per_group': {'F': {'total': 2,\n",
       "   'pass': 2,\n",
       "   'intent_ok': 2,\n",
       "   'flags_ok': 2,\n",
       "   'next_step_ok': 2,\n",
       "   'static_ok': 2,\n",
       "   'dynamic_ok': 2,\n",
       "   'accuracy': 100.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 100.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 100.0},\n",
       "  'G': {'total': 1,\n",
       "   'pass': 1,\n",
       "   'intent_ok': 1,\n",
       "   'flags_ok': 1,\n",
       "   'next_step_ok': 1,\n",
       "   'static_ok': 1,\n",
       "   'dynamic_ok': 1,\n",
       "   'accuracy': 100.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 100.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 100.0},\n",
       "  'C': {'total': 1,\n",
       "   'pass': 1,\n",
       "   'intent_ok': 1,\n",
       "   'flags_ok': 1,\n",
       "   'next_step_ok': 1,\n",
       "   'static_ok': 1,\n",
       "   'dynamic_ok': 1,\n",
       "   'accuracy': 100.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 100.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 100.0},\n",
       "  'T': {'total': 1,\n",
       "   'pass': 1,\n",
       "   'intent_ok': 1,\n",
       "   'flags_ok': 1,\n",
       "   'next_step_ok': 1,\n",
       "   'static_ok': 1,\n",
       "   'dynamic_ok': 1,\n",
       "   'accuracy': 100.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 100.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 100.0},\n",
       "  'R': {'total': 1,\n",
       "   'pass': 1,\n",
       "   'intent_ok': 1,\n",
       "   'flags_ok': 1,\n",
       "   'next_step_ok': 1,\n",
       "   'static_ok': 1,\n",
       "   'dynamic_ok': 1,\n",
       "   'accuracy': 100.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 100.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 100.0},\n",
       "  'P': {'total': 1,\n",
       "   'pass': 0,\n",
       "   'intent_ok': 1,\n",
       "   'flags_ok': 1,\n",
       "   'next_step_ok': 1,\n",
       "   'static_ok': 1,\n",
       "   'dynamic_ok': 0,\n",
       "   'accuracy': 0.0,\n",
       "   'intent_accuracy': 100.0,\n",
       "   'flags_accuracy': 100.0,\n",
       "   'next_step_accuracy': 100.0,\n",
       "   'static_accuracy': 100.0,\n",
       "   'dynamic_accuracy': 0.0}},\n",
       " 'fails': [{'test_id': 'P01',\n",
       "   'group': 'P',\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': False,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': ['dynamic single step expected=StepId.DRAFT got=StepId.RAG']}],\n",
       " 'per_test': [{'id': 'F01',\n",
       "   'pass': True,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.FRESHNESS: 'freshness'>, reasoning_summary='User requests recent information, next_step set to websearch to retrieve up-to-date external info.', ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.WEBSEARCH: 'websearch'>, use_websearch=True, use_user_longterm_memory=False, use_rag=False, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'freshness', 'next_step': 'websearch', 'reasoning_summary': 'User requests recent information, next_step set to websearch to retrieve up-to-date external info.', 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_json_len': 371, 'next_step_raw': 'websearch', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_raw_len': 371, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"freshness\",\\n  \"next_step\": \"websearch\",\\n  \"reasoning_summary\": \"User requests recent information, next_step set to websearch to retrieve up-to-date external info.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}', 'planner_raw_tail_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"freshness\",\\n  \"next_step\": \"websearch\",\\n  \"reasoning_summary\": \"User requests recent information, next_step set to websearch to retrieve up-to-date external info.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.WEBSEARCH: 'websearch'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.WEBSEARCH: 'websearch'>]},\n",
       "  {'id': 'G01',\n",
       "   'pass': True,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.GENERIC: 'generic'>, reasoning_summary='The user asks for a general explanation of REST vs GraphQL, which does not require up-to-date information or project-specific details.', ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.SYNTHESIZE: 'synthesize'>, use_websearch=False, use_user_longterm_memory=False, use_rag=False, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'generic', 'next_step': 'synthesize', 'reasoning_summary': 'The user asks for a general explanation of REST vs GraphQL, which does not require up-to-date information or project-specific details.', 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': False, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_json_len': 407, 'next_step_raw': 'synthesize', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': False, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_raw_len': 407, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"generic\",\\n  \"next_step\": \"synthesize\",\\n  \"reasoning_summary\": \"The user asks for a general explanation of REST vs GraphQL, which does not require up-to-date information or project-specific details.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": ', 'planner_raw_tail_preview': 'rsion\": \"1.0\",\\n  \"intent\": \"generic\",\\n  \"next_step\": \"synthesize\",\\n  \"reasoning_summary\": \"The user asks for a general explanation of REST vs GraphQL, which does not require up-to-date information or project-specific details.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.DRAFT: 'draft'>]},\n",
       "  {'id': 'C01',\n",
       "   'pass': True,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.CLARIFY: 'clarify'>, reasoning_summary=\"User request is ambiguous, missing critical information; intent='clarify' to ask for clarification.\", ask_clarifying_question=True, clarifying_question='Please provide the full traceback of your Python code exception and any relevant environment or version details.', next_step=<EngineNextStep.CLARIFY: 'clarify'>, use_websearch=False, use_user_longterm_memory=False, use_rag=False, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'clarify', 'next_step': 'clarify', 'reasoning_summary': \"User request is ambiguous, missing critical information; intent='clarify' to ask for clarification.\", 'ask_clarifying_question': True, 'clarifying_question': 'Please provide the full traceback of your Python code exception and any relevant environment or version details.', 'use_websearch': False, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_json_len': 478, 'next_step_raw': 'clarify', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': False, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_raw_len': 478, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"clarify\",\\n  \"next_step\": \"clarify\",\\n  \"reasoning_summary\": \"User request is ambiguous, missing critical information; intent=\\'clarify\\' to ask for clarification.\",\\n  \"ask_clarifying_question\": true,\\n  \"clarifying_question\": \"Please provide the full traceback of your Python code exception and any relevant environment or version details.\",\\n  \"use_websearch\": false,\\n ', 'planner_raw_tail_preview': 'oning_summary\": \"User request is ambiguous, missing critical information; intent=\\'clarify\\' to ask for clarification.\",\\n  \"ask_clarifying_question\": true,\\n  \"clarifying_question\": \"Please provide the full traceback of your Python code exception and any relevant environment or version details.\",\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.CLARIFY: 'clarify'>, <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.CLARIFY: 'clarify'>]},\n",
       "  {'id': 'T01',\n",
       "   'pass': True,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.GENERIC: 'generic'>, reasoning_summary='Intent is generic because the user provides structured input and asks for transformation. Next step is tools because deterministic transformation/validation is needed.', ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.TOOLS: 'tools'>, use_websearch=False, use_user_longterm_memory=True, use_rag=False, use_tools=True, debug={'raw_json': {'version': '1.0', 'intent': 'generic', 'next_step': 'tools', 'reasoning_summary': 'Intent is generic because the user provides structured input and asks for transformation. Next step is tools because deterministic transformation/validation is needed.', 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': False, 'use_user_longterm_memory': True, 'use_rag': False, 'use_tools': True}, 'planner_json_len': 433, 'next_step_raw': 'tools', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': False, 'use_user_longterm_memory': True, 'use_rag': False, 'use_tools': True}, 'planner_raw_len': 433, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"generic\",\\n  \"next_step\": \"tools\",\\n  \"reasoning_summary\": \"Intent is generic because the user provides structured input and asks for transformation. Next step is tools because deterministic transformation/validation is needed.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": true,\\n  \"use_r', 'planner_raw_tail_preview': ' \"generic\",\\n  \"next_step\": \"tools\",\\n  \"reasoning_summary\": \"Intent is generic because the user provides structured input and asks for transformation. Next step is tools because deterministic transformation/validation is needed.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": true,\\n  \"use_rag\": false,\\n  \"use_tools\": true\\n}'}),\n",
       "   'static_steps': [<StepId.TOOLS: 'tools'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.TOOLS: 'tools'>]},\n",
       "  {'id': 'R01',\n",
       "   'pass': True,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.PROJECT_ARCHITECTURE: 'project_architecture'>, reasoning_summary=\"User's intent is to answer based on their codebase, so project architecture intent is chosen. Use rag capability first as it requires internal documents or user long-term memory.\", ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.RAG: 'rag'>, use_websearch=False, use_user_longterm_memory=True, use_rag=True, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'project_architecture', 'next_step': 'rag', 'reasoning_summary': \"User's intent is to answer based on their codebase, so project architecture intent is chosen. Use rag capability first as it requires internal documents or user long-term memory.\", 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': False, 'use_user_longterm_memory': True, 'use_rag': True, 'use_tools': False}, 'planner_json_len': 455, 'next_step_raw': 'rag', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': False, 'use_user_longterm_memory': True, 'use_rag': True, 'use_tools': False}, 'planner_raw_len': 455, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"project_architecture\",\\n  \"next_step\": \"rag\",\\n  \"reasoning_summary\": \"User\\'s intent is to answer based on their codebase, so project architecture intent is chosen. Use rag capability first as it requires internal documents or user long-term memory.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_m', 'planner_raw_tail_preview': '\",\\n  \"next_step\": \"rag\",\\n  \"reasoning_summary\": \"User\\'s intent is to answer based on their codebase, so project architecture intent is chosen. Use rag capability first as it requires internal documents or user long-term memory.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": true,\\n  \"use_rag\": true,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.LTM_SEARCH: 'ltm_search'>,\n",
       "    <StepId.RAG: 'rag'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.RAG: 'rag'>]},\n",
       "  {'id': 'F02',\n",
       "   'pass': True,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': True,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': [],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.FRESHNESS: 'freshness'>, reasoning_summary=\"Intent is 'freshness' because the user asks for changes in Python 3.13 compared to 3.12, implying a need for up-to-date information. Websearch will be used first as it's likely required for this task.\", ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.WEBSEARCH: 'websearch'>, use_websearch=True, use_user_longterm_memory=False, use_rag=False, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'freshness', 'next_step': 'websearch', 'reasoning_summary': \"Intent is 'freshness' because the user asks for changes in Python 3.13 compared to 3.12, implying a need for up-to-date information. Websearch will be used first as it's likely required for this task.\", 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_json_len': 473, 'next_step_raw': 'websearch', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': True, 'use_user_longterm_memory': False, 'use_rag': False, 'use_tools': False}, 'planner_raw_len': 473, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"freshness\",\\n  \"next_step\": \"websearch\",\\n  \"reasoning_summary\": \"Intent is \\'freshness\\' because the user asks for changes in Python 3.13 compared to 3.12, implying a need for up-to-date information. Websearch will be used first as it\\'s likely required for this task.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use', 'planner_raw_tail_preview': ',\\n  \"reasoning_summary\": \"Intent is \\'freshness\\' because the user asks for changes in Python 3.13 compared to 3.12, implying a need for up-to-date information. Websearch will be used first as it\\'s likely required for this task.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": true,\\n  \"use_user_longterm_memory\": false,\\n  \"use_rag\": false,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.WEBSEARCH: 'websearch'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.WEBSEARCH: 'websearch'>]},\n",
       "  {'id': 'P01',\n",
       "   'pass': False,\n",
       "   'intent_ok': True,\n",
       "   'flags_ok': True,\n",
       "   'next_step_ok': True,\n",
       "   'static_ok': True,\n",
       "   'dynamic_ok': False,\n",
       "   'intent_errors': [],\n",
       "   'flag_errors': [],\n",
       "   'next_step_errors': [],\n",
       "   'static_errors': [],\n",
       "   'dynamic_errors': ['dynamic single step expected=StepId.DRAFT got=StepId.RAG'],\n",
       "   'engine_plan': EnginePlan(version='1.0', intent=<PlanIntent.PROJECT_ARCHITECTURE: 'project_architecture'>, reasoning_summary='User requires project-specific information, so project_architecture intent chosen and rag capability selected first.', ask_clarifying_question=False, clarifying_question=None, next_step=<EngineNextStep.RAG: 'rag'>, use_websearch=False, use_user_longterm_memory=True, use_rag=True, use_tools=False, debug={'raw_json': {'version': '1.0', 'intent': 'project_architecture', 'next_step': 'rag', 'reasoning_summary': 'User requires project-specific information, so project_architecture intent chosen and rag capability selected first.', 'ask_clarifying_question': False, 'clarifying_question': None, 'use_websearch': False, 'use_user_longterm_memory': True, 'use_rag': True, 'use_tools': False}, 'planner_json_len': 393, 'next_step_raw': 'rag', 'capability_clamp': {'websearch_available': True, 'user_ltm_available': True, 'rag_available': True, 'tools_available': True, 'use_websearch': False, 'use_user_longterm_memory': True, 'use_rag': True, 'use_tools': False}, 'planner_raw_len': 393, 'planner_raw_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"project_architecture\",\\n  \"next_step\": \"rag\",\\n  \"reasoning_summary\": \"User requires project-specific information, so project_architecture intent chosen and rag capability selected first.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": true,\\n  \"use_rag\": true,\\n  \"use_tools\": false\\n}', 'planner_raw_tail_preview': '{\\n  \"version\": \"1.0\",\\n  \"intent\": \"project_architecture\",\\n  \"next_step\": \"rag\",\\n  \"reasoning_summary\": \"User requires project-specific information, so project_architecture intent chosen and rag capability selected first.\",\\n  \"ask_clarifying_question\": false,\\n  \"clarifying_question\": null,\\n  \"use_websearch\": false,\\n  \"use_user_longterm_memory\": true,\\n  \"use_rag\": true,\\n  \"use_tools\": false\\n}'}),\n",
       "   'static_steps': [<StepId.LTM_SEARCH: 'ltm_search'>,\n",
       "    <StepId.RAG: 'rag'>,\n",
       "    <StepId.DRAFT: 'draft'>,\n",
       "    <StepId.VERIFY: 'verify'>,\n",
       "    <StepId.FINAL: 'final'>],\n",
       "   'dynamic_steps': [<StepId.RAG: 'rag'>]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.planning.engine_plan_models import PlanIntent, EngineNextStep\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import StepId\n",
    "\n",
    "MORE_TESTS: List[PlannerTestCase] = [\n",
    "    PlannerTestCase(\n",
    "        id=\"F01\",\n",
    "        question=\"What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\",\n",
    "        use_project_context=False,\n",
    "        engine=EngineExpect(\n",
    "            intent=PlanIntent.FRESHNESS,\n",
    "            next_step=EngineNextStep.WEBSEARCH,\n",
    "            use_websearch=True,\n",
    "            use_rag=False,\n",
    "            use_tools=False,\n",
    "            use_ltm=False,\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.WEBSEARCH, StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.LTM_SEARCH, StepId.TOOLS, StepId.RAG},\n",
    "            dynamic_single_step=StepId.WEBSEARCH,\n",
    "        ),\n",
    "    ),\n",
    "    PlannerTestCase(\n",
    "        id=\"G01\",\n",
    "        question=\"Explain the main differences between REST and GraphQL.\",\n",
    "        use_project_context=False,\n",
    "        engine=EngineExpect(\n",
    "            intent=PlanIntent.GENERIC,\n",
    "            use_websearch=False,  # prefer no web for this general question\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.WEBSEARCH},  # allow RAG/tools depending on your policies; keep it light\n",
    "            dynamic_single_step=StepId.DRAFT,  # likely SYNTHESIZE; if engine picks differently, this will flag it\n",
    "        ),\n",
    "    ),\n",
    "    \n",
    "    # --- CLARIFY ---    \n",
    "    PlannerTestCase(\n",
    "        id=\"C01\",\n",
    "        question=\"I have an exception in my Python code but I didn't include the traceback. What should I do?\",\n",
    "        use_project_context=False,\n",
    "        engine=EngineExpect(\n",
    "            intent=None,\n",
    "            next_step=EngineNextStep.CLARIFY,\n",
    "            use_websearch=False,\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.CLARIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.WEBSEARCH, StepId.TOOLS, StepId.RAG, StepId.LTM_SEARCH},\n",
    "            dynamic_single_step=StepId.CLARIFY,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # --- TOOLS (structured action) ---\n",
    "    PlannerTestCase(\n",
    "        id=\"T01\",\n",
    "        question=(\n",
    "            \"Return the exact list of unique 'pkd' values from this JSON sorted ascending. \"\n",
    "            \"Only output the JSON array of strings.\\n\\n\"\n",
    "            f\"JSON:\\n{r'''\n",
    "{\n",
    "  \"items\": [\n",
    "    {\"pkd\": \"62.01.Z\"},\n",
    "    {\"pkd\": \"62.02.Z\"},\n",
    "    {\"pkd\": \"62.01.Z\"},\n",
    "    {\"pkd\": \"70.22.Z\"}\n",
    "  ]\n",
    "}\n",
    "'''.strip()}\"\n",
    "        ),\n",
    "        use_project_context=False,\n",
    "        engine=EngineExpect(\n",
    "            intent=PlanIntent.GENERIC,\n",
    "            next_step=EngineNextStep.TOOLS,\n",
    "            use_tools=True,\n",
    "            use_websearch=False,\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.TOOLS, StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.WEBSEARCH},\n",
    "            dynamic_single_step=StepId.TOOLS,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # --- RAG (doc/KB lookup) ---\n",
    "    PlannerTestCase(\n",
    "        id=\"R01\",\n",
    "        question=\"In our runtime, what does StepPlannerConfig.max_total_steps control? Answer based on our codebase.\",\n",
    "        use_project_context=True,\n",
    "        engine=EngineExpect(\n",
    "            # This is where you *want* project intent, but currently may be GENERIC.\n",
    "            intent=None,\n",
    "            next_step=EngineNextStep.RAG,\n",
    "            use_rag=True,\n",
    "            use_websearch=False,\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.RAG, StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.WEBSEARCH},\n",
    "            dynamic_single_step=StepId.RAG,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # --- FRESHNESS / WEBSEARCH (another example) ---\n",
    "    PlannerTestCase(\n",
    "        id=\"F02\",\n",
    "        question=\"What changed in Python 3.13 compared to 3.12? Provide a concise summary with dates.\",\n",
    "        use_project_context=False,\n",
    "        engine=EngineExpect(\n",
    "            intent=PlanIntent.FRESHNESS,\n",
    "            next_step=EngineNextStep.WEBSEARCH,\n",
    "            use_websearch=True,\n",
    "            use_tools=False,\n",
    "            use_rag=False,\n",
    "            use_ltm=False,\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.WEBSEARCH, StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.TOOLS, StepId.RAG, StepId.LTM_SEARCH},\n",
    "            dynamic_single_step=StepId.WEBSEARCH,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # --- PROJECT_ARCHITECTURE (soft intent, hard \"no web\", prefer internal retrieval) ---\n",
    "    PlannerTestCase(\n",
    "        id=\"P01\",\n",
    "        question=\"In Intergrax, describe the difference between STATIC and DYNAMIC plan build modes in StepPlanner.\",\n",
    "        use_project_context=True,\n",
    "        engine=EngineExpect(\n",
    "            # soft intent until EnginePlanner prompt is improved\n",
    "            intent=None,\n",
    "            # planner may choose RAG or SYNTHESIZE; we do not hard-fail on next_step yet\n",
    "            next_step=None,\n",
    "            use_websearch=False,   # hard: no web for internal question\n",
    "            use_tools=False,       # hard: no tools needed\n",
    "            # allow rag/ltm: at least one of them should be true, but EngineExpect can't express OR;\n",
    "            # we’ll handle this as a separate “policy check” later if needed.\n",
    "        ),\n",
    "        step=StepExpect(\n",
    "            static_must_include={StepId.DRAFT, StepId.VERIFY, StepId.FINAL},\n",
    "            static_forbid={StepId.WEBSEARCH, StepId.TOOLS},\n",
    "            dynamic_single_step=None,  # allow planner to choose\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Total tests:\", len(MORE_TESTS))\n",
    "print(\"Added:\", [t.id for t in MORE_TESTS])\n",
    "\n",
    "await run_all_metrics(tests=MORE_TESTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
