{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cef8ef",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61048e3b",
   "metadata": {},
   "source": [
    "# 06 — Session & Memory Roundtrip (Configurable, Real Adapters)\n",
    "\n",
    "Goals:\n",
    "- Verify that DropInKnowledgeRuntime can:\n",
    "  - create a new session (when session_id is None),\n",
    "  - reuse an existing session (when session_id is provided),\n",
    "  - persist and load conversation history via SessionManager.get_history(...),\n",
    "  - produce a consistent debug_trace[\"steps\"].\n",
    "\n",
    "Notes:\n",
    "- This notebook uses real adapters (no fakes, no fallbacks).\n",
    "- Edit only the \"Runtime configuration\" cell to switch LLM / embeddings / vector store implementations.\n",
    "- RAG / websearch / tools are disabled in this notebook to keep the baseline stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ab1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8ba68",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LangChainOllamaAdapter.__init__() missing 1 required positional argument: 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintergrax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GLOBAL_SETTINGS\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintergrax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_adapters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mollama_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangChainOllamaAdapter\n\u001b[1;32m----> 9\u001b[0m llm_adapter \u001b[38;5;241m=\u001b[39m \u001b[43mLangChainOllamaAdapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGLOBAL_SETTINGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_ollama_model\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# --- Embeddings ---\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mintergrax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingManager\n",
      "\u001b[1;31mTypeError\u001b[0m: LangChainOllamaAdapter.__init__() missing 1 required positional argument: 'chat'"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# RUNTIME CONFIGURATION\n",
    "# ==========================================================\n",
    "\n",
    "from intergrax.globals.settings import GLOBAL_SETTINGS\n",
    "from intergrax.llm_adapters.ollama_adapter import LangChainOllamaAdapter\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "\n",
    "\n",
    "# --- Adapter ---\n",
    "llm_adapter = LangChainOllamaAdapter()\n",
    "\n",
    "\n",
    "# --- Embeddings ---\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=\"ollama\",\n",
    "    model_name=GLOBAL_SETTINGS.default_ollama_embed_model, \n",
    "    assume_ollama_dim=1536)\n",
    "\n",
    "# --- Vector store ---\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "\n",
    "vectorstore_manager = VectorstoreManager(\n",
    "    config = VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=\"intergrax_docs\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Feature flags ---\n",
    "ENABLE_RAG = False\n",
    "ENABLE_WEBSEARCH = False\n",
    "ENABLE_TOOLS = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
