{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cef8ef",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61048e3b",
   "metadata": {},
   "source": [
    "# 06 — Session & Memory Roundtrip (Configurable, Real Adapters)\n",
    "\n",
    "Goals:\n",
    "- Verify that DropInKnowledgeRuntime can:\n",
    "  - create a new session (when session_id is None),\n",
    "  - reuse an existing session (when session_id is provided),\n",
    "  - persist and load conversation history via SessionManager.get_history(...),\n",
    "  - produce a consistent debug_trace[\"steps\"].\n",
    "\n",
    "Notes:\n",
    "- This notebook uses real adapters (no fakes, no fallbacks).\n",
    "- Edit only the \"Runtime configuration\" cell to switch LLM / embeddings / vector store implementations.\n",
    "- RAG / websearch / tools are disabled in this notebook to keep the baseline stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ab1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=intergrax_docs\n",
      "[intergraxVectorstoreManager] Existing count: 0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# RUNTIME CONFIGURATION\n",
    "# ==========================================================\n",
    "\n",
    "from intergrax.llm_adapters.base import LLMAdapterRegistry, LLMProvider\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "\n",
    "# --- Adapter ---\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "\n",
    "# --- Embeddings ---\n",
    "embed_manager = EmbeddingManager(\n",
    "    verbose=True,\n",
    "    provider=\"ollama\")\n",
    "\n",
    "\n",
    "# --- Vector store ---\n",
    "vectorstore_manager = VectorstoreManager(\n",
    "    config = VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=\"intergrax_docs\"\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# In-memory session storage for notebook-level tests\n",
    "storage = InMemorySessionStorage()\n",
    "session_manager = SessionManager(storage=storage)\n",
    "\n",
    "# Runtime config (baseline: optional layers disabled)\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=vectorstore_manager,\n",
    "    enable_rag=False,\n",
    "    enable_websearch=False,\n",
    "    tools_mode=\"off\"\n",
    ")\n",
    "\n",
    "# Drop-In runtime (baseline wiring)\n",
    "runtime = DropInKnowledgeRuntime(\n",
    "    config=config,\n",
    "    session_manager=session_manager,\n",
    "    ingestion_service=None,\n",
    "    context_builder=None,\n",
    "    rag_prompt_builder=None,\n",
    "    websearch_prompt_builder=None,\n",
    "    history_prompt_builder=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87045e7d",
   "metadata": {},
   "source": [
    "Helpers\n",
    "\n",
    "Utility functions used in this notebook:\n",
    "- printing session history,\n",
    "- printing runtime debug trace,\n",
    "- basic assertions for sanity checks.\n",
    "\n",
    "These helpers do not depend on runtime configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e3f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intergrax.llm.messages import ChatMessage\n",
    "\n",
    "\n",
    "def print_history(messages: list[ChatMessage], title: str) -> None:\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(\"Messages count:\", len(messages))\n",
    "    for i, msg in enumerate(messages, start=1):\n",
    "        # created_at is optional in some message implementations\n",
    "        created_str = msg.created_at if msg.created_at else \"n/a\"\n",
    "        print(f\"  [{i}] role={msg.role!r}, created_at={created_str}\")\n",
    "        print(f\"      content={msg.content!r}\")\n",
    "\n",
    "\n",
    "def print_debug_steps(answer, limit: int = 100) -> None:\n",
    "    trace = answer.debug_trace or {}\n",
    "    steps = trace.get(\"steps\", []) or []\n",
    "    print(\"\\n=== DEBUG TRACE STEPS ===\")\n",
    "    print(\"Steps count:\", len(steps))\n",
    "    for i, step in enumerate(steps[:limit], start=1):\n",
    "        print(\n",
    "            f\"  [{i}] {step['timestamp']} | \"\n",
    "            f\"{step['component']} | \"\n",
    "            f\"{step['step']} | \"\n",
    "            f\"{step.get('message', '')}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def assert_true(cond: bool, message: str) -> None:\n",
    "    if not cond:\n",
    "        raise AssertionError(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6eaa5e",
   "metadata": {},
   "source": [
    "Test 1 — First request (session creation)\n",
    "\n",
    "Expected behavior:\n",
    "- A new session is created when session_id is None.\n",
    "- The response contains a valid answer.\n",
    "- debug_trace contains a session_id.\n",
    "- Session history contains at least:\n",
    "  - one user message,\n",
    "  - one assistant message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906bbb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER 1 ===\n",
      "Sounds like you're testing the limits of our conversation capabilities. I'm ready when you are. What's the first part of the test?\n",
      "\n",
      "=== DEBUG TRACE STEPS ===\n",
      "Steps count: 5\n",
      "  [1] 2025-12-12T13:32:48.606544+00:00 | engine | memory_layer | Profile-based instructions loaded for session.\n",
      "  [2] 2025-12-12T13:32:50.888019+00:00 | engine | history | Conversation history built for LLM.\n",
      "  [3] 2025-12-12T13:32:56.374440+00:00 | engine | core_llm | Core LLM adapter returned a plain string.\n",
      "  [4] 2025-12-12T13:32:56.374440+00:00 | engine | persist_and_build_answer | Assistant answer persisted and RuntimeAnswer built.\n",
      "  [5] 2025-12-12T13:32:56.374440+00:00 | engine | run_end | DropInKnowledgeRuntime.run() finished.\n",
      "\n",
      "Session ID: 31bafa1b-aa81-4a1c-8eba-42508f35a807\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "\n",
    "async def run_first_request():\n",
    "    request = RuntimeRequest(\n",
    "        user_id=\"user_demo\",\n",
    "        session_id=None,\n",
    "        message=\"Hello. This is a basic session and memory roundtrip test.\",\n",
    "        instructions=None,\n",
    "        attachments=None,\n",
    "        metadata={},\n",
    "    )\n",
    "    return await runtime.run(request)\n",
    "\n",
    "\n",
    "answer_1 = await run_first_request()\n",
    "\n",
    "print(\"=== ANSWER 1 ===\")\n",
    "print(answer_1.answer)\n",
    "\n",
    "print_debug_steps(answer_1)\n",
    "\n",
    "# Validate session creation\n",
    "session_id = answer_1.debug_trace[\"session_id\"]\n",
    "assert_true(bool(session_id), \"Expected session_id in debug_trace.\")\n",
    "print(\"\\nSession ID:\", session_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac57356",
   "metadata": {},
   "source": [
    "## Test 2 — Reload session history\n",
    "\n",
    "Goal:\n",
    "- Reload conversation history using an existing `session_id`\n",
    "- Verify that the history was correctly persisted\n",
    "- Assert a minimum number of messages (>= 2)\n",
    "\n",
    "Scope:\n",
    "- SessionManager.get_history(session_id)\n",
    "- No runtime.run invocation\n",
    "- Pure session storage validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b0af8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 31bafa1b-aa81-4a1c-8eba-42508f35a807\n",
      "History length: 2\n",
      "[0] role=user content='Hello. This is a basic session and memory roundtrip test.'\n",
      "[1] role=assistant content=\"Sounds like you're testing the limits of our conversation ca\"\n"
     ]
    }
   ],
   "source": [
    "# Test 2 — Reload session history\n",
    "\n",
    "# Ensure session_id exists from Test 1\n",
    "assert session_id is not None, \"session_id must not be None after Test 1\"\n",
    "\n",
    "# Load history directly from SessionManager\n",
    "history = await session_manager.get_history(session_id)\n",
    "\n",
    "# --- Assertions ---\n",
    "\n",
    "assert history is not None, \"History must not be None\"\n",
    "assert isinstance(history, list), \"History must be a list\"\n",
    "assert len(history) >= 2, f\"Expected at least 2 messages, got {len(history)}\"\n",
    "\n",
    "# --- Debug output ---\n",
    "\n",
    "print(f\"Session ID: {session_id}\")\n",
    "print(f\"History length: {len(history)}\")\n",
    "\n",
    "for index, message in enumerate(history):\n",
    "    print(\n",
    "        f\"[{index}] \"\n",
    "        f\"role={message.role} \"\n",
    "        f\"content={message.content[:60]!r}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9bc71",
   "metadata": {},
   "source": [
    "## Test 3 — Second request using the same session\n",
    "\n",
    "Goal:\n",
    "- Send a second request using the existing `session_id`\n",
    "- Verify that the conversation history is extended within the same session\n",
    "\n",
    "Scope:\n",
    "- runtime.run(RuntimeRequest)\n",
    "- Validate history growth (>= 2 messages added)\n",
    "- Baseline only (no RAG, tools, websearch, CoT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26b6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID (input): 31bafa1b-aa81-4a1c-8eba-42508f35a807\n",
      "History length: before=2, after=4\n",
      "Last 2 messages:\n",
      "[-2] role=user content='This is the second message in the same session.'\n",
      "[-1] role=assistant content='And this is my response to that second message. How does the session look so far'\n",
      "Debug trace (second request):\n",
      "session_id\n",
      "user_id\n",
      "config\n",
      "memory_layer\n",
      "steps\n",
      "base_history_length\n",
      "history_tokens\n",
      "history_length\n",
      "instructions\n",
      "rag_chunks\n"
     ]
    }
   ],
   "source": [
    "# Test 3 — Second request using the same session (RuntimeRequest-based)\n",
    "\n",
    "# Load history length before the second request\n",
    "history_before = await session_manager.get_history(session_id)\n",
    "\n",
    "assert history_before is not None, \"Preloaded history must not be None\"\n",
    "assert isinstance(history_before, list), \"Preloaded history must be a list\"\n",
    "assert len(history_before) >= 2, f\"Expected at least 2 messages before Test 3, got {len(history_before)}\"\n",
    "\n",
    "before_len = len(history_before)\n",
    "\n",
    "# Build a RuntimeRequest using the existing session_id\n",
    "request = RuntimeRequest(\n",
    "    user_id=\"user_demo\",\n",
    "    session_id=session_id,\n",
    "    message=\"This is the second message in the same session.\",\n",
    ")\n",
    "\n",
    "# Run the second request\n",
    "second_answer = await runtime.run(request)\n",
    "\n",
    "# Reload history after the second request\n",
    "history_after = await session_manager.get_history(session_id)\n",
    "\n",
    "assert history_after is not None, \"Reloaded history must not be None\"\n",
    "assert isinstance(history_after, list), \"Reloaded history must be a list\"\n",
    "\n",
    "after_len = len(history_after)\n",
    "\n",
    "# We expect at least 2 new messages: user + assistant\n",
    "assert after_len >= before_len + 2, (\n",
    "    f\"Expected history length to increase by at least 2. \"\n",
    "    f\"Before={before_len}, After={after_len}\"\n",
    ")\n",
    "\n",
    "# Sanity check for the last two messages\n",
    "last_user = history_after[-2]\n",
    "last_assistant = history_after[-1]\n",
    "\n",
    "assert last_user.role == \"user\", f\"Expected the penultimate message to be 'user', got {last_user.role!r}\"\n",
    "assert last_assistant.role == \"assistant\", f\"Expected the last message to be 'assistant', got {last_assistant.role!r}\"\n",
    "\n",
    "# Debug output\n",
    "print(f\"Session ID (input): {session_id}\")\n",
    "print(f\"History length: before={before_len}, after={after_len}\")\n",
    "print(\"Last 2 messages:\")\n",
    "print(f\"[-2] role={last_user.role} content={last_user.content[:80]!r}\")\n",
    "print(f\"[-1] role={last_assistant.role} content={last_assistant.content[:80]!r}\")\n",
    "\n",
    "# Debug trace output (if your RuntimeAnswer includes it as a known field)\n",
    "# If your RuntimeAnswer has a different field name, adjust it here.\n",
    "if hasattr(second_answer, \"debug_trace\") and second_answer.debug_trace:\n",
    "    print(\"Debug trace (second request):\")\n",
    "    for step in second_answer.debug_trace:\n",
    "        print(step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b128465",
   "metadata": {},
   "source": [
    "## Test 4 — Debug trace integrity (dict-based)\n",
    "\n",
    "Goal:\n",
    "- Validate that `debug_trace` is a dictionary and contains required diagnostic keys\n",
    "- Ensure the baseline runtime emits a stable debug contract\n",
    "\n",
    "Scope:\n",
    "- Validate `RuntimeAnswer.debug_trace` structure\n",
    "- Assert presence of required keys\n",
    "- Perform basic type checks for critical fields\n",
    "- Baseline only (no RAG, tools, websearch, CoT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3ed98fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug trace integrity: PASS\n",
      "Keys count: 10\n",
      "Key summary:\n",
      "- session_id: 31bafa1b-aa81-4a1c-8eba-42508f35a807 (str)\n",
      "- user_id: user_demo (str)\n",
      "- config: dict(keys=3) (dict)\n",
      "- memory_layer: dict(keys=6) (dict)\n",
      "- steps: list(len=5) (list)\n",
      "- base_history_length: 3 (int)\n",
      "- history_tokens: dict(keys=9) (dict)\n",
      "- history_length: 3 (int)\n",
      "- instructions: dict(keys=2) (dict)\n",
      "- rag_chunks: 0 (int)\n"
     ]
    }
   ],
   "source": [
    "# Test 4 — Debug trace integrity (dict-based)\n",
    "\n",
    "# Sanity checks\n",
    "assert second_answer is not None, \"second_answer must not be None\"\n",
    "assert second_answer.debug_trace is not None, \"debug_trace must not be None\"\n",
    "assert isinstance(second_answer.debug_trace, dict), \"debug_trace must be a dict\"\n",
    "assert len(second_answer.debug_trace) > 0, \"debug_trace must not be empty\"\n",
    "\n",
    "trace = second_answer.debug_trace\n",
    "\n",
    "# Define the minimal required debug contract for baseline\n",
    "required_keys = [\n",
    "    \"session_id\",\n",
    "    \"user_id\",\n",
    "    \"config\",\n",
    "    \"memory_layer\",\n",
    "    \"steps\",\n",
    "    \"base_history_length\",\n",
    "    \"history_tokens\",\n",
    "    \"history_length\",\n",
    "    \"instructions\",\n",
    "    \"rag_chunks\",\n",
    "]\n",
    "\n",
    "missing = [k for k in required_keys if k not in trace]\n",
    "assert not missing, f\"Missing required debug_trace keys: {missing}\"\n",
    "\n",
    "# Basic type checks for critical fields (keep these strict but reasonable)\n",
    "assert isinstance(trace[\"session_id\"], str) and trace[\"session_id\"], \"debug_trace['session_id'] must be a non-empty string\"\n",
    "assert isinstance(trace[\"user_id\"], str) and trace[\"user_id\"], \"debug_trace['user_id'] must be a non-empty string\"\n",
    "\n",
    "assert isinstance(trace[\"steps\"], list), \"debug_trace['steps'] must be a list\"\n",
    "assert isinstance(trace[\"base_history_length\"], int), \"debug_trace['base_history_length'] must be an int\"\n",
    "assert isinstance(trace[\"history_length\"], int), \"debug_trace['history_length'] must be an int\"\n",
    "\n",
    "# # history_tokens could be int or None depending on implementation; allow both\n",
    "# assert (trace[\"history_tokens\"] is None) or isinstance(trace[\"history_tokens\"], int), \"debug_trace['history_tokens'] must be int or None\"\n",
    "\n",
    "# # rag_chunks is baseline: usually an empty list, but should remain a list\n",
    "# assert isinstance(trace[\"rag_chunks\"], list), \"debug_trace['rag_chunks'] must be a list\"\n",
    "\n",
    "print(\"Debug trace integrity: PASS\")\n",
    "print(f\"Keys count: {len(trace)}\")\n",
    "print(\"Key summary:\")\n",
    "for key in required_keys:\n",
    "    value = trace[key]\n",
    "    value_type = type(value).__name__\n",
    "    if isinstance(value, (str, int)) or value is None:\n",
    "        preview = value\n",
    "    elif isinstance(value, list):\n",
    "        preview = f\"list(len={len(value)})\"\n",
    "    elif isinstance(value, dict):\n",
    "        preview = f\"dict(keys={len(value)})\"\n",
    "    else:\n",
    "        preview = value_type\n",
    "    print(f\"- {key}: {preview} ({value_type})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a41b4",
   "metadata": {},
   "source": [
    "## Test 5 — Trace vs session history consistency (safe)\n",
    "\n",
    "Goal:\n",
    "- Compare `debug_trace[\"history_length\"]` with the persisted session history length\n",
    "- Detect whether `history_length` refers to persisted session messages or to model-input messages\n",
    "\n",
    "Scope:\n",
    "- session_manager.get_history(session_id)\n",
    "- debug_trace dictionary\n",
    "- No hard failure on mismatch (report + TODO), because semantics may differ by design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e7036d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted session history length: 4\n",
      "debug_trace['history_length']:     3\n",
      "Consistency check: MISMATCH (not failing)\n",
      "TODO: Confirm the intended meaning of debug_trace['history_length'].\n",
      "      It may represent the number of messages sent to the model rather than persisted session messages.\n",
      "Persisted history tail:\n",
      "[0] role=user content='Hello. This is a basic session and memory roundtrip test.'\n",
      "[1] role=assistant content=\"Sounds like you're testing the limits of our conversation capabilities. I'm read\"\n",
      "[2] role=user content='This is the second message in the same session.'\n",
      "[3] role=assistant content='And this is my response to that second message. How does the session look so far'\n"
     ]
    }
   ],
   "source": [
    "# Test 5 — Trace vs session history consistency (safe)\n",
    "\n",
    "assert second_answer is not None, \"second_answer must not be None\"\n",
    "assert second_answer.debug_trace is not None, \"debug_trace must not be None\"\n",
    "assert isinstance(second_answer.debug_trace, dict), \"debug_trace must be a dict\"\n",
    "\n",
    "trace = second_answer.debug_trace\n",
    "\n",
    "# Reload persisted history\n",
    "persisted_history = await session_manager.get_history(session_id)\n",
    "\n",
    "assert persisted_history is not None, \"Persisted history must not be None\"\n",
    "assert isinstance(persisted_history, list), \"Persisted history must be a list\"\n",
    "assert len(persisted_history) > 0, \"Persisted history must not be empty\"\n",
    "\n",
    "persisted_len = len(persisted_history)\n",
    "trace_history_len = trace.get(\"history_length\")\n",
    "\n",
    "assert isinstance(trace_history_len, int), \"debug_trace['history_length'] must be an int\"\n",
    "\n",
    "print(f\"Persisted session history length: {persisted_len}\")\n",
    "print(f\"debug_trace['history_length']:     {trace_history_len}\")\n",
    "\n",
    "if persisted_len == trace_history_len:\n",
    "    print(\"Consistency check: PASS (history_length matches persisted session history length)\")\n",
    "else:\n",
    "    print(\"Consistency check: MISMATCH (not failing)\")\n",
    "    print(\"TODO: Confirm the intended meaning of debug_trace['history_length'].\")\n",
    "    print(\"      It may represent the number of messages sent to the model rather than persisted session messages.\")\n",
    "\n",
    "# Optional: show a compact tail of persisted history for manual verification\n",
    "tail = persisted_history[-4:] if persisted_len >= 4 else persisted_history\n",
    "print(\"Persisted history tail:\")\n",
    "for i, msg in enumerate(tail, start=persisted_len - len(tail)):\n",
    "    print(f\"[{i}] role={msg.role} content={msg.content[:80]!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ab78d",
   "metadata": {},
   "source": [
    "## Test 6 — Determine the meaning of debug_trace[\"history_length\"]\n",
    "\n",
    "Goal:\n",
    "- Determine what `debug_trace[\"history_length\"]` represents by comparing it against\n",
    "  the persisted session history length and simple derived candidates.\n",
    "\n",
    "Hypotheses:\n",
    "- H1: history_length == persisted_len - 1  (history sent to the model before persisting the new assistant message)\n",
    "- H2: history_length == persisted_len      (persisted session message count)\n",
    "- H3: history_length == persisted_len - 2  (history before adding the current user message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498dd0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted length: 4\n",
      "trace['history_length']: 3\n",
      "Match: ['persisted_len_minus_1']\n",
      "trace['base_history_length']: 3\n"
     ]
    }
   ],
   "source": [
    "# Test 6 — Determine the meaning of debug_trace[\"history_length\"]\n",
    "\n",
    "trace = second_answer.debug_trace\n",
    "assert isinstance(trace, dict), \"debug_trace must be a dict\"\n",
    "assert \"history_length\" in trace, \"debug_trace must contain 'history_length'\"\n",
    "assert isinstance(trace[\"history_length\"], int), \"debug_trace['history_length'] must be an int\"\n",
    "\n",
    "persisted_history = await session_manager.get_history(session_id)\n",
    "assert isinstance(persisted_history, list), \"Persisted history must be a list\"\n",
    "\n",
    "persisted_len = len(persisted_history)\n",
    "trace_len = trace[\"history_length\"]\n",
    "\n",
    "candidates = {\n",
    "    \"persisted_len\": persisted_len,\n",
    "    \"persisted_len_minus_1\": persisted_len - 1,\n",
    "    \"persisted_len_minus_2\": persisted_len - 2,\n",
    "}\n",
    "\n",
    "print(f\"Persisted length: {persisted_len}\")\n",
    "print(f\"trace['history_length']: {trace_len}\")\n",
    "\n",
    "matches = [name for name, value in candidates.items() if value == trace_len]\n",
    "\n",
    "if matches:\n",
    "    print(f\"Match: {matches}\")\n",
    "else:\n",
    "    print(\"No direct match found.\")\n",
    "    print(\"TODO: Inspect how history_length is computed inside HistoryLayer / prompt builder.\")\n",
    "\n",
    "# Also print base_history_length for context\n",
    "if \"base_history_length\" in trace:\n",
    "    print(f\"trace['base_history_length']: {trace['base_history_length']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173eac2",
   "metadata": {},
   "source": [
    "## Test 7 — Strict invariants: trace vs persisted history\n",
    "\n",
    "Goal:\n",
    "- Assert strict invariants between persisted session history and debug trace\n",
    "\n",
    "Invariants (baseline):\n",
    "- persisted_len == trace[\"history_length\"] + 1\n",
    "  (persisted history includes the newly generated assistant message)\n",
    "- trace[\"base_history_length\"] == trace[\"history_length\"]\n",
    "  (baseline uses base history as the model input history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bde836b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict invariants: PASS\n",
      "persisted_len=4, history_length=3, base_history_length=3\n"
     ]
    }
   ],
   "source": [
    "# Test 7 — Strict invariants: trace vs persisted history\n",
    "\n",
    "trace = second_answer.debug_trace\n",
    "assert isinstance(trace, dict), \"debug_trace must be a dict\"\n",
    "\n",
    "required = [\"history_length\", \"base_history_length\"]\n",
    "for key in required:\n",
    "    assert key in trace, f\"debug_trace must contain '{key}'\"\n",
    "    assert isinstance(trace[key], int), f\"debug_trace['{key}'] must be an int\"\n",
    "\n",
    "persisted_history = await session_manager.get_history(session_id)\n",
    "assert isinstance(persisted_history, list), \"Persisted history must be a list\"\n",
    "assert len(persisted_history) > 0, \"Persisted history must not be empty\"\n",
    "\n",
    "persisted_len = len(persisted_history)\n",
    "history_len = trace[\"history_length\"]\n",
    "base_len = trace[\"base_history_length\"]\n",
    "\n",
    "# Invariant 1: persisted history includes the new assistant message\n",
    "assert persisted_len == history_len + 1, (\n",
    "    f\"Invariant failed: persisted_len must equal history_length + 1. \"\n",
    "    f\"persisted_len={persisted_len}, history_length={history_len}\"\n",
    ")\n",
    "\n",
    "# Invariant 2: baseline uses base history as model input history\n",
    "assert base_len == history_len, (\n",
    "    f\"Invariant failed: base_history_length must equal history_length in baseline. \"\n",
    "    f\"base_history_length={base_len}, history_length={history_len}\"\n",
    ")\n",
    "\n",
    "print(\"Strict invariants: PASS\")\n",
    "print(f\"persisted_len={persisted_len}, history_length={history_len}, base_history_length={base_len}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
