{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dada617",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework\n",
    "\n",
    "# Notebook 11 — ChatGPT-like E2E (behavior-level)\n",
    "\n",
    "This notebook is an **integration / behavior** test that exercises the Drop-In Knowledge Runtime end-to-end, in a way that resembles a real ChatGPT usage pattern.\n",
    "\n",
    "## What we test (behavior)\n",
    "- Multi-session behavior (A/B/C) with **isolated session history** and **shared user LTM**\n",
    "- User LTM persistence + recall across sessions\n",
    "- Session-level consolidation (history → summary) without cross-session leakage\n",
    "- RAG ingestion + Q&A over a document\n",
    "- Websearch as a context layer that affects the final answer\n",
    "- Tools execution (tool + LLM) without breaking the **user-last invariant**\n",
    "- Reasoning enabled for observability, but **not persisted into user-visible history**\n",
    "\n",
    "## What we do NOT test (intentionally out of scope for this notebook)\n",
    "- Retry / fallback logic for empty LLM outputs\n",
    "- Formal “adapter contract” beyond `generate_messages(...) -> str`\n",
    "- Tools contract v1 (final answer vs context-only)\n",
    "- Prompt tuning, quality scoring, reranking\n",
    "- Debug refactors (e.g., removing getattr) unless strictly required to run the notebook\n",
    "\n",
    "## Hard invariants\n",
    "- **User-last invariant:** the last message sent to the core LLM must always be `role=\"user\"`\n",
    "- No empty assistant answers in the final output\n",
    "- No memory leakage between sessions (history isolation)\n",
    "\n",
    "## Requirements\n",
    "This notebook expects your local environment to provide:\n",
    "- Intergrax sources on PYTHONPATH\n",
    "- LLM credentials (e.g., `OPENAI_API_KEY`) if using OpenAI adapters\n",
    "- Vectorstore backend deps (Chroma/Qdrant) if enabling RAG/LTM vector retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27ca231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c26e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS\\AppData\\Local\\Temp\\ipykernel_22996\\4077648888.py:25: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  RUN_ID = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=rag_docs_20251222_082640_1a1d5e42\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=user_ltm_20251222_082640_1a1d5e42\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "BOOTSTRAP OK\n",
      "RUN_ID: 20251222_082640_1a1d5e42\n",
      "ARTIFACTS_DIR: D:\\Projekty\\intergrax\\notebooks\\drop_in_knowledge_mode\\_artifacts\\notebook_11_chatgpt_like\\20251222_082640_1a1d5e42\n",
      "USER_ID: user_chatgpt_like_001\n",
      "SESSION_A: sess_chatgpt_like_A\n",
      "SESSION_B: sess_chatgpt_like_B\n",
      "SESSION_C: sess_chatgpt_like_C\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from intergrax.llm_adapters.base import LLMAdapter, LLMAdapterRegistry, LLMProvider\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import ReasoningConfig, RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "from intergrax.memory.user_profile_manager import UserProfileManager\n",
    "from intergrax.memory.stores.in_memory_user_profile_store import InMemoryUserProfileStore\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "\n",
    "# =====================================================================\n",
    "# Global test identifiers / paths (no tests executed in this cell)\n",
    "# =====================================================================\n",
    "\n",
    "USER_ID = \"user_chatgpt_like_001\"\n",
    "\n",
    "SESSION_A = \"sess_chatgpt_like_A\"\n",
    "SESSION_B = \"sess_chatgpt_like_B\"\n",
    "SESSION_C = \"sess_chatgpt_like_C\"\n",
    "\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).resolve()\n",
    "ARTIFACTS_DIR = BASE_DIR / \"_artifacts\" / \"notebook_11_chatgpt_like\" / RUN_ID\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Separate vectorstore collections (1 instance = 1 collection_name)\n",
    "# - RAG_DOCS: document ingestion/retrieval\n",
    "# - USER_LTM: user long-term memory retrieval\n",
    "RAG_DIR = ARTIFACTS_DIR / \"vs_rag_docs\"\n",
    "LTM_DIR = ARTIFACTS_DIR / \"vs_user_ltm\"\n",
    "RAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LTM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LLM adapter (real adapter, no wrappers)\n",
    "# - assumes env is configured (OPENAI_API_KEY etc.)\n",
    "# ---------------------------------------------------------------------\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Embeddings + vectorstore (real managers)\n",
    "# Pick providers you actually use in your repo/env.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "embed_manager = EmbeddingManager(\n",
    "    provider=\"ollama\",\n",
    ")\n",
    "\n",
    "rag_vs = VectorstoreManager(\n",
    "    config=VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=f\"rag_docs_{RUN_ID}\",\n",
    "        chroma_persist_directory=str(RAG_DIR),\n",
    "    )\n",
    ")\n",
    "\n",
    "# User LTM vectorstore\n",
    "ltm_vs = VectorstoreManager(\n",
    "    VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=f\"user_ltm_{RUN_ID}\",\n",
    "        chroma_persist_directory=str(LTM_DIR),\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Stores\n",
    "# ---------------------------------------------------------------------\n",
    "session_store = InMemorySessionStorage()\n",
    "user_profile_store = InMemoryUserProfileStore()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Managers\n",
    "# ---------------------------------------------------------------------\n",
    "user_profile_manager = UserProfileManager(\n",
    "    store=user_profile_store,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=ltm_vs,\n",
    ")\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=session_store,\n",
    "    user_profile_manager=user_profile_manager,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Runtime config\n",
    "# ---------------------------------------------------------------------\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=rag_vs,\n",
    "    enable_user_profile_memory=True,\n",
    "    enable_org_profile_memory=False,\n",
    "    enable_user_longterm_memory=True,\n",
    "    enable_rag=True,\n",
    "    enable_websearch=True,\n",
    "    tools_mode=\"off\",\n",
    "    reasoning_config=ReasoningConfig()\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Runtime factory (keeps Cell-2..Cell-8 focused on behavior)\n",
    "# =====================================================================\n",
    "def build_runtime(*, override_config: dict | None = None, **runtime_kwargs) -> DropInKnowledgeRuntime:\n",
    "    \"\"\"\n",
    "    Build a DropInKnowledgeRuntime instance.\n",
    "    - override_config: dict of RuntimeConfig fields to override (shallow).\n",
    "    - runtime_kwargs: runtime init kwargs (e.g., ingestion_service, context_builder, prompt builders).\n",
    "    \"\"\"\n",
    "    if override_config:\n",
    "        for k, v in override_config.items():\n",
    "            setattr(config, k, v)\n",
    "\n",
    "    return DropInKnowledgeRuntime(\n",
    "        config=config,\n",
    "        session_manager=session_manager,\n",
    "        ingestion_service=runtime_kwargs.get(\"ingestion_service\"),\n",
    "        context_builder=runtime_kwargs.get(\"context_builder\"),\n",
    "        rag_prompt_builder=runtime_kwargs.get(\"rag_prompt_builder\"),\n",
    "        websearch_prompt_builder=runtime_kwargs.get(\"websearch_prompt_builder\"),\n",
    "        history_prompt_builder=runtime_kwargs.get(\"history_prompt_builder\"),\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"BOOTSTRAP OK\")\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"ARTIFACTS_DIR:\", str(ARTIFACTS_DIR))\n",
    "print(\"USER_ID:\", USER_ID)\n",
    "print(\"SESSION_A:\", SESSION_A)\n",
    "print(\"SESSION_B:\", SESSION_B)\n",
    "print(\"SESSION_C:\", SESSION_C)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
