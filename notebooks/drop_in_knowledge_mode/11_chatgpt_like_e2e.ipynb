{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dada617",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Integrax framework\n",
    "\n",
    "# Notebook 11 — ChatGPT-like E2E (behavior-level)\n",
    "\n",
    "This notebook is an **integration / behavior** test that exercises the Drop-In Knowledge Runtime end-to-end, in a way that resembles a real ChatGPT usage pattern.\n",
    "\n",
    "## What we test (behavior)\n",
    "- Multi-session behavior (A/B/C) with **isolated session history** and **shared user LTM**\n",
    "- User LTM persistence + recall across sessions\n",
    "- Session-level consolidation (history → summary) without cross-session leakage\n",
    "- RAG ingestion + Q&A over a document\n",
    "- Websearch as a context layer that affects the final answer\n",
    "- Tools execution (tool + LLM) without breaking the **user-last invariant**\n",
    "- Reasoning enabled for observability, but **not persisted into user-visible history**\n",
    "\n",
    "## What we do NOT test (intentionally out of scope for this notebook)\n",
    "- Retry / fallback logic for empty LLM outputs\n",
    "- Formal “adapter contract” beyond `generate_messages(...) -> str`\n",
    "- Tools contract v1 (final answer vs context-only)\n",
    "- Prompt tuning, quality scoring, reranking\n",
    "- Debug refactors (e.g., removing getattr) unless strictly required to run the notebook\n",
    "\n",
    "## Hard invariants\n",
    "- **User-last invariant:** the last message sent to the core LLM must always be `role=\"user\"`\n",
    "- No empty assistant answers in the final output\n",
    "- No memory leakage between sessions (history isolation)\n",
    "\n",
    "## Requirements\n",
    "This notebook expects your local environment to provide:\n",
    "- Intergrax sources on PYTHONPATH\n",
    "- LLM credentials (e.g., `OPENAI_API_KEY`) if using OpenAI adapters\n",
    "- Vectorstore backend deps (Chroma/Qdrant) if enabling RAG/LTM vector retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27ca231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8005ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_CSE_API_KEY\"] = os.getenv(\"GOOGLE_CSE_API_KEY\") or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c26e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS\\AppData\\Local\\Temp\\ipykernel_40832\\2251295334.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  RUN_ID = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=rag_docs_20251223_091134_741683ee\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "[intergraxVectorstoreManager] Initialized provider=chroma, collection=user_ltm_20251223_091134_741683ee\n",
      "[intergraxVectorstoreManager] Existing count: 0\n",
      "BOOTSTRAP OK\n",
      "RUN_ID: 20251223_091134_741683ee\n",
      "ARTIFACTS_DIR: D:\\Projekty\\intergrax\\notebooks\\drop_in_knowledge_mode\\_artifacts\\notebook_11_chatgpt_like\\20251223_091134_741683ee\n",
      "USER_ID: user_chatgpt_like_001\n",
      "SESSION_A: sess_chatgpt_like_A\n",
      "SESSION_B: sess_chatgpt_like_B\n",
      "SESSION_C: sess_chatgpt_like_C\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "from intergrax.llm_adapters.base import LLMAdapter, LLMAdapterRegistry, LLMProvider\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import ReasoningConfig, RuntimeConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.engine.runtime import DropInKnowledgeRuntime\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.in_memory_session_storage import InMemorySessionStorage\n",
    "from intergrax.runtime.drop_in_knowledge_mode.session.session_manager import SessionManager\n",
    "from intergrax.memory.user_profile_manager import UserProfileManager\n",
    "from intergrax.memory.stores.in_memory_user_profile_store import InMemoryUserProfileStore\n",
    "from intergrax.rag.embedding_manager import EmbeddingManager\n",
    "from intergrax.rag.vectorstore_manager import VSConfig, VectorstoreManager\n",
    "from intergrax.runtime.user_profile.session_memory_consolidation_service import SessionMemoryConsolidationService\n",
    "from intergrax.runtime.user_profile.user_profile_instructions_service import UserProfileInstructionsService\n",
    "from intergrax.websearch.providers.google_cse_provider import GoogleCSEProvider\n",
    "from intergrax.websearch.service.websearch_executor import WebSearchExecutor\n",
    "\n",
    "# =====================================================================\n",
    "# Global test identifiers / paths (no tests executed in this cell)\n",
    "# =====================================================================\n",
    "\n",
    "USER_ID = \"user_chatgpt_like_001\"\n",
    "\n",
    "SESSION_A = \"sess_chatgpt_like_A\"\n",
    "SESSION_B = \"sess_chatgpt_like_B\"\n",
    "SESSION_C = \"sess_chatgpt_like_C\"\n",
    "\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + uuid.uuid4().hex[:8]\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).resolve()\n",
    "ARTIFACTS_DIR = BASE_DIR / \"_artifacts\" / \"notebook_11_chatgpt_like\" / RUN_ID\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Separate vectorstore collections (1 instance = 1 collection_name)\n",
    "# - RAG_DOCS: document ingestion/retrieval\n",
    "# - USER_LTM: user long-term memory retrieval\n",
    "RAG_DIR = ARTIFACTS_DIR / \"vs_rag_docs\"\n",
    "LTM_DIR = ARTIFACTS_DIR / \"vs_user_ltm\"\n",
    "RAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LTM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LLM adapter (real adapter, no wrappers)\n",
    "# - assumes env is configured (OPENAI_API_KEY etc.)\n",
    "# ---------------------------------------------------------------------\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Embeddings + vectorstore (real managers)\n",
    "# Pick providers you actually use in your repo/env.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "embed_manager = EmbeddingManager(\n",
    "    provider=\"ollama\",\n",
    ")\n",
    "\n",
    "rag_vs = VectorstoreManager(\n",
    "    config=VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=f\"rag_docs_{RUN_ID}\",\n",
    "        chroma_persist_directory=str(RAG_DIR),\n",
    "    )\n",
    ")\n",
    "\n",
    "# User LTM vectorstore\n",
    "ltm_vs = VectorstoreManager(\n",
    "    VSConfig(\n",
    "        provider=\"chroma\",\n",
    "        collection_name=f\"user_ltm_{RUN_ID}\",\n",
    "        chroma_persist_directory=str(LTM_DIR),\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Stores\n",
    "# ---------------------------------------------------------------------\n",
    "session_store = InMemorySessionStorage()\n",
    "user_profile_store = InMemoryUserProfileStore()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Managers\n",
    "# ---------------------------------------------------------------------\n",
    "user_profile_manager = UserProfileManager(\n",
    "    store=user_profile_store,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=ltm_vs,\n",
    ")\n",
    "\n",
    "user_profile_instructions_service = UserProfileInstructionsService(\n",
    "    llm=llm_adapter,\n",
    "    manager=user_profile_manager,\n",
    ")\n",
    "\n",
    "session_memory_consolidation_service = SessionMemoryConsolidationService(\n",
    "    llm=llm_adapter,\n",
    "    profile_manager=user_profile_manager,\n",
    "    instructions_service=user_profile_instructions_service,\n",
    ")\n",
    "\n",
    "\n",
    "session_manager = SessionManager(\n",
    "    storage=session_store,\n",
    "    user_profile_manager=user_profile_manager,\n",
    "    session_memory_consolidation_service=session_memory_consolidation_service\n",
    ")\n",
    "\n",
    "\n",
    "websearch_executor = WebSearchExecutor(\n",
    "    providers=[\n",
    "        GoogleCSEProvider(),\n",
    "    ],\n",
    "    max_text_chars=None,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Runtime config\n",
    "# ---------------------------------------------------------------------\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=rag_vs,\n",
    "    websearch_executor=websearch_executor,\n",
    "    enable_user_profile_memory=True,\n",
    "    enable_org_profile_memory=False,\n",
    "    enable_user_longterm_memory=True,\n",
    "    enable_rag=True,\n",
    "    enable_websearch=True,\n",
    "    tools_mode=\"off\",\n",
    "    reasoning_config=ReasoningConfig()\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Runtime factory (keeps Cell-2..Cell-8 focused on behavior)\n",
    "# =====================================================================\n",
    "def build_runtime(*, override_config: dict | None = None, **runtime_kwargs) -> DropInKnowledgeRuntime:\n",
    "    \"\"\"\n",
    "    Build a DropInKnowledgeRuntime instance.\n",
    "    - override_config: dict of RuntimeConfig fields to override (shallow).\n",
    "    - runtime_kwargs: runtime init kwargs (e.g., ingestion_service, context_builder, prompt builders).\n",
    "    \"\"\"\n",
    "    if override_config:\n",
    "        for k, v in override_config.items():\n",
    "            setattr(config, k, v)\n",
    "\n",
    "    return DropInKnowledgeRuntime(\n",
    "        config=config,\n",
    "        session_manager=session_manager,\n",
    "        ingestion_service=runtime_kwargs.get(\"ingestion_service\"),\n",
    "        context_builder=runtime_kwargs.get(\"context_builder\"),\n",
    "        rag_prompt_builder=runtime_kwargs.get(\"rag_prompt_builder\"),\n",
    "        websearch_prompt_builder=runtime_kwargs.get(\"websearch_prompt_builder\"),\n",
    "        history_prompt_builder=runtime_kwargs.get(\"history_prompt_builder\"),\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"BOOTSTRAP OK\")\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"ARTIFACTS_DIR:\", str(ARTIFACTS_DIR))\n",
    "print(\"USER_ID:\", USER_ID)\n",
    "print(\"SESSION_A:\", SESSION_A)\n",
    "print(\"SESSION_B:\", SESSION_B)\n",
    "print(\"SESSION_C:\", SESSION_C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be54a0",
   "metadata": {},
   "source": [
    "## Cell 2 — Session A: onboarding + LTM write\n",
    "\n",
    "Goal:\n",
    "- Simulate a real onboarding turn (user introduces themselves and preferences).\n",
    "- Run the full runtime pipeline via `runtime.run(...)`.\n",
    "- Close the session to trigger **session → LTM consolidation**.\n",
    "- Minimal asserts:\n",
    "  1) Assistant answer is non-empty\n",
    "  2) Session history is persisted\n",
    "  3) User LTM contains at least one entry after closing the session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c572699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 1\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 2\n",
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 3\n",
      "SESSION A OK\n",
      "Answer length: 331\n",
      "History messages: 2\n",
      "LTM hits_count: None\n",
      "LTM debug: {'enabled': True, 'used': True, 'reason': 'hits', 'where': {'user_id': 'user_chatgpt_like_001', 'deleted': 0}, 'top_k': 5, 'threshold': 0.0, 'raw_ids': ['5e8d35195aa8467db8959b416f6477bf', '3619e58acfc64a458f97dbd45b99682b', '91205505565f42508ce18a0760c3084d'], 'raw_scores': [0.23801147937774658, 0.1308668851852417, 0.008136987686157227], 'raw_metadatas': [{'entry_id': '5e8d35195aa8467db8959b416f6477bf', 'tags': 'user,goal', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'kind': 'user_fact', 'deleted': 0}, {'kind': 'session_summary', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'tags': 'session_summary', 'deleted': 0, 'entry_id': '3619e58acfc64a458f97dbd45b99682b'}, {'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation', 'entry_id': '91205505565f42508ce18a0760c3084d', 'tags': 'communication,tone', 'kind': 'preference', 'deleted': 0}], 'raw_documents_preview': ['Artur buduje Integrax i Mooff.', 'Artur przedstawił się i podał swoje preferencje dotyczące komunikacji. Zapytał, jakie mogę mu zaproponować wsparcie w kwestiach związanych z Integrax i Mooff.', 'preferuje krótkie, techniczne odpowiedzi bez użycia emotikoni w kodzie lub dokumentach technicznych.'], 'returned_count': 3, 'hits_count': 3}\n",
      "LTM hits: 3\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build runtime (no special overrides yet)\n",
    "# ---------------------------------------------------------------------\n",
    "runtime = build_runtime()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Session A — onboarding message (behaves like real ChatGPT)\n",
    "# ---------------------------------------------------------------------\n",
    "onboarding_message = (\n",
    "    \"Hi. I am Artur. I build Integrax and Mooff. \"\n",
    "    \"I prefer concise, technical answers. Never use emojis in code or technical docs. \"\n",
    "    \"Please remember this for future sessions.\"\n",
    ")\n",
    "\n",
    "request_a = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_A,\n",
    "    message=onboarding_message,\n",
    ")\n",
    "\n",
    "answer_a = await runtime.run(request_a)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal assert #1: non-empty assistant answer\n",
    "# ---------------------------------------------------------------------\n",
    "assert isinstance(answer_a.answer, str) and answer_a.answer.strip(), \"Empty assistant answer in Session A.\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal assert #2: session history persisted (via SessionManager storage)\n",
    "# ---------------------------------------------------------------------\n",
    "history_a = await session_manager.get_history(session_id=SESSION_A)\n",
    "assert len(history_a) >= 2, f\"Expected >=2 messages in history, got {len(history_a)}.\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Trigger consolidation to LTM by closing the session\n",
    "# (This is the behavior boundary: Session -> LTM)\n",
    "# ---------------------------------------------------------------------\n",
    "await session_manager.close_session(session_id=SESSION_A)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Minimal assert #3: LTM entries were created (semantic recall evidence)\n",
    "# We do a vector search against the user's LTM store.\n",
    "# ---------------------------------------------------------------------\n",
    "ltm_search = await user_profile_manager.search_longterm_memory(\n",
    "        user_id=USER_ID,\n",
    "        query=\"Artur Integrax Mooff preferences concise technical no emojis\",\n",
    "        top_k=5,\n",
    "        score_threshold=0.0,\n",
    "    )\n",
    "\n",
    "assert ltm_search.get(\"used_longterm\") is True, \"Expected long-term memory retrieval to be enabled.\"\n",
    "assert (ltm_search.get(\"debug\") or {}).get(\"hits_count\", 0) > 0, \"Expected at least one LTM entry after closing Session A.\"\n",
    "\n",
    "print(\"SESSION A OK\")\n",
    "print(\"Answer length:\", len(answer_a.answer))\n",
    "print(\"History messages:\", len(history_a))\n",
    "print(\"LTM hits_count:\", ltm_search.get(\"hits_count\"))\n",
    "print(\"LTM debug:\", ltm_search.get(\"debug\"))\n",
    "print(\"LTM hits:\", len(ltm_search.get(\"hits\") or []))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f27e96",
   "metadata": {},
   "source": [
    "## Cell 3 — Session B: recall (ChatGPT behavior)\n",
    "\n",
    "Goal:\n",
    "- Start a new session_id (fresh history).\n",
    "- Ask the system to recall facts from Session A using User LTM.\n",
    "- Minimal asserts:\n",
    "  1) Non-empty assistant answer\n",
    "  2) debug_trace shows User LTM was used\n",
    "  3) Answer contains recalled facts/preferences\n",
    "  4) Session B history is isolated from Session A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b3bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION B OK\n",
      "Answer length: 598\n",
      "LTM debug: {'enabled': True, 'used': True, 'reason': 'hits', 'where': {'user_id': 'user_chatgpt_like_001', 'deleted': 0}, 'top_k': 8, 'threshold': None, 'raw_ids': ['3619e58acfc64a458f97dbd45b99682b', '91205505565f42508ce18a0760c3084d', '5e8d35195aa8467db8959b416f6477bf'], 'raw_scores': [-0.26603829860687256, -0.37259626388549805, -0.49666833877563477], 'raw_metadatas': [{'entry_id': '3619e58acfc64a458f97dbd45b99682b', 'tags': 'session_summary', 'deleted': 0, 'source': 'session_consolidation', 'kind': 'session_summary', 'user_id': 'user_chatgpt_like_001'}, {'kind': 'preference', 'entry_id': '91205505565f42508ce18a0760c3084d', 'deleted': 0, 'tags': 'communication,tone', 'user_id': 'user_chatgpt_like_001', 'source': 'session_consolidation'}, {'deleted': 0, 'source': 'session_consolidation', 'user_id': 'user_chatgpt_like_001', 'entry_id': '5e8d35195aa8467db8959b416f6477bf', 'tags': 'user,goal', 'kind': 'user_fact'}], 'raw_documents_preview': ['Artur przedstawił się i podał swoje preferencje dotyczące komunikacji. Zapytał, jakie mogę mu zaproponować wsparcie w kwestiach związanych z Integrax i Mooff.', 'preferuje krótkie, techniczne odpowiedzi bez użycia emotikoni w kodzie lub dokumentach technicznych.', 'Artur buduje Integrax i Mooff.'], 'returned_count': 3, 'hits_count': 3, 'context_blocks_count': 1, 'context_preview': 'USER LONG-TERM MEMORY (retrieved)\\nUse these as factual user memory only if relevant to the question.\\nIf not relevant, ignore them.\\n\\n- [id=3619e58acfc64a458f97dbd45b99682b, session=sess_chatgpt_like_A, kind=session_summary, importance=medium] Artur przedstawił się i podał swoje preferencje dotyczące komunikacji. Zapytał, jakie mogę mu zaproponować wsparcie w kwestiach związanych z Integrax i Mooff.\\n- [id=91205505565f42508ce18a0760c3084d, session=sess_chatgpt_like_A, kind=preference, importance=high] preferuje krótkie, techniczne odpowiedzi bez użycia emotikoni w kodzie lub dokumentach technicznych.\\n- [id=5e8d35195aa8467db8959b416f6477bf, session=sess_chatgpt_like_A, kind=user_fact, importance=high] Artur buduje Integrax i Mooff.', 'context_preview_chars': 737, 'hits_preview': [{'entry_id': '3619e58acfc64a458f97dbd45b99682b', 'title': 'Podsumowanie sesji', 'kind': 'session_summary', 'deleted': False}, {'entry_id': '91205505565f42508ce18a0760c3084d', 'title': 'Przykładowe preferencje dotyczące komunikacji', 'kind': 'preference', 'deleted': False}, {'entry_id': '5e8d35195aa8467db8959b416f6477bf', 'title': 'Współtwórca Integrax i Mooff', 'kind': 'user_fact', 'deleted': False}]}\n",
      "History messages: 2\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "\n",
    "runtime = build_runtime()\n",
    "\n",
    "recall_prompt = (\n",
    "    \"Before we continue: remind me who I am and what I build. \"\n",
    "    \"Also remind me what answer style I prefer.\"\n",
    ")\n",
    "\n",
    "request_b = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_B,\n",
    "    message=recall_prompt,\n",
    ")\n",
    "\n",
    "answer_b = await runtime.run(request_b)\n",
    "\n",
    "# 1) Non-empty answer\n",
    "assert isinstance(answer_b.answer, str) and answer_b.answer.strip(), \"Empty assistant answer in Session B.\"\n",
    "\n",
    "# 2) Debug evidence: User LTM used\n",
    "dbg = answer_b.debug_trace or {}\n",
    "ltm_dbg = dbg.get(\"user_longterm_memory\") or {}\n",
    "ltm_used = bool(ltm_dbg.get(\"used\") is True or ltm_dbg.get(\"used_longterm\") is True)\n",
    "assert ltm_used, f\"Expected LTM to be used in Session B. user_longterm_memory={ltm_dbg}\"\n",
    "\n",
    "# 3) Behavior evidence: answer contains recalled facts/preferences\n",
    "ans_norm = answer_b.answer.lower()\n",
    "expected_any = [\n",
    "    \"artur\",\n",
    "    \"intergrax\",\n",
    "    \"mooff\",\n",
    "    \"concise\",\n",
    "    \"technical\",\n",
    "    \"never use emojis\",\n",
    "    \"no emojis\",\n",
    "]\n",
    "assert any(k in ans_norm for k in expected_any), (\n",
    "    \"Expected the answer to include recalled facts/preferences from Session A. \"\n",
    "    f\"Answer was:\\n{answer_b.answer}\"\n",
    ")\n",
    "\n",
    "# 4) Session history isolation sanity check\n",
    "history_b = await session_manager.get_history(session_id=SESSION_B)\n",
    "assert len(history_b) >= 2, f\"Expected >=2 messages in Session B history, got {len(history_b)}.\"\n",
    "\n",
    "# First user message in Session B should be the recall prompt (not Session A onboarding)\n",
    "first_user_b = next((m for m in history_b if getattr(m, \"role\", None) == \"user\"), None)\n",
    "assert first_user_b is not None, \"Expected a user message in Session B history.\"\n",
    "assert recall_prompt.strip() in (first_user_b.content or \"\"), \"Session B history isolation issue.\"\n",
    "\n",
    "print(\"SESSION B OK\")\n",
    "print(\"Answer length:\", len(answer_b.answer))\n",
    "print(\"LTM debug:\", ltm_dbg)\n",
    "print(\"History messages:\", len(history_b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6d343",
   "metadata": {},
   "source": [
    "## Cell 4 — Ingestion + RAG (document Q&A)\n",
    "\n",
    "Goal:\n",
    "- Ingest a real document into the **RAG vectorstore** via the runtime attachment ingestion flow.\n",
    "- Ask a question that can be answered only from the document.\n",
    "- Ask a second question that requires **RAG + User LTM** at the same time.\n",
    "\n",
    "Minimal asserts:\n",
    "1) Ingestion step completed (debug evidence)\n",
    "2) RAG was used (debug evidence: rag.used or rag_chunks > 0)\n",
    "3) Answer is non-empty and includes document facts\n",
    "4) For the combined question: both RAG and LTM were used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b1b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[intergraxVectorstoreManager] Upserting 1 items (dim=1536) to provider=chroma...\n",
      "[intergraxVectorstoreManager] Upsert complete. New count: 1\n",
      "INGEST OK\n",
      "Ingestion debug: [{'attachment_id': 'rag_doc_001', 'attachment_type': 'md', 'num_chunks': 1, 'vector_ids_count': 1, 'metadata': {'source_path': 'D:\\\\Projekty\\\\intergrax\\\\notebooks\\\\drop_in_knowledge_mode\\\\_artifacts\\\\notebook_11_chatgpt_like\\\\20251223_091134_741683ee\\\\rag_doc_001.md', 'session_id': 'sess_chatgpt_like_RAG', 'user_id': 'user_chatgpt_like_001', 'tenant_id': None, 'workspace_id': None}}]\n",
      "DOC QA OK\n",
      "Answer length: 176\n",
      "RAG debug: {'enabled': True, 'used': True, 'hits_count': 1, 'where_filter': {'session_id': 'sess_chatgpt_like_RAG', 'user_id': 'user_chatgpt_like_001'}, 'top_k': 8, 'score_threshold': None, 'hits': [{'id': 'rag_doc_001-0', 'score': 0.9068, 'metadata': {'source_name': 'rag_doc_001.md', 'chunk_total': 1, 'attachment_type': 'md', 'attachment_id': 'rag_doc_001', 'source_path': 'D:\\\\Projekty\\\\intergrax\\\\notebooks\\\\drop_in_knowledge_mode\\\\_artifacts\\\\notebook_11_chatgpt_like\\\\20251223_091134_741683ee\\\\rag_doc_001.md', 'user_id': 'user_chatgpt_like_001', 'chunk_id': '63326708b65e8c42#ch0000-467cd427', 'chunk_index': 0, 'source': 'D:\\\\Projekty\\\\intergrax\\\\notebooks\\\\drop_in_knowledge_mode\\\\_artifacts\\\\notebook_11_chatgpt_like\\\\20251223_091134_741683ee\\\\rag_doc_001.md', 'ext': '.md', 'parent_id': '63326708b65e8c42', 'label': 'RAG Demo Document', 'session_id': 'sess_chatgpt_like_RAG'}, 'preview': '# Integrax — RAG Demo Document\\n\\nThis document is used for an E2E test of Drop-In Knowledge Runtime.\\n\\nKey modules:\\n- Drop-In Knowledge Runtime\\n- User Long-Term Memory (LTM)\\n- Websearch\\n\\nImportant const'}]}\n",
      "RAG chunks: 1\n",
      "COMBINED RAG+LTM OK\n",
      "Answer length: 267\n",
      "RAG chunks: 1\n",
      "LTM debug: {'enabled': True, 'used': True, 'reason': 'hits', 'where': {'user_id': 'user_chatgpt_like_001', 'deleted': 0}, 'top_k': 8, 'threshold': None, 'raw_ids': ['3619e58acfc64a458f97dbd45b99682b', '91205505565f42508ce18a0760c3084d', '5e8d35195aa8467db8959b416f6477bf'], 'raw_scores': [-0.26478874683380127, -0.2678338289260864, -0.34972238540649414], 'raw_metadatas': [{'kind': 'session_summary', 'deleted': 0, 'source': 'session_consolidation', 'tags': 'session_summary', 'user_id': 'user_chatgpt_like_001', 'entry_id': '3619e58acfc64a458f97dbd45b99682b'}, {'kind': 'preference', 'entry_id': '91205505565f42508ce18a0760c3084d', 'source': 'session_consolidation', 'deleted': 0, 'user_id': 'user_chatgpt_like_001', 'tags': 'communication,tone'}, {'source': 'session_consolidation', 'deleted': 0, 'kind': 'user_fact', 'user_id': 'user_chatgpt_like_001', 'tags': 'user,goal', 'entry_id': '5e8d35195aa8467db8959b416f6477bf'}], 'raw_documents_preview': ['Artur przedstawił się i podał swoje preferencje dotyczące komunikacji. Zapytał, jakie mogę mu zaproponować wsparcie w kwestiach związanych z Integrax i Mooff.', 'preferuje krótkie, techniczne odpowiedzi bez użycia emotikoni w kodzie lub dokumentach technicznych.', 'Artur buduje Integrax i Mooff.'], 'returned_count': 3, 'hits_count': 3, 'context_blocks_count': 1, 'context_preview': 'USER LONG-TERM MEMORY (retrieved)\\nUse these as factual user memory only if relevant to the question.\\nIf not relevant, ignore them.\\n\\n- [id=3619e58acfc64a458f97dbd45b99682b, session=sess_chatgpt_like_A, kind=session_summary, importance=medium] Artur przedstawił się i podał swoje preferencje dotyczące komunikacji. Zapytał, jakie mogę mu zaproponować wsparcie w kwestiach związanych z Integrax i Mooff.\\n- [id=91205505565f42508ce18a0760c3084d, session=sess_chatgpt_like_A, kind=preference, importance=high] preferuje krótkie, techniczne odpowiedzi bez użycia emotikoni w kodzie lub dokumentach technicznych.\\n- [id=5e8d35195aa8467db8959b416f6477bf, session=sess_chatgpt_like_A, kind=user_fact, importance=high] Artur buduje Integrax i Mooff.', 'context_preview_chars': 737, 'hits_preview': [{'entry_id': '3619e58acfc64a458f97dbd45b99682b', 'title': 'Podsumowanie sesji', 'kind': 'session_summary', 'deleted': False}, {'entry_id': '91205505565f42508ce18a0760c3084d', 'title': 'Przykładowe preferencje dotyczące komunikacji', 'kind': 'preference', 'deleted': False}, {'entry_id': '5e8d35195aa8467db8959b416f6477bf', 'title': 'Współtwórca Integrax i Mooff', 'kind': 'user_fact', 'deleted': False}]}\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "from intergrax.llm.messages import AttachmentRef\n",
    "from intergrax.runtime.drop_in_knowledge_mode.ingestion.attachments import FileSystemAttachmentResolver\n",
    "from intergrax.runtime.drop_in_knowledge_mode.ingestion.ingestion_service import AttachmentIngestionService\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Create a real document file to ingest\n",
    "# ---------------------------------------------------------------------\n",
    "DOC_SESSION = \"sess_chatgpt_like_RAG\"\n",
    "\n",
    "doc_path = ARTIFACTS_DIR / \"rag_doc_001.md\"\n",
    "doc_text = \"\"\"# Integrax — RAG Demo Document\n",
    "\n",
    "This document is used for an E2E test of Drop-In Knowledge Runtime.\n",
    "\n",
    "Key modules:\n",
    "- Drop-In Knowledge Runtime\n",
    "- User Long-Term Memory (LTM)\n",
    "- Websearch\n",
    "\n",
    "Important constants:\n",
    "- The default max entries per LTM query is 8.\n",
    "- The project codename for the demo is \"NEBULA-11\".\n",
    "\n",
    "Behavior requirement:\n",
    "- Answers must be concise and technical.\n",
    "\"\"\"\n",
    "doc_path.write_text(doc_text, encoding=\"utf-8\")\n",
    "\n",
    "attachment = AttachmentRef(\n",
    "    id=\"rag_doc_001\",\n",
    "    type=\"md\",\n",
    "    uri=doc_path.as_uri(),          # raw path is supported by FileSystemAttachmentResolver\n",
    "    metadata={\"label\": \"RAG Demo Document\"}\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build ingestion service (indexes into rag_vs)\n",
    "# ---------------------------------------------------------------------\n",
    "resolver = FileSystemAttachmentResolver()\n",
    "\n",
    "ingestion_service = AttachmentIngestionService(\n",
    "    resolver=resolver,\n",
    "    embedding_manager=embed_manager,\n",
    "    vectorstore_manager=rag_vs,     # IMPORTANT: documents go to RAG vectorstore\n",
    ")\n",
    "\n",
    "runtime = build_runtime(ingestion_service=ingestion_service)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Turn 1: Upload + ingestion\n",
    "# ---------------------------------------------------------------------\n",
    "request_ingest = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=DOC_SESSION,\n",
    "    message=\"I uploaded a document. Please ingest it and confirm.\",\n",
    "    attachments=[attachment],\n",
    ")\n",
    "\n",
    "answer_ingest = await runtime.run(request_ingest)\n",
    "\n",
    "assert isinstance(answer_ingest.answer, str) and answer_ingest.answer.strip(), \"Empty assistant answer after ingestion.\"\n",
    "\n",
    "dbg_ingest = answer_ingest.debug_trace or {}\n",
    "ing_dbg = dbg_ingest.get(\"ingestion\") or {}\n",
    "\n",
    "# Ingestion debug shape can vary, so keep it minimal:\n",
    "# - either presence of ingestion trace\n",
    "# - or a non-empty list of ingested results\n",
    "assert ing_dbg is not None, f\"Expected ingestion debug trace. debug_trace={dbg_ingest}\"\n",
    "\n",
    "print(\"INGEST OK\")\n",
    "print(\"Ingestion debug:\", ing_dbg)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Turn 2: Pure document Q&A (RAG must be used)\n",
    "# ---------------------------------------------------------------------\n",
    "q_doc = \"In the uploaded document: what is the demo codename and what are the three key modules?\"\n",
    "\n",
    "request_doc_qa = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=DOC_SESSION,\n",
    "    message=q_doc,\n",
    ")\n",
    "\n",
    "answer_doc = await runtime.run(request_doc_qa)\n",
    "\n",
    "assert isinstance(answer_doc.answer, str) and answer_doc.answer.strip(), \"Empty assistant answer in document Q&A.\"\n",
    "\n",
    "dbg_doc = answer_doc.debug_trace or {}\n",
    "rag_dbg = dbg_doc.get(\"rag\") or {}\n",
    "rag_chunks = dbg_doc.get(\"rag_chunks\", 0)\n",
    "\n",
    "assert (rag_dbg.get(\"used\") is True) or (rag_chunks and rag_chunks > 0), (\n",
    "    f\"Expected RAG to be used. rag={rag_dbg}, rag_chunks={rag_chunks}\"\n",
    ")\n",
    "\n",
    "ans_doc_norm = answer_doc.answer.lower()\n",
    "assert (\"nebula-11\".lower() in ans_doc_norm), \"Expected the answer to include the codename from the document.\"\n",
    "assert (\"drop-in\" in ans_doc_norm) or (\"long-term\" in ans_doc_norm) or (\"websearch\" in ans_doc_norm), (\n",
    "    \"Expected the answer to include at least one key module from the document.\"\n",
    ")\n",
    "\n",
    "print(\"DOC QA OK\")\n",
    "print(\"Answer length:\", len(answer_doc.answer))\n",
    "print(\"RAG debug:\", rag_dbg)\n",
    "print(\"RAG chunks:\", rag_chunks)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Turn 3: Combined question (RAG + LTM)\n",
    "# - Must use RAG (document facts) and LTM (user preference / identity).\n",
    "# ---------------------------------------------------------------------\n",
    "q_combined = (\n",
    "    \"Using the uploaded document AND what you remember about me: \"\n",
    "    \"write a concise technical answer that (1) states who I am and what I build, \"\n",
    "    \"(2) lists the document's key modules, and (3) includes the codename.\"\n",
    ")\n",
    "\n",
    "request_combined = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=DOC_SESSION,\n",
    "    message=q_combined,\n",
    ")\n",
    "\n",
    "answer_combined = await runtime.run(request_combined)\n",
    "\n",
    "assert isinstance(answer_combined.answer, str) and answer_combined.answer.strip(), \"Empty assistant answer in combined RAG+LTM question.\"\n",
    "\n",
    "dbg_combined = answer_combined.debug_trace or {}\n",
    "\n",
    "# RAG evidence\n",
    "rag_dbg2 = dbg_combined.get(\"rag\") or {}\n",
    "rag_chunks2 = dbg_combined.get(\"rag_chunks\", 0)\n",
    "rag_used2 = (rag_dbg2.get(\"used\") is True) or (rag_chunks2 and rag_chunks2 > 0)\n",
    "assert rag_used2, f\"Expected RAG to be used in combined question. rag={rag_dbg2}, rag_chunks={rag_chunks2}\"\n",
    "\n",
    "# LTM evidence\n",
    "ltm_dbg2 = dbg_combined.get(\"user_longterm_memory\") or {}\n",
    "ltm_used2 = bool(ltm_dbg2.get(\"used\") is True or ltm_dbg2.get(\"used_longterm\") is True)\n",
    "assert ltm_used2, f\"Expected LTM to be used in combined question. ltm={ltm_dbg2}\"\n",
    "\n",
    "# Behavior evidence: contains memory facts + doc codename\n",
    "ans2 = answer_combined.answer.lower()\n",
    "assert \"artur\" in ans2, \"Expected the combined answer to include 'Artur' from LTM.\"\n",
    "assert (\"intergrax\" in ans2) or (\"mooff\" in ans2), \"Expected the combined answer to include Integrax/Mooff from LTM.\"\n",
    "assert \"nebula-11\" in ans2, \"Expected the combined answer to include the document codename.\"\n",
    "\n",
    "print(\"COMBINED RAG+LTM OK\")\n",
    "print(\"Answer length:\", len(answer_combined.answer))\n",
    "print(\"RAG chunks:\", rag_chunks2)\n",
    "print(\"LTM debug:\", ltm_dbg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28407554",
   "metadata": {},
   "source": [
    "## Cell 5 — Websearch (current knowledge, no documents)\n",
    "\n",
    "Goal:\n",
    "- Ask a question that requires up-to-date knowledge.\n",
    "- Ensure there are no documents in context (disable RAG for this cell).\n",
    "- Verify that:\n",
    "  1) Websearch was used (debug evidence)\n",
    "  2) Websearch produced context blocks / sources\n",
    "  3) The final answer references the retrieved web context (behavior evidence)\n",
    "\n",
    "Minimal asserts:\n",
    "- Non-empty assistant answer\n",
    "- debug_trace.websearch.used == True (or equivalent)\n",
    "- debug_trace.websearch.context_blocks_count > 0 (or sources_count > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed30c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEBSEARCH OK\n",
      "Answer length: 818\n",
      "Websearch blocks: 1\n",
      "Websearch preview chars: 2537\n",
      "Websearch docs preview: [{'title': 'Strategie projektowania promptów \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'url': 'https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=pl'}, {'title': 'JavaScript Jobs for December 2025 | Freelancer', 'url': 'https://www.freelancer.pl/jobs/javascript'}, {'title': 'HTML5 Jobs for December 2025 | Freelancer', 'url': 'https://www.freelancer.pl/jobs/html5'}, {'title': 'Relation 2025 - Data&AI Warsaw Tech Summit | Data&AI Warsaw Tech Summit', 'url': 'https://dataiwarsaw.tech/relation-2025/'}, {'title': 'Promocja urodzinowa Ebookpoint 2025 – Informatyka | Świat Czytników', 'url': 'https://swiatczytnikow.pl/ebookpoint-informatyka/'}]\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.responses.response_schema import RuntimeRequest\n",
    "from intergrax.websearch.service.websearch_config import WebSearchConfig, WebSearchStrategyType\n",
    "\n",
    "WEB_SESSION = \"sess_chatgpt_like_WEB\"\n",
    "\n",
    "# Build runtime with RAG disabled but websearch enabled.\n",
    "runtime_web = build_runtime(\n",
    "    override_config={\n",
    "        \"enable_rag\": False,\n",
    "        \"enable_websearch\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ensure websearch strategy + adapters are configured for this runtime instance.\n",
    "cfg = runtime_web._config  # assuming your build_runtime attaches config here (as in other notebook cells)\n",
    "cfg.websearch_config = WebSearchConfig(strategy=WebSearchStrategyType.MAP_REDUCE)\n",
    "cfg.websearch_config.llm.map_adapter = cfg.llm_adapter\n",
    "cfg.websearch_config.llm.reduce_adapter = cfg.llm_adapter\n",
    "\n",
    "web_q = (\n",
    "    \"What are the most recent major changes to the OpenAI API regarding the Responses API \"\n",
    "    \"and tool calling? Provide a concise technical summary with the date of the change.\"\n",
    ")\n",
    "\n",
    "request_web = RuntimeRequest(\n",
    "    user_id=USER_ID,\n",
    "    session_id=WEB_SESSION,\n",
    "    message=web_q,\n",
    ")\n",
    "\n",
    "answer_web = await runtime_web.run(request_web)\n",
    "\n",
    "# 1) Non-empty answer (runtime answered)\n",
    "assert isinstance(answer_web.answer, str) and answer_web.answer.strip(), \"Empty assistant answer in Websearch session.\"\n",
    "\n",
    "# 2) Routing says websearch was used\n",
    "route = answer_web.route\n",
    "assert route is not None, \"Missing route in RuntimeAnswer.\"\n",
    "assert route.used_websearch is True, f\"Expected used_websearch=True. route={route}\"\n",
    "\n",
    "dbg = answer_web.debug_trace or {}\n",
    "ws_dbg = dbg.get(\"websearch\") or {}\n",
    "\n",
    "# 3) Websearch produced context blocks + preview\n",
    "ctx_blocks = int(ws_dbg.get(\"context_blocks_count\", 0) or 0)\n",
    "assert ctx_blocks > 0, f\"Expected websearch context blocks > 0. websearch={ws_dbg}\"\n",
    "\n",
    "preview = (ws_dbg.get(\"context_preview\") or \"\").strip()\n",
    "assert preview, f\"Expected non-empty websearch context preview. websearch={ws_dbg}\"\n",
    "\n",
    "# 4) Diagnostics: raw results preview should exist (ensures executor returned objects with url/title)\n",
    "raw_preview = ws_dbg.get(\"raw_results_preview\") or []\n",
    "assert isinstance(raw_preview, list), f\"Expected raw_results_preview list. websearch={ws_dbg}\"\n",
    "assert len(raw_preview) > 0, f\"Expected raw_results_preview non-empty. websearch={ws_dbg}\"\n",
    "\n",
    "first = raw_preview[0] or {}\n",
    "assert isinstance(first, dict), f\"Expected raw_results_preview items to be dicts. got={type(first)}\"\n",
    "assert first.get(\"url\"), f\"Expected URL in raw_results_preview[0]. raw_results_preview={raw_preview[:2]}\"\n",
    "\n",
    "print(\"WEBSEARCH OK\")\n",
    "print(\"Answer length:\", len(answer_web.answer))\n",
    "print(\"used_websearch:\", route.used_websearch)\n",
    "print(\"Websearch blocks:\", ctx_blocks)\n",
    "print(\"Websearch preview chars:\", ws_dbg.get(\"context_preview_chars\"))\n",
    "print(\"Websearch no_evidence:\", ws_dbg.get(\"no_evidence\"))\n",
    "print(\"Websearch docs preview:\", ws_dbg.get(\"docs_preview\"))\n",
    "print(\"Websearch raw preview top urls:\", [x.get(\"url\") for x in raw_preview[:4]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
