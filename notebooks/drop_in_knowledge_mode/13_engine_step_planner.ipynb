{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4046145",
   "metadata": {},
   "source": [
    "# © Artur Czarnecki. All rights reserved.\n",
    "# Intergrax framework – proprietary and confidential.\n",
    "# Use, modification, or distribution without written permission is prohibited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50627d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a0a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\genai_longchain\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intergrax.llm_adapters.llm_provider import LLMProvider\n",
    "from intergrax.llm_adapters.llm_provider_registry import LLMAdapterRegistry\n",
    "from intergrax.runtime.drop_in_knowledge_mode.config import RuntimeConfig\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Global test constants\n",
    "# -------------------------------------------------------\n",
    "\n",
    "USER_ID = \"demo-user-planner\"\n",
    "SESSION_ID = \"sess_planner_only_001\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Build LLM adapter and RuntimeConfig\n",
    "# IMPORTANT: Replace build_llm_adapter() with your actual builder.\n",
    "# This must be the same adapter/config used in your runtime.\n",
    "# -------------------------------------------------------\n",
    "\n",
    "llm_adapter = LLMAdapterRegistry.create(LLMProvider.OLLAMA)\n",
    "\n",
    "config = RuntimeConfig(\n",
    "    llm_adapter=llm_adapter,\n",
    "    enable_rag=True,\n",
    "    enable_websearch=True,\n",
    "    tools_mode=\"auto\",\n",
    "    enable_user_longterm_memory=True,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2909c",
   "metadata": {},
   "source": [
    "# Hard-coded intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847e97ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK freshness routing: PlanIntent.FRESHNESS ['USE_WEBSEARCH', 'SYNTHESIZE_DRAFT', 'VERIFY_ANSWER', 'FINALIZE_ANSWER']\n",
      "OK project routing: PlanIntent.PROJECT_ARCHITECTURE ['USE_USER_LONGTERM_MEMORY_SEARCH', 'SYNTHESIZE_DRAFT', 'VERIFY_ANSWER', 'FINALIZE_ANSWER']\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.planning.step_planner import StepPlanner, StepPlannerConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import EngineHints, PlanIntent, StepAction\n",
    "\n",
    "sp = StepPlanner(StepPlannerConfig())\n",
    "\n",
    "# TEST 2: freshness -> websearch\n",
    "plan = sp.build_plan(\n",
    "    user_message=\"What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\",\n",
    "    engine_hints=EngineHints(\n",
    "        enable_websearch=True,\n",
    "        enable_ltm=True,\n",
    "        enable_rag=False,\n",
    "        enable_tools=False,\n",
    "        intent=PlanIntent.FRESHNESS,\n",
    "    ),\n",
    ")\n",
    "assert plan.intent == PlanIntent.FRESHNESS\n",
    "assert plan.steps[0].action == StepAction.USE_WEBSEARCH\n",
    "print(\"OK freshness routing:\", plan.intent, [s.action.value for s in plan.steps])\n",
    "\n",
    "# TEST 3: project -> ltm\n",
    "plan = sp.build_plan(\n",
    "    user_message=\"StepPlanner vs EnginePlanner integration in Intergrax. What should own intent?\",\n",
    "    engine_hints=EngineHints(\n",
    "        enable_websearch=True,\n",
    "        enable_ltm=True,\n",
    "        enable_rag=False,\n",
    "        enable_tools=False,\n",
    "        intent=PlanIntent.PROJECT_ARCHITECTURE,\n",
    "    ),\n",
    ")\n",
    "assert plan.intent == PlanIntent.PROJECT_ARCHITECTURE\n",
    "assert plan.steps[0].action == StepAction.USE_USER_LONGTERM_MEMORY_SEARCH\n",
    "print(\"OK project routing:\", plan.intent, [s.action.value for s in plan.steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d9d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: Explain how to implement an async retry strategy in Python for API calls.\n",
      "plan_id: stepplan-001\n",
      "mode: PlanMode.EXECUTE\n",
      "intent: PlanIntent.GENERIC\n",
      "steps: [<StepAction.SYNTHESIZE_DRAFT: 'SYNTHESIZE_DRAFT'>, <StepAction.VERIFY_ANSWER: 'VERIFY_ANSWER'>, <StepAction.FINALIZE_ANSWER: 'FINALIZE_ANSWER'>]\n",
      "last: StepAction.FINALIZE_ANSWER\n",
      "================================================================================\n",
      "TEST 2: What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\n",
      "plan_id: stepplan-001\n",
      "mode: PlanMode.EXECUTE\n",
      "intent: PlanIntent.GENERIC\n",
      "steps: [<StepAction.SYNTHESIZE_DRAFT: 'SYNTHESIZE_DRAFT'>, <StepAction.VERIFY_ANSWER: 'VERIFY_ANSWER'>, <StepAction.FINALIZE_ANSWER: 'FINALIZE_ANSWER'>]\n",
      "last: StepAction.FINALIZE_ANSWER\n",
      "================================================================================\n",
      "TEST 3: In my Intergrax runtime, should StepPlanner be separate from EnginePlanner? Give recommendation based on our architecture decisions.\n",
      "plan_id: stepplan-001\n",
      "mode: PlanMode.EXECUTE\n",
      "intent: PlanIntent.GENERIC\n",
      "steps: [<StepAction.SYNTHESIZE_DRAFT: 'SYNTHESIZE_DRAFT'>, <StepAction.VERIFY_ANSWER: 'VERIFY_ANSWER'>, <StepAction.FINALIZE_ANSWER: 'FINALIZE_ANSWER'>]\n",
      "last: StepAction.FINALIZE_ANSWER\n"
     ]
    }
   ],
   "source": [
    "from intergrax.runtime.drop_in_knowledge_mode.planning.step_planner import StepPlanner, StepPlannerConfig\n",
    "from intergrax.runtime.drop_in_knowledge_mode.planning.stepplan_models import EngineHints\n",
    "\n",
    "\n",
    "sp = StepPlanner(StepPlannerConfig())\n",
    "\n",
    "tests = [\n",
    "    \"Explain how to implement an async retry strategy in Python for API calls.\",\n",
    "    \"What are the most recent major changes to the OpenAI Responses API and tool calling? Provide dates.\",\n",
    "    \"In my Intergrax runtime, should StepPlanner be separate from EnginePlanner? Give recommendation based on our architecture decisions.\",\n",
    "]\n",
    "\n",
    "for i, q in enumerate(tests, 1):\n",
    "    plan = sp.build_plan(user_message=q, engine_hints=EngineHints(\n",
    "        enable_websearch=True,\n",
    "        enable_ltm=True,\n",
    "        enable_rag=False,\n",
    "        enable_tools=False,\n",
    "    ))\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"TEST {i}: {q}\")\n",
    "    print(\"plan_id:\", plan.plan_id)\n",
    "    print(\"mode:\", plan.mode)\n",
    "    print(\"intent:\", plan.intent)\n",
    "    print(\"steps:\", [s.action for s in plan.steps])\n",
    "    print(\"last:\", plan.steps[-1].action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53fb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK freshness routing: PlanIntent.FRESHNESS ['USE_WEBSEARCH', 'SYNTHESIZE_DRAFT', 'VERIFY_ANSWER', 'FINALIZE_ANSWER']\n",
      "OK project routing: PlanIntent.PROJECT_ARCHITECTURE ['USE_USER_LONGTERM_MEMORY_SEARCH', 'SYNTHESIZE_DRAFT', 'VERIFY_ANSWER', 'FINALIZE_ANSWER']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_longchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
